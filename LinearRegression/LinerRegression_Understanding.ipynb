{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liner Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Understand first Regression\n",
    "\n",
    "Predictive modelling which investigates the relationship between a _dependent variable and independent variable._<br>\n",
    "Please find below Temperature and iCe Cream sales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Temperature  Ice Cream Sales\n",
      "0          14.2            215.0\n",
      "1          16.4            325.0\n",
      "2          11.9            185.0\n",
      "3          15.2            332.0\n",
      "4          18.5            406.0\n",
      "5          22.1            522.0\n",
      "6          19.4            412.0\n",
      "7          25.1            614.0\n",
      "8          23.4            544.0\n",
      "9          18.1            421.0\n",
      "10         22.6            445.0\n",
      "11         17.2            408.0\n"
     ]
    }
   ],
   "source": [
    "url = 'Data/Ice_Cream_Sales_vs_Temperature.csv'\n",
    "import pandas as pd\n",
    "df = pd.read_csv(url,encoding = 'unicode_escape')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1fe4dc34898>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot.scatter(x='Temperature',y='Ice Cream Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here machine learning is trying to learn.<br>\n",
    "1)\tMachine learning like a brain <br>\n",
    "2)\tMachine Learning will see that, at 11.9 °, number are items are sold is 185  and at 25.1 ° the number of item are sold is 614.<br>\n",
    "3)\tMachine learning internally start preparation formula.<br>\n",
    "4)\tMachine learning is plotting a data between dependent variable and independent variable.<br>\n",
    "5)\tSo that in future if someone asked what will be value on sold ice-cream on temperature at 50°.<br>\n",
    "\n",
    "IS it not same as _Correlation_ ? __No__\n",
    "\n",
    "### Correlation \n",
    "\n",
    "When two sets of data are strongly linked together we say they have a _High Correlation._\n",
    "\n",
    "    Correlation is Positive when the values increase together, and\n",
    "    Correlation is Negative when one value decreases as the other increases\n",
    "    \n",
    "A correlation is assumed to be linear (following a line).\n",
    "![image.png](image/Correlation.png)\n",
    "\n",
    "    1 is a perfect positive correlation\n",
    "    0 is no correlation (the values don't seem linked at all)\n",
    "    -1 is a perfect negative correlation\n",
    "    \n",
    "Numpy implements a corrcoef() function that returns a matrix of correlations of x with x, x with y, y with x and y with y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.95750662],\n",
       "       [0.95750662, 1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.corrcoef(df.Temperature , df['Ice Cream Sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How To Calculate__\n",
    "\n",
    "_Step 1:_ Find the mean of x, and the mean of y <br>\n",
    "_Step 2:_ Subtract the mean of x from every x value (call them \"a\"), do the same for y\t(call them \"b\") <br>\n",
    "_Step 3:_ Calculate: ab, a2 and b2 for every value <br>\n",
    "_Step 4:_ Sum up ab, sum up a2 and sum up b2 <br>\n",
    "_Step 5:_ Divide the sum of ab by the square root of [(sum of a2) × (sum of b2)] <br>\n",
    "\n",
    "![image.png](image/Correlation_Calculation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Regression analysis, we have to find relation between _one dependent_ and _one or many independent variables._<br>\n",
    "        \n",
    "        Age = 5 + Height * 10 + Weight * 13\n",
    "Here we are establishing a relationship between Height & Weight of a person with his/ Her Age.\n",
    "### Linear Regression \n",
    "\n",
    "THis is liner approach, modelling the releationship between a response( or dependent variable) and or independent variables.\n",
    "“Linear Regression” is a method to predict dependent variable (Y) based on values of independent variables (X). whereas independent variables can have either continuous or categorical values. It can be used for the cases where we want to predict some continuous quantity. E.g., Predicting traffic in a retail store.\n",
    "\n",
    "__ Prerequisites__\n",
    "\n",
    "    Correlation (r) – Explains the relationship between two variables, possible values -1 to +1\n",
    "    \n",
    "    Variance (σ2)– \n",
    "        1) Measure of spread in your data It measures how far each number in the set is from the mean and is calculated by taking the differences between each number in the set and the mean, squaring the differences (to make them positive) and dividing the sum of the squares by the number of values in the set.\n",
    "        2) A variance value of zero indicates that all values within a set of numbers are identical; all variances that are non-zero will be positive numbers. A large variance indicates that numbers in the set are far from the mean and each other, while a small variance indicates the opposite.\n",
    "        Example of How to Use Variance\n",
    "                - Let's consider a hypothetical example: Returns for a stock are 10% in Year 1, 20% in Year 2, and -15% in Year 3. The average of these three returns is 5%. The differences between each return and the average are 5%, 15%, and -20% for each consecutive year.\n",
    "                - Squaring these deviations yields 25%, 225%, and 400%, respectively. Summing these squared deviations gives 650%. Dividing the sum of 650% by the number of returns in the data set (3 in this case) yields the variance of 216.67%. Taking the square root of the variance yields the standard deviation of 14.72% for the returns.\n",
    "        \n",
    "    Standard Deviation (σ) – Measure of spread in your data (Square root of Variance)\n",
    "        1) If the data points are further from the mean, there is a higher deviation within the data set; thus, the more spread out the data, the higher the standard deviation.\n",
    "        \n",
    "        \n",
    "    Normal distribution-\n",
    "        \n",
    "    Residual (error term) – {Actual value – Predicted value}\n",
    "\n",
    "__Linear Regression Line__\n",
    "\n",
    "While doing linear regression our objective is to fit a line through the distribution which is nearest to most of the points. Hence reducing the distance (error term) of data points from the fitted line.\n",
    "\n",
    "![image.png](image/Linear_Regression.png)\n",
    "\n",
    "For example, in above figure (left) dots represent various data points and line (right) represents an approximate line which can explain the relationship between ‘x’ & ‘y’ axes. Through, linear regression we try to find out such a line. For example, if we have one dependent variable ‘Y’ and one independent variable ‘X’ – relationship between ‘X’ & ‘Y’ can be represented in a form of following equation:\n",
    "\n",
    "Y = Β0 + Β1X\n",
    "\n",
    "Where,\n",
    "\n",
    "        Y = Dependent Variable\n",
    "        X = Independent Variable\n",
    "        Β0 = Constant term a.k.a Intercept\n",
    "        Β1 = Coefficient of relationship between ‘X’ & ‘Y’\n",
    "\n",
    "__Few properties of linear regression line__\n",
    "\n",
    "1) Regression line always passes through mean of independent variable (x) as well as mean of dependent variable (y)<br>\n",
    "2) Regression line minimizes the sum of “Square of Residuals”. That’s why the method of Linear Regression is known as “Ordinary Least Square (OLS).<br>\n",
    "    \n",
    "    Why to reduce “Square of errors” and not just the errors?\n",
    "3) Β1 explains change in Y with a change in X  by one unit. In other words, if we increase the value of ‘X’ by one unit then what will be the change in value of Y\n",
    "\n",
    "    Will correlation coefficient between ‘X’ and ‘Y’ be same as Β1?\n",
    "    \n",
    "    \n",
    "![images.png](image/LinearRegression_line.png)\n",
    "\n",
    "__our mottto is not to get a line of perfect fit but to get line is is best fit which has least amount error.__\n",
    "\n",
    "### Model Evaluation Metrics\n",
    "\n",
    "    RMSE\n",
    "        1) less amount of error that line will be best line.\n",
    "        2) it compares a predicted value and an observed or known value.\n",
    "       Algo: \n",
    "           1) Plotting the data points.\n",
    "           2) Try to fit 1000 lines.\n",
    "           3) Calculate RMSE of 1000 lines.\n",
    "           4) Finally, find a min RMSE Value.\n",
    "    R-Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assuption \n",
    "\n",
    "We make a few assumptions when we use linear regression to model the relationship between a response and a predictor. These assumptions are essentially conditions that should be met before we draw inferences regarding the model estimates or before we use a model to make prediction.\n",
    "\n",
    "__Assumption #1:__ Your _dependent variable_ should be measured at the continuous level. Examples of such continuous variables\n",
    "\n",
    "    include height (measured in feet and inches), \n",
    "    temperature (measured in oC), \n",
    "    salary (measured in US dollars), \n",
    "    revision time (measured in hours), \n",
    "    intelligence (measured using IQ score), \n",
    "    reaction time (measured in milliseconds), \n",
    "    test performance (measured from 0 to 100), \n",
    "    sales (measured in number of transactions per month)\n",
    "    \n",
    "    \n",
    "__Assumption #2:__ Your _independent variable_ should be measured at the continuous or categorical level.  In case you are unsure, examples of categorical variables include gender (e.g., 2 groups: male and female), ethnicity (e.g., 3 groups: Caucasian, African American and Hispanic), physical activity level (e.g., 4 groups: sedentary, low, moderate and high), and profession (e.g., 5 groups: surgeon, doctor, nurse, dentist, therapist). In this guide, we show you the linear regression procedure and Stata output when both your dependent and independent variables were measured on a continuous level.\n",
    "\n",
    "\n",
    "__Assumption #3:__ THere should be liner relationship depedent varaible(response) and independent (predictor) varaibles.\n",
    "there are a number of ways to check whether a linear relationship exists between your two variables, we suggest creating a scatterplot using Stata, where you can plot the dependent variable against your independent variable. You can then visually inspect the scatterplot to check for linearity. \n",
    "\n",
    "__Assumption #4:__ There should be no significant outliers. Outliers are simply single data points within your data that do not follow the usual pattern (e.g., in a study of 100 students' IQ scores, where the mean score was 108 with only a small variation between students, one student had a score of 156, which is very unusual, and may even put her in the top 1% of IQ scores globally). The following \n",
    "\n",
    "__Assumption #5:__ Target value should be _normally distributed_\n",
    "    \n",
    "        if target variable is not normally distributed.\n",
    "            1) log transformation or\n",
    "            2) square root transformation.\n",
    "        Can bring into normal distribution.\n",
    "             \n",
    "\n",
    "__Assumption #6:__  Multicollinearity occurs when independent variables in a regression model are correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results\n",
    "        \n",
    "          if __multicolllinearity__ is present, it become diffcult to find out which variable is actually contributing to predict the response variable.\n",
    "          The idea is that you can change the value of one independent variable and not the others. However, when independent variables are correlated, it indicates that changes in one variable are associated with shifts in another variable. The stronger the correlation, the more difficult it is to change one variable without changing another. It becomes difficult for the model to estimate the relationship between each independent variable and the dependent variable independently because the independent variables tend to change in unison.\n",
    "          \n",
    "If I have more than indepdent variable they should not correlated among them self. There should not releation ship between indepdent variable. If there is we will not find a effort one variable to other depdent variable.\n",
    "\n",
    "          \n",
    "__What is a Variance Inflation Factor?__ A variance inflation factor(VIF) detects multicollinearity in regression analysis. Multicollinearity is when there’s correlation between predictors (i.e. independent variables) in a model.\n",
    "- 1 = not correlated.\n",
    "- Between 1 and 5 = moderately correlated.\n",
    "- Greater than 5 = highly correlated.\n",
    "          \n",
    "Colinearity is the state where two variables are highly correlated and contain similiar information about the variance within a given dataset. To detect colinearity among variables, simply create a correlation matrix and find variables with large absolute values.\n",
    "\n",
    "Multicolinearity on the other hand is more troublesome to detect because it emerges when three or more variables, which are highly correlated, are included within a model. To make matters worst multicolinearity can emerge even when isolated pairs of variables are not colinear.\n",
    "\n",
    "__Steps for Implementing VIF__\n",
    "\n",
    "1) Run a multiple regression.\n",
    "2) Calculate the VIF factors.\n",
    "3) Inspect the factors for each predictor variable, if the VIF is between 5-10, multicolinearity is likely present and you should consider dropping the variable.\n",
    "\n",
    "        VIF Factor\tfeatures\n",
    "        0\t5.1\t    Intercept\n",
    "        1\t1.0\t    dti\n",
    "        2\t678.4\tfunded_amnt\n",
    "        3\t678.4\tloan_amnt\n",
    "\n",
    "As expected, the total funded amount for the loan and the amount of the loan have a high variance inflation factor because they \"explain\" the same variance within this dataset. We would need to discard one of these variables before moving on to model building or risk building a model with high multicolinearity.\n",
    "\n",
    "\n",
    "\n",
    "__Assumption #7:__ THe error term must have constant variance. THis phenomenon is know as homoscedasticity. THe presence of non-constant variance is referred to hetroscedasticity. THe error term must be normally distributed.\n",
    "\n",
    "\n",
    "Example - Heteroscedasticity often occurs when there is a large difference among the sizes of the observations. A classic example of heteroskedasticity is that of income versus expenditure on meals. As one's income increases, the variability of food consumption will increase. A poorer person will spend a rather constant amount by always eating less expensive food; a wealthier person may occasionally buy inexpensive food and at other times eat expensive meals. Those with higher incomes display a greater variability of food consumption.\n",
    "\n",
    "Imagine you are watching a rocket take off nearby and measuring the distance it has traveled once each second. In the first couple of seconds your measurements may be accurate to the nearest centimeter, say. However, 5 minutes later as the rocket recedes into space, the accuracy of your measurements may only be good to 100 m, because of the increased distance, atmospheric distortion and a variety of other factors. The data you collect would exhibit heteroscedasticity.\n",
    "\n",
    "![images.png](image/Assumption.png)\n",
    "\n",
    "\n",
    "__Example :__<br> \n",
    "\tUnderstand the data.<br>\n",
    "\tLoad data<br>\n",
    "\tEDA<br>\n",
    "\n",
    "        o\tDistribution of Feature (We are checking distribution of data)\n",
    "        o\tRelation between Dependent variable and Independent variables.\n",
    "        o\tcorrelation between independent variables, correlation should not exist.\n",
    "        \n",
    "\tIf there are no/less relation <br>\n",
    "\n",
    "        o\tTV is highest correlation with sales, Higher the value, higher is relation 0.78, highly correlated.\n",
    "        o\tIf there is good relation between independent variable (TV and Radio) you might have to remove one of the variable.\n",
    "\tYou can drop the column<br>\n",
    "\tYou can club the column <br>\n",
    "\n",
    "    Very Important \n",
    "    Normalization \n",
    "    Scaling\n",
    "    standardization  \n",
    "\n",
    "If your data is not in distribution, try to bring in normal distribution, whether it’s used Normalization /standardization.\n",
    "If the scale is different, that you have to bring into same scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions in Regression\n",
    "\n",
    "Regression is a parametric approach. ‘Parametric’ means it makes assumptions about data for the purpose of analysis. Due to its parametric side, regression is restrictive in nature. It fails to deliver good results with data sets which doesn’t fulfill its assumptions. Therefore, for a successful regression analysis, it’s essential to validate these assumptions.\n",
    "\n",
    "So, how would you check (validate) if a data set follows all regression assumptions? You check it using the regression plots (explained below) along with some statistical test.\n",
    "\n",
    "Let’s look at the important assumptions in regression analysis:\n",
    "\n",
    "    * There should be a linear and additive relationship between dependent (response) variable and independent (predictor) variable(s). A linear relationship suggests that a change in response Y due to one unit change in X¹ is constant, regardless of the value of X¹. An additive relationship suggests that the effect of X¹ on Y is independent of other variables.\n",
    "    * There should be no correlation between the residual (error) terms. Absence of this phenomenon is known as Autocorrelation.\n",
    "    * The independent variables should not be correlated. Absence of this phenomenon is known as multicollinearity.\n",
    "    * The error terms must have constant variance. This phenomenon is known as homoskedasticity. The presence of non-constant variance is referred to heteroskedasticity.\n",
    "    * The error terms must be normally distributed.\n",
    "    \n",
    "    \n",
    "#### What if these assumptions get violated ?\n",
    "\n",
    "__1) Linear and Additive:__  If you fit a linear model to a non-linear, non-additive data set, the regression algorithm would fail to capture the trend mathematically, thus resulting in an inefficient model. Also, this will result in erroneous predictions on an unseen data set.\n",
    "\n",
    "How to check: Look for residual vs fitted value plots (explained below). Also, you can include polynomial terms (X, X², X³) in your model to capture the non-linear effect.\n",
    "\n",
    "\n",
    "__2. Autocorrelation:__ The presence of correlation in error terms drastically reduces model’s accuracy. This usually occurs in time series models where the next instant is dependent on previous instant. If the error terms are correlated, the estimated standard errors tend to underestimate the true standard error.\n",
    "\n",
    "If this happens, it causes confidence intervals and prediction intervals to be narrower. Narrower confidence interval means that a 95% confidence interval would have lesser probability than 0.95 that it would contain the actual value of coefficients. Let’s understand narrow prediction intervals with an example:\n",
    "\n",
    "For example, the least square coefficient of X¹ is 15.02 and its standard error is 2.08 (without autocorrelation). But in presence of autocorrelation, the standard error reduces to 1.20. As a result, the prediction interval narrows down to (13.82, 16.22) from (12.94, 17.10).\n",
    "\n",
    "Also, lower standard errors would cause the associated p-values to be lower than actual. This will make us incorrectly conclude a parameter to be statistically significant.\n",
    "\n",
    "How to check: Look for Durbin – Watson (DW) statistic. It must lie between 0 and 4. If DW = 2, implies no autocorrelation, 0 < DW < 2 implies positive autocorrelation while 2 < DW < 4 indicates negative autocorrelation. Also, you can see residual vs time plot and look for the seasonal or correlated pattern in residual values.\n",
    "A rule of thumb is that test statistic values in the range of 1.5 to 2.5 are relatively normal. Values outside of this range could be cause for concern.\n",
    "\n",
    "__3. Multicollinearity:__ This phenomenon exists when the independent variables are found to be moderately or highly correlated. In a model with correlated variables, it becomes a tough task to figure out the true relationship of a predictors with response variable. In other words, it becomes difficult to find out which variable is actually contributing to predict the response variable.\n",
    "\n",
    "Another point, with presence of correlated predictors, the standard errors tend to increase. And, with large standard errors, the confidence interval becomes wider leading to less precise estimates of slope parameters.\n",
    "\n",
    "Also, when predictors are correlated, the estimated regression coefficient of a correlated variable depends on which other predictors are available in the model. If this happens, you’ll end up with an incorrect conclusion that a variable strongly / weakly affects target variable. Since, even if you drop one correlated variable from the model, its estimated regression coefficients would change. That’s not good!\n",
    "\n",
    "How to check: You can use scatter plot to visualize correlation effect among variables. Also, you can also use VIF factor. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. Above all, a correlation table should also solve the purpose.\n",
    "\n",
    "__4. Heteroskedasticity:__ The presence of non-constant variance in the error terms results in heteroskedasticity. Generally, non-constant variance arises in presence of outliers or extreme leverage values. Look like, these values get too much weight, thereby disproportionately influences the model’s performance. When this phenomenon occurs, the confidence interval for out of sample prediction tends to be unrealistically wide or narrow.\n",
    "\n",
    "How to check: You can look at residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern (shown in next section). Also, you can use Breusch-Pagan / Cook – Weisberg test or White general test to detect this phenomenon.\n",
    "\n",
    " \n",
    "\n",
    "__5. Normal Distribution of error terms:__ If the error terms are non- normally distributed, confidence intervals may become too wide or narrow. Once confidence interval becomes unstable, it leads to difficulty in estimating coefficients based on minimization of least squares. Presence of non – normal distribution suggests that there are a few unusual data points which must be studied closely to make a better model.\n",
    "\n",
    "How to check: You can look at QQ plot (shown below). You can also perform statistical tests of normality such as Kolmogorov-Smirnov test, Shapiro-Wilk test.\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Regression Plots\n",
    "\n",
    "![images.png](image/Interpretation_of_Regression_Plots.png)\n",
    "\n",
    "\n",
    "__1. Residual vs Fitted Values__\n",
    "\n",
    "This scatter plot shows the distribution of residuals (errors) vs fitted values (predicted values). It is one of the most important plot which everyone must learn. It reveals various useful insights including outliers. The outliers in this plot are labeled by their observation number which make them easy to detect.\n",
    "\n",
    "There are two major things which you should learn:\n",
    "\n",
    "If there exist any pattern (may be, a parabolic shape) in this plot, consider it as signs of non-linearity in the data. It means that the model doesn’t capture non-linear effects.\n",
    "If a funnel shape is evident in the plot, consider it as the signs of non constant variance i.e. heteroskedasticity.\n",
    "Solution: To overcome the issue of non-linearity, you can do a non linear transformation of predictors such as log (X), √X or X² transform the dependent variable. To overcome heteroskedasticity, a possible way is to transform the response variable such as log(Y) or √Y. Also, you can use weighted least square method to tackle heteroskedasticity.\n",
    "\n",
    " \n",
    " \n",
    "__2. Normal Q-Q Plot__\n",
    "\n",
    "This q-q or quantile-quantile is a scatter plot which helps us validate the assumption of normal distribution in a data set. Using this plot we can infer if the data comes from a normal distribution. If yes, the plot would show fairly straight line. Absence of normality in the errors can be seen with deviation in the straight line.\n",
    "\n",
    "If you are wondering what is a ‘quantile’, here’s a simple definition: Think of quantiles as points in your data below which a certain proportion of data falls. Quantile is often referred to as percentiles. For example: when we say the value of 50th percentile is 120, it means half of the data lies below 120.\n",
    "\n",
    "Solution: If the errors are not normally distributed, non – linear transformation of the variables (response or predictors) can bring improvement in the model.\n",
    "\n",
    "__3. Scale Location Plot__\n",
    "\n",
    "This plot is also used to detect homoskedasticity (assumption of equal variance). It shows how the residual are spread along the range of predictors. It’s similar to residual vs fitted value plot except it uses standardized residual values. Ideally, there should be no discernible pattern in the plot. This would imply that errors are normally distributed. But, in case, if the plot shows any discernible pattern (probably a funnel shape), it would imply non-normal distribution of errors.\n",
    "\n",
    "Solution: Follow the solution for heteroskedasticity given in plot 1.\n",
    "\n",
    "__4. Residuals vs Leverage Plot__\n",
    "\n",
    "It is also known as Cook’s Distance plot. Cook’s distance attempts to identify the points which have more influence than other points. Such influential points tends to have a sizable impact of the regression line. In other words, adding or removing such points from the model can completely change the model statistics.\n",
    "\n",
    "But, can these influential observations be treated as outliers? This question can only be answered after looking at the data. Therefore, in this plot, the large values marked by cook’s distance might require further investigation.\n",
    "\n",
    "Solution: For influential observations which are nothing but outliers, if not many, you can remove those rows. Alternatively, you can scale down the outlier observation with maximum value in data or else treat those values as missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When Do You Need to Standardize the Variables in a Regression Model?\n",
    "Standardization is also called Normalization and Scaling. \n",
    "__Standardization / Scaling__\n",
    "\n",
    "The concept of standardization comes into picture when continuous independent variables are measured at different scales. It means these variables do not give equal contribution to the analysis. For example, we are performing customer segmentation analysis in which we are trying to group customers based on their homogenous (similar) attributes. A variable called 'transaction amount' that ranges between $100 and $10000 carries more weightage as compared to a variable i.e. number of transactions that in general ranges between 0 and 30. Hence, it is required to transform the data to comparable scales. The idea is to rescale an original variable to have equal range and/or variance.\n",
    "\n",
    "__Methods of Standardization / Normalization__\n",
    "\n",
    "There are main four methods of standardization. They are as follows -\n",
    "\n",
    "1. Z score\n",
    "\n",
    "Z score standardization is one of the most popular method to normalize data. In this case, we rescale an original variable to have a mean of zero and standard deviation of one.\n",
    "\n",
    "$$ Z = \\frac{x - mean}{std.dev} $$\n",
    "\n",
    "A value of 1 implies that the value for that case is one standard deviation above the mean, while a value of -1 indicates that a case has a value one standard deviations lower than the mean.\n",
    "\n",
    "2. Min-Max Scaling\n",
    "\n",
    "It is also called 0-1 scaling because the standardized value using this method lies between 0 and 1.\n",
    "\n",
    "The formula is shown below -\n",
    "\n",
    "$$ \\frac{x-min(x)}{max(x)-min(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of regression coefficients\n",
    "Let us consider an example where the dependent variable is marks obtained by a student and explanatory variables are number of hours studied and no. of classes attended. Suppose on fitting linear regression we got the linear regression as:\n",
    "__Marks obtained = 5 + 2 (no. of hours studied) + 0.5(no. of classes attended)__\n",
    "\n",
    "Thus we can have the regression coefficients 2 and 0.5 which can interpreted as:\n",
    "\n",
    "1) If no. of hours studied and no. of classes are 0 then the student will obtain 5 marks.<br>\n",
    "2) Keeping no. of classes attended constant, if student studies for one hour more then he will score 2 more marks in the examination. <br>\n",
    "3) Similarly keeping no. of hours studied constant, if student attends one more class then he will attain 0.5 marks more.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples \n",
    "\n",
    "__Problem & Dataset__\n",
    "\n",
    "Our objective in this problem will be to train a model that accurately predicts the profits of a food truck.\n",
    "The first column in our dataset file contains city populations and the second column contains food truck profits in each city, both in 10,000s. Here are the first few training examples:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Both x and y are one dimensional arrays, because we have one feature (population) and one target variable (profit) in this problem. Therefore we can conveniently visualize our dataset using a scatter plot:'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.loadtxt('data/food_truck_data.txt', delimiter=\",\")\n",
    "x = data[:, 0] # city populations\n",
    "y = data[:, 1] # food truck profits\n",
    "'''Both x and y are one dimensional arrays, because we have one feature (population) and one target variable (profit) in this problem. Therefore we can conveniently visualize our dataset using a scatter plot:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEcCAYAAADpzeJvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnXm8HFWZ97+/hE1QEphACKCCoiwyEAmSAIooKoi+iI4y4IaiA4pJboiooEKuOvPKoCYkMOgo8AIjI8s4igKyuCDiJGESZJOILIICSQhCwhpMcp/3j1NF161b3V3dXdXdt/v5fj716a5Tp049XbfueeqcZzkyMxzHcRynUcZ0WgDHcRxndOIKxHEcx2kKVyCO4zhOU7gCcRzHcZrCFYjjOI7TFK5AHMdxnKZwBeLURdLHJFmV7W1tlsUkDdY4/mANWV/c2igykhZJ+nkT5+2WknutpOWSbpA0Q9IWTcozQdKgpL2aOb9IJJ0s6YhOy+E0x0adFsAZVXwAeDhVdncnBKnBe4FNE/vnAmOBEzojTiEMAtcR/l8nAgcDXwdmSnqHmf2pwfYmAHOA+4A7ihOzKU4GrgJ+0mE5nCZwBeI0wm1mdl+nhaiFmf0uuS/pKWAjM1uU53xJm5rZC6UI1zz3p+T/oaRvA78FLgP264xYTr/jU1hOYUjaVdKPJK2W9Hw0dXNYRr3DJC2M6qyR9GNJu6bqjJX0z9GUzXOSbpT0uoLlPUPSekl7SvqFpGeAi6NjKyR9J1V/s2gq6ZRU+RRJP5H0RPSblkk6uc61/1nSC5I+0IzsZnY3cAbwBkkHJtr9qKRfS1ol6WlJSyV9MHF8N2BZtPsfiemxo6Pjh0u6Nvr9z0q6U9JMScP6imha8/aozpro+3GpOm+L/m7PRNvVknZPHF9BGFF9IiHHsHvudDc+AnEaYayk5DNjZrYBQNL2wM3A08B0YA3wGeBqSe82s59F9Q4DrgZ+Cfwj8FLgq8DNkiab2SNR24PAF4G5wPXAvpQzzSHgSuC7wD8DGxo6WXoj8HNCpzwAPALsGm1Z9TcC/h14P/BOM/tl05LDNQQlciBhNAKwM3ApYXoK4C0ERbGJmV0IPAgcHdUZJEyNAdwbfb4KuBY4C/gbYXRzBrB1VB9JhwAXEP42swn9yB7A+MTvfB9wBfAj4IOEacRTgZsk7WVmy4HDgRsIz83Xo1NXtnA/nHZjZr75VnMDPgZYxnZzos43gfXALomyscA9wK2JsiWEzmqjRNnOwDpgbrS/FfAM8J2UHF+IrjvYgOw3JuVMHTsjau+EjGMrMq6/WVT/lETZLcADwGY1ZFhEUDKbAz+N2p6cQ/bdout9uMrxcdHxeVWOjyF07v8BLM7bbqKeovO/BqxMlH8ZeLTGeWOAvwDXpMq3BlYDZ6Tu83mdfsZ9a27zKSynEd4LvCGxfSJx7CBgkSVsJBZGJz8AJkvaMvIa2ge4zMzWJ+r9ifAG/eao6O+BLYDLU9e/tNif8yI/auYkSeMJ9+FiM1tbp/p4ghLZDTjAzG5r5pppEaLPF73KJO0u6XJJjxIU+jrgw1QZEY1oUNpR0vmS/hydu46gMLaNfi8EpTlJ0oXRlNeWqWZeB+wIfF/SRvEGPAX8L+FZcXoAVyBOI9xlZksS2z2JY1sDyzPOWUHo6LaKNtWot3X0fVL0mZ7OKGN6Y8jMHmvy3L+LPtOeaVm8CpgKXGVmDzR5vTQvjz6Xw4sK7QaCkvoc8EaCgruEMHqqSdTJXw28DfgKwdvrDcA3oiqbAZjZ9cAxwKsJ04p/lXRdwka1bfR5CRUlFG9vo3LfnFGO20CcongC2C6jfDvCG/ITwFD0vVq9v0bfYwUzEfh9os7EQiQdTrWYkLXAJqmydMcXy7tDjuvcSphKukDS82b2xfwiVuVd0efN0eebIlmONLMlcSVJG+dsb3dgL+ADZvZfifNHGPrN7FLgUkkvA94KnElQPjtRuS+fBW7KuE690ZozSvARiFMUvwamSdopLpA0lmAo/52ZPW1mzwJLgQ9Ex+J6rwQOiNqAEJvwLHBU6hpHlyb9SB4C9kyVvSu5Y2arCdM5H5W0KXUws4uAY4HPSzqjFeEk7UGwCS0ys4VR8ebR57pEvW0JxuoksZvyS1LlWedvShhtZBL9Xa8EzgdeGU1n3Qk8CuyeGrHG210pWdJyOKMEH4E4RTGPYGy/QdIcwnz3icBrGd7xnkZ4U71K0rkEL6yvELy2vgWhY5Y0D/iSpKcJXlhpm0vZXAqcK+lfo+vvQ+j808wGfgH8NpL5UWAXQuc5O13ZzL4vaQPBM2qsmX0uhyyvljSN4JQQBxIeF10rqVR/Q1C8/y7pq8CWwOmEqb8dE/UeJvx9PiTpHuA54H6C4n4UODPhtvtZgjfWi0TKb0uCwl8OvILwt15kZk9FdaYDV0jaHPghYVSyHcFj7I9mdk7U3N3AWyQdDjwGPGZmf85xT5xuoNNWfN+6f6PihbVLnXq7Aj8mKIO1BO+jwzLqHQYsBJ6P6l4J7JqqM5bgVrsiqncjwVW0aC+s9VWOjSV4H/2Z0ClfHf2+YV5YUd03EFxq1xA647uB2Ynji4Cfp845ivCm/60assfeUvH2QnQ/bgBmAFtknHMocHt0z+4FPh39zrWpeh8A/hDJYMDRUfm+0d/mOYIn1WkE5WDAdlGdIyMZVkQy/ZngBj0xdY03AT8Dnoyehz8B/wnsl6jz9wQHiueia3yn2v3wrfs2RX9Ex3Ecx2kIt4E4juM4TdF2BSLp5ZJ+FaV7+L2kgah8UNIjkm6LtrThz3Ecx+ki2j6FJWkSMMnMbo1cAJcS5lSPAp4xs2+2VSDHcRynKdruhWUhB87y6PvTkpaRz4/ecRzH6SI6akSPYgZuIvjbzyZ4+zxFyJf0WTN7MuOc44HjAbbYYospu+22W5ukdRzH6Q2WLl36uJlt02o7HVMgkl5K8CP/FzP7b0kTgccJrnxfI0xzHVerjX333deWLFlSq4rjOI6TQtJSM9u31XY64oUVpVb4IXCJmf03gJmtNLMNZjYEfA9fJMdxHKer6YQXlghpD5aZ2dxE+aREtfcCd6XPdRzH6RrSszd9GFPXiVQmBwIfAe6UFKe0/iJwjKTJhCmsBxnda1g7jtPLDA7C6tUwbx5IQXmcdBKMHx+O9Qmd8MK6mco6BkmuabcsjuM4DWMWlMf8+WF/3rygPObPh4GBcFxZXVzv4ckUHcdxGkEKSgOC0ogVycBAZUTSJ4zqXFjuheU4TscwgzEJM/LQ0KhRHqPaC8txHGdUE9s8kpx0Ut8Z0l2BOI7jNEKsPGKbx9BQ+Jw/v++UiNtAHMdxGkEK3lZJm0dsExk/ftRMYxWB20Acx3GaIe1tNYq8rzpuA5G0oyQfwTiO05+klcUoUR5FkkuBRGt1fDixfxVhGcsVklrWYo7j9AEeud1z5B2BfAy4H0DSocD+wMHAFYT1lh3HcaozODjcwBwbovsoarsXyatAtgMejr4fDlxhZjcBc4EpZQjmOE6PkIzcjpVI7MW0erWPREYxeW0YTwA7An8BDgVOi8oFjC1BLsdxegWP3O5Z8o5Afgx8P7J9bAtcG5VPBu4rQzDHcXqIpBKJceUx6smrQGYBFwCPAIeZ2dNR+SuB88oQzHGcHsIjt3uSXFNYZvY34F8yyr9RuESO4/QW6cjtZPZa8JHIKCaXApH0JuAAgjHdgJXAb6PU7I7jONUpM3J7FAfz9QI1I9ElbQf8CJgKPEBQHCLYQV4FLAaONLOV5Ys6Eo9Ed5xRRNGdvS/q1DTtikT/N2AIeI2Z7WJmB5rZAWa2C/AaYH1Ux3EcpzZFRm67a3BXUG8K6x3AQWZ2f/qAmd0vaRZwYxmCOY7jVMVdg7uCeiOQF4Ataxx/WVTHcRynvbhrcMepp0CuAC6SdKSkLeJCSVtIOhL4f8DlZQroOI6TSTXX4KGhzsjTh9RTILOBXxEUyVOSnpH0NPBUVParqI7jOE77SNo8Jk+GDRsqizpNmQJz5nRawr6gpg3EzJ4HPi7pZIIn1sTo0ArgFjP7a8nyOY7jjESCceOC8rjtNpg9G+bOhV//Ouy/+c3u0tsGfEEpx3FGL0NDQXnERnRwQ3oOinLjratAJE0EjicjkBA4z8xWtCpEs7gCcRwHMxiTmI0fGnLlUYe2xIFImgrcA3ySkAfrGkIixUeismWS9mtVCMdxnKbwHFsdpV4cyALgUjP7VNZBSd+O6kwrWjDHcZyaeI6tjlNPgewFHFvj+Hzgd8WJ4ziOk5Myc2w5uainQFYA+wF/qHJ8P4I9JDeSXg5cTLCnDAHfNbP5krYGLgN2Ah4EjjKzJxtp23GcPmNwcLi3VaxEXHm0hXoK5Czgu5L2AW4gKAsjdP5vB04AvtDgNdcDnzWzWyW9DFgq6QbCuuu/MLMzJJ0CnNJE247j9BtF5thyGqJeHMh8SauBk4AZhEy8EJTIncAJZnZxIxc0s+XA8uj705KWATsA7wEOjqpdRMix5QrEcRynS6m7HoiZXURIZ7I5lUDClWb2XKsXl7QT8HpCWviJkXLBzJZL2rbV9h3HcZzyyLukLZHCeAL4a0HK46XAD4FZZvZUA+cdL2mJpCWrVq1qVQzHcRynSeoqEEmHS/qFpGcICuRJSc9GZe9s5qKSNiYoj0vM7L+j4pWSJkXHJwGPZZ1rZt81s33NbN9tttmmmcs7juM4BVAvkPDjwJXAo8CngMOAdxKM548AP5b0sUYuKEnA+cAyM5ubOPQTKi7Dx0bXdRzHcbqUejaQU4EBMzs349j3JS0EvgRc2MA1DwQ+Atwp6bao7IvAGcDlkj4B/Bn4QANtOo7jOG2mngJ5BfCLGsd/CcytcXwEZnYzFW+uNIc00pbjOI7TOerZQJYBn6hx/LiojuM4jtNn1BuBnAxcJelQ4HpGBhK+Fnh3qRI6juM4XUm9QMJfSNob+AxwEEFxQEhxciMh3cgfS5XQcRzH6UryBBL+ERhogyyO4zjOKCJ3IKHjOI7jJGlZgUi6W9LcKDGi4ziO0ycUMQK5g7DcbbWU747jON1JeuVCX8mwIeraQOphZkcDSJrQujiO4zhtYnAQVq+urB8Sr3A4fnw45tSlMBuImT1eVFujBn97cZzRiVlQHvPnV9ZQj5fDXb3a/5dzUncEImkicDxhmmo7QhzISuC3wHlmtqJUCbsVf3txnNFLcvnb+fMr66gnl8d16lIvmeJU4B7gk4TkidcA10bfPwksk7Rf2UJ2Hf724jijn6QSiXHl0RD1RiALgEvN7FNZByV9O6ozrWjBuhp/e3Gc0U/84pfkpJP8f7gB6tlA9iKsi16N+cDexYkzivC3F8dpjG6yGSZnDQYGYGgofCZnFZy61FMgK4BaU1T7Eewh/Ue1txd/8BxnJIODw/8/4v+fTtkLpWCvTM4azJsX9seP9xfBnNSbwjoL+K6kfYAbGJlM8QTgC6VK2I2k317mzavsg49EHCdJ0mYIw/9fBgbC8U78vwwODr92rET8fzc39ZIpzpe0GjgJmEFlHQ8D7gROMLOLyxWxC6n29gL+9uI4abrZZpi+tv/vNoQs55SLpM2BidHuSjN7rjSpcrLvvvvakiVLOidA+s2pU29SjjMaMIMxiVnzoSH/f+kQkpaa2b6ttpM7kNDMnjOzP0Vbx5VHV+BvL46TD7cZ9iQtRaJL2lnSNUUJ4zhOD+IeTz1Lq7mwtgQOLUIQx3F6FLcZ9iw1FYikz9c5f1KBsjiO06u4x1NPUm8EcgbwMPBCleObFCuO4zg9i9sMe456CuQh4AtmdnnWQUmTgaWFS+U4juN0PfWM6LcC+9Q4blRiQxynv+im1ByO0wHqjUC+Cmxe4/gyYPfixHGcUYKn83ec2iMQM7vdzBbWOP43M7uneLEcp4vxdP6OAxSwpK3j9B3dnJrDcdpI7lQm3UjHU5k4/Y2n5nBGKW1PZVIUki6Q9JikuxJlg5IekXRbtB3ebrkcpyE8NYfjtF+BABcCh2WUzzOzydHm6VGc8mnWi8pTczgO0AEbiJndJGmndl/XcYbRiheVp+ZwHKABBSJpInAgsC2pkYuZnVuALNMlfRRYAnzWzJ6sIsfxwPEAr3jFKwq4rNN3FLHAkafmcJx8RnRJHwAuBsYCTxACCGPMzLZv6KJhBHKVme0Z7U8EHo/a/RowycyOq9eOG9GdpklOQ8W4F5XTJxRlRM+rQO4DrgRONbO/tXzRlALJeyyNKxCnJdyLyulT2u2FNQn4tyKURxaSkll93wvcVa2u4xSCe1E5TsvkVSDXAVOKuKCkHwALgV0lPSzpE8CZku6UdAfwFsIa7I5TDu5F5TiFkNeI/hPgG5J2Be4E1iUPNuJ2a2bHZBSfn/d8x2kZ96JynELIawMZqnHYzGxscSLlx20gVUh7EeXxKupH/D45fUq7bSAvqbHVytbrtJvBweHTMPF0Tb9niM0KGvQFjhynJXIpEDN7odZWtpBOTjxLbDauVB2nFKraQCSdCFxgZmuj71UpKJDQaRXPEjuSIoIGHcfJpKoNRNJyYE8z+2v0vRoNBxIWhdtAquDxDcPxoEHHGUbpNhAzm2Rmf018r7Z1RHk4VfD4hpEkR2Yxrjwcp2U6kY3XKQuPb8jGlarjlIIrkF6iWnzDwED/xje4UnWc0vAlbXsNzxI7HA8adJzS8CVtnf7AgwYd50VG7ZK2jtMRPGjQcQonlwKRdLekrTPKx0m6u3ixnLbS7NKuTmP4fXZ6jLwjkN3ItpdsBry6OHGclkh2SGYj97PwKO324PfZ6UFqGtElHZ7YPUTSmsT+WOBtwJ/LEKynKWM+PrnG91e+Ak9GKwJvtRXMmZO93rdHabcHv89Oj1LPC+uq6NOAS1LHDHgYmFW0UD1NsqOXKm+i6c69EZIdVPyGu2BB+Jw5E2bNCvvpzspTn7QHv89Or2JmVTdgU8I01XJgx2g/3sbWOrcd25QpU2xUMTRkNjAQJpcGBrL3i2g7a6vV/tDQ8LqtyOFUx++z0yUAS6yAPrimDcRCtt21FlKWPGzDs/BuKFe19SDJwL7580O+qngao9U30ax0HUmqte9R2u3B77PTg1RVIJJOlLRZ4nvVrX3i9gBl5WXK6qCSZHVWHqXdHvw+Oz1KLRvIacBlwNroezUM8HTuean2JtqKEkl2UDNnhrKkDQSGG3CTNhCP0i4fv89Oj+KR6O0k/Saa9sZpRYk044WVlMujtMvH77PTJRQViV5rPZDngFea2SpJ5wKfN7NnWr1gkYw6BQLleGHFJDuk+O+a3PfOynEc2qNAniUsKPUnSRuA7cxsVasXLJJRqUDA30Qdx+koRSmQWjaQxcB/SboFEHCmpOezKpqZG9IbwfMyOY7TA9RSIB8BTgF2IRjKXwX8LaPe6DWiOI7jOE1TVYGY2SPADHhxffT3WbTEreM4juPkWlDKzCaVLYjjlIrbnRyncHKvByLpEEnXS3pY0l8kXSfprWUK5ziF4JlwHacU8q4H8lHgOuCvwNeBM4AngeskfaSRC0q6QNJjku5KlG0t6QZJ90afWzXSpuNUJZloMlYicezN6tUeBe44LZArkFDSPcB3zGxeqnw2cLyZ7Zb7gtJBwDPAxWa2Z1R2JvCEmZ0h6RRgKzP7Qr22Rq0br9NekkojxjPhOn1M6XEgqYu9ALzOzO5Lle8C3GVmmzV0UWkn4KqEArkHONjMlkuaBNxoZrvWa8cViJMbs5C8MmZoyJWH07e0e030h4G3ZJS/JTrWKhPNbDlA9LlttYqSjpe0RNKSVau6Kq7R6VY8E67jlEJeBXIWcLaksyUdI+loSecAC4AaOcSLx8y+a2b7mtm+22yzTTsv7YxGPBOu45RGXjfesyU9DnwW+HhU/AfgY2Z2WQFyrJQ0KTGF9VgBbTaPu3z2Dp4J13FKo64CkbQRcBBwrZn9oCQ5fgIcS/DuOha4sqTr1KfMZIdOZxgczF7KN608/MXBcRqi7hSWma0HrgHGF3FBST8AFgK7RjElnyAojrdLuhd4e7Tfftzls3epl3/MY0Ucp2FyTWEBdwE7A39q9YJmdkyVQ4e02nbLJKc35s+vuH26y2dvk3xxgJHrtPhIxHEyyevG+w5CAOGXgKXAs8njZvZcKdLVoTQ3Xnf57D88VsTpI9rtxnst8HrCVNYK4OnU1ju4y2d/UtZa9Y7Tw+SdwjqcfkjbXmvJWfAOpZcpY616x+lx8rrxXlu2IF2Bu3z2J/7i4DhNUVOBSNoU+L/AkcDGwM+B2Wa2ug2ydYa8Lp9O7+AvDo7TFPVGIHOAzwCXA88DRwGbA0eXLFdn8SVnu4d2xWb4i4PjNEw9BXIU8Ekz+z6ApIuAGyWNMbOh0qVzup8yO/h2B3X6i4PjNEQ9L6xXADfGO2b2P8AQsH2JMjmjhTKD7zyo03G6nnojkI2AF1Jl6wj2EKdfyBplQLnBdx7U6ThdT81AQklDwE8ZrkTeC1xPIpjQzI4qS8Ba+HogbaDWNNKcOeUH33lQp+MUTrsCCS8HngM2JLb/Ap5KlfUHaWXbbdMoRctXbxoJyg2+86BOx+luzGzUblOmTLG2MWeO2cCA2dBQ2B8aCvtz5rRPhlqUJV/cTui2wxZfp9axWu3V2s+6btxmet9xnKYAllgBfXDeVCb9TbcbdMuUr1qKD2h8oaZGjO7VYjMGBjw2w3G6hSK0UKe2to5AhobMZs4c/rY9c2b3vAk3MxqIz6u3X63dRkY9zY4o8o5YHMfJDQWNQDquBFrZ2j6FlaVAypjCyuo083SkQ0PD5avX2dZTAPU6/Q0bhssWf88zLdWIknMcp1CKUiA+hZUHM3jySViwYHj5ggWh3OoYr9P7tcia5tl//7Aly+Kpn3RZkloGZ8sx7VVrGum222D27Iq8Q0MVmWpNS3nGW8fpHfJoGWDTGse2K0KTNbO1bQSSNX1VbRqrFWN21ht/8rrxteI6U6eGsg0bKmWTJ5udfnr+6aE8I4L0fvp6WZ/V2vERiON0HNo5hUVYo3xMRvkkYFkRgjSzdd0UVhGeQ1md7MyZ2deOywYGgtJIdt5xJ19PcTU67VVLznpKyL2qHKcraLcCWQxcmCqbBNwDXF6EIM1sXWlEL+ItO6tTr1ZW61p5DOityJqWqZ4S6nZXaMfpE9qtQCYAdwNzo/3tI+VxWdbIpF1bW6ewGnl7rtbZN3qtWiOQZCxGqyOIZkYEjY5AkufV2nccp3SKUiB5F5R6XNKhwM1RepP/A/wO+JD1Q1beRtaLsMjonWTWrPC51Va1Ew2ajVzYaNasivF+5kw466xKHcswkOddRa+VNTCSck6eHAzqyc9aCzF5xlvH6Rlq5sIaUVnaFfgN8Au6QHk0nQvLrLkU5PXOMxvZ4cNIBVDrWlm5p2KFtHBhpWzWLFi8OGxpxZLnOnl/Uz05x42DNWtg7tzglRXvl5Vy3XGclikqF1ZVBSJpFWSug/5SwuJSL+bAMrNtWxWkGZpSIGWvMTE4GFx7Ybjb79SpFQVQj6xOHUaWfeUrcO21oe2zzgrlsWI57LDyO/BYzmqfjuN0JUUpkFpTWF9utfGuw6y8FOTxuXFshtlwBZJWHrXe/PNM80ghG24cn5Kcglq8GKZNa+73NDIiSctbTX7HcXqTIgwpndqaMqKXEYsQexdt2BD216+vuNRmXaNIb6Qif083ekm50d1xCod2RqJLOlLSuzLKD5d0ROFarUxqJQdshuSoZsoUOP102G67YFCeMAG+/OWKYTmO2C4y8WHe6O50u1n73ZYwsswVDx3HaZ08Wga4E3hnRvk7gDuK0GTNbE2PQKZOHf7GHrvJNvumfdppZhMmDG9z7NjwmYzQPv30igz1Rg3NpD2v1lbekUU3RYp74KHjlAZtjgN5Htgpo3wn4NkiBInaezBSVrfl+YENK5Cs1CBZqUIabbNaPESyE46nt5LnZcVvNJLlNk8nW0QMS6foJoXmOD1EuxXICuCtGeWHAI8VIYhVFMiEvPWbGoHEKUnSgXlTpzbfMW3YMNLmUasTrtYxbtgQ5IpHSHk6/DzKppGcV93WYXeTQnOcHqHdCuS70ahgp0TZzoRgwvOKEMTapUDM8kdw55lGyjsCSXfw6emt5Ge9yPOs35JHzrxKrVumjLpRoTlOD1CUAsmbzv3zwDrgj5LulXQvIZXJBuBzDZhc6mHA9ZKWSjo+q4Kk4yUtkbRk1apVzV+pXurzvAZcKQTPTZgwvHyzzcJn0oBuNjwCfOnSSmp0CJ8DAyGmI47riJk3L8R9ZMn0la+MlClJXK/W7+22FQBjmRtZ8dBxnPaSV9MAYwgpTE4DTgfeTcF5sIDto89tgduBg2rVb9mNtwi7QXpEkcyKO3lyMLBXs1/En8k37PXrq+fDmjGj8RFCMzaQWvvtpBvdih2nB6DXVyQEBoGTa9Vpegorz0p8jUyfpONA4lTqSa+rLLKuMWFCRVFMnmy2115mO+xQ2d9vv/oypffj9UFGY0fcTQrNcXqEohRIrlxYkk6sM4o5t7nxz7BrbEEY0Twdfb8B+KqZXVvtnKZzYUFlOim9n0x1AjAmMcs3NFR9Kqdae7Wun5yi+eY3YdIkePxxeMlL4LWvhdtvD3X32ivIcdttIc9VMsI9lilL/mSqlnHjhk911ZPPcZyepR2pTJKcltrfGNgaWAusBlpWIMBE4EcKndpGwH/WUh4tk5UuxKwSTJelWGfNqp6kME/6kWSnHdtOBgbC58knw/LlFSUSKw+AO+4In3FyxiQnnTQykWGeVC1ZdpJmkio6jtO35E3nPildJunlwHnAgpFnNI6ZPQDsXURbOS+Y3WHOmxe+J9/yk1l14zqNdq5ZI4M1a2DLLYd3+p/+NHzta9XbWbBgZOr0X/+6YoCfOzfUmz+/0mbSMJ5XtiITTDqO05u0Mv8FTAHuKWIurZmtVBtI2uU1r92gmpttLUN2co3xWlvWGujVXIXzxk50o/uu4zilQjcY0YHJwFNFCNLMVrgXVr3OuV5nWksx1TPKr19fXXH83d9V4kLiaPlakeyNxk54vIXj9BVtVSDA4antXcAngTuA64oQpJmtpUDCdIc5dWrwfkp21DNmDI8KT6cjqdZmLffgrJGCDriBAAAXoklEQVRNvWDEgw7Kp4ySyq/R0YRHfDtO31CUAslrRL8qo+wp4JdAhmW3y4ntGLGNAMKiTAsWhM+ZM2HRIrjlFpg+PZSNG1cxVKcD95JtQrb9AUYG8+2/f2gbwrVnzgzd9xVXwIoVlXqvf334TLaTXPY23oeKYb6RZWrNsgMNm7H1OI7TP+TRMsCmqW3jIrRXq1uhI5Cs9CFx7MW6dZVjcXBgsq0kGzZkjzLSI4HktWLbRhz/EV8njt9IjyDy2HDSvzfPvXAbiOP0BbRrBCJpY+B64J/M7I8l6rL2EL9tZ73Bp91k9947eDdtvHHYj9f5gPBmLg33XhoaCmuCJInf5NNpQuJUJfG65osXh/2pU8M2b16I/bDInTg5gohXPUy65CZHC3lcipPHslKYpK/pOI6Toq4CMbN1kvYAhtogT/nEHebMmcM7zKGhSidejRUrKkrloINCWezuO3duUB6xe+3SpWHKKxmTEV8//oyVSDJYceHCkfWyppIaURL1qKeQHMdxMsibTPES4ONlCtJRzOA3vwkKZGAANmwIiiIZzBdz++3DEx7GCf7Gjh2uPMaMGZmMMKtDzrI9pGlHR16kQnIcpy/Ia0QHmC7pbcAS4NnkATP7fKFSlYlF0ebJoMDZsyud/9y5ofzNb85WIADf+lalg00b42PlAbXf5GtNpcXteifuOE4Xk1eBTAXujr7vkzpWP5lWN1HLW2ru3IrdYdGi6m1MmQK33hraSo8YZs+utBNfr5ocbntwHGcUkyuZYrfScjLFrESJyZHBfvvBQw/BypWhzowZIW3IHXeEY9OmVVKLJG0ekyfDEUdku/tmyeE5qBzHaSNFJVOsaQORtEHStq1epOuoFvcQd97xyGDhQthpp3B88mTYais4+OCwHydDjPNSxSOPeH/NmuyEjGnc9uA4zmillo8vwfNq2yL8hcvYWk5lEqcFSe+bVaLO0zEbWfU8DYjjOKMI2rykbe8QjzDiCHCojCQWLQrTTvEIJc5Em15eNt6Pp8Fi20WMG8Adx+kD8hjRj5L0VK0KZnZxQfK0hzlz4MknKzEcixdXAgSnTQvrfixYADvsAE88MTI+ZNq0sC1eDIceGqarkngaEMdx+oA8CuQMantaGTC6FEgcxCcNd8GF4euATJgAZ5898vxbbgkbwAsvVNbicFdcx3H6iDwK5FVm9ljpkrSbrISKSeJI9QMOqB6hPnNmmA5785sr8SPxdNa4ca48HMfpaerZQEavj289sjyx0kiV1CJZnHVWsJlsuWXwwoq9uObODdNavpqf4zg9TL0RSG++QidjPZLL1SZJ72cxa1ZQFj/9acWGEke2Z61D7jiO00PUDCSU9P+AmWb2dPtEyk9LgYTxOuBz51amqfbeG97znmBgP/tsmDgxBBFWy4sFw9cnj5UI1F+H3AMIHcfpEEUFEvZvJDpUOu3BwaA05s2Dr34Vrr0Wnn8+2Ddi997f/x722CMsAhUvNhUT20vGjq2UxZHtWcTKK1Yw8Yho/Hif9nIcp3TaEone8yTX14i9sp58MoxG7rgjjDzMgrJ49tngujtuXPhMc8ABw/fjyPY0cTLH+fMrdeLptNWr80WvO47jdAE+AklPI8UdetoGMmNG+LzllupeWTNmVBRDbF+JFVP6ukl3X6g/5eU4jlMQPgJplcHB4aMEszCyOOCAYBdJIwW7yNq12e1tu+3wFO8zZwZFU2/99BhXHo7jjDL6U4FkTSNNm1YZXeyboZgXLAhK4YgjwtRWkhkzYOedQ52ka/DixZVpqeRIr1YyR8dxnFFCVTdeSRfkbcTMjitGnDZRbU0QCNHnSW+qJENDQSGkPbIk+J//qbjvxu3FxnWoGMnnzPGFpBzH6QlqxYFsk9o/iJCd985of0/CCOamEuQqnzgde5IZM7JTl8Scc07le2zfiDv/OIAwK7I9qTCg+IWk3CXYcZwOkMuILulU4PXAx83s2ahsC+B84E4z+5dChJEOA+YDY4HzzOyMWvVbMqIPDYWVBZOjjb32Ct5XSSZMgMcfH16WNI7H01HjxoXo82qpUdJG8qI6fXcJdhynQdptRJ8JDMbKAyD6/jVgRqtCAEgaC/wb8E5gD+AYSXsU0fYIzIavgx6TVh4QlMeECdXbSqYuiUcZQ0OVCPeY9NRUEQtJuUuw4zgdJK8CeSmwfUb5JGDzgmTZD7jPzB4ws78BlwLvKajt4SRXHVy6dPixiRNh/frhZY8/Hupu2BA+Y2N53EGPGTN8WiqLMozk8fTXwEBQGmPGDLet+DSW4zhlkmfVKeBC4C/A0cBO0XY08BBwYRErWwHvJ0xbxfsfAc6pdU5TKxImyVpNcPr0kSsQTp48fIXCgQGzOXNGtjc0VHvFw7JWKhwaGi6vr4boOE4NaPOKhJ8Gfhopkvuj7SLgauDE1tUYkJ24ccQru6TjJS2RtGTVqlXNXy2exkpOO82YEQzlsctuPOKI1zyP7RTz5mXbF6Tho5vYThKPEpo1ktf7He4S7DhOB2goEj0ynL+a0NnfZwmbSMuCSPsT7CyHRvunApjZ16ud03IkepwDK2kQnzatksa9FaN0OzyjkjaPtEuwT2M5jlOFoozoeRaUSrKB4Mpr0fci+V/gNZJ2Bh4hTJF9sOBr1GfatOEjhXgE0WhHXISRPM81inYJdhzHyUkuBSJpI+DrwHRgE8II5AVJZwNfMrN1rQpiZuslTQeuI7jxXmBmv2+13RoXDJ5KCxZUOt44B1Z6HY9u7ogHB0fK6iMPx3HaQN4RyJnAMcCngJujsjcRlMoY4OQihDGza4BrimirLtWi0QcGKsvTjhbaMdpxHMdJkVeBfBA4LurgY+6XtAo4j4IUSNvJikafOzcYzD0Qz3EcpyZ5vbDGETyv0twPjC9OnDYzNAQ/+cnwsilTmg/ES9d3TyjHcXqYvArkdkI0epoBoErmwS6nWjR6vN/oNFZWeviTTvJRjOM4PUteBfJ54FhJf5R0kaQLJd0DfBj4XHnilUitaPQjjghR3XnxlCKO4/QhuWwgZnaTpNcCnwF2I3hhXQGca2aPlihfuQwOhmms2bOHl69Z01jcRi2DvHtEOY7To/iStkUG4pkNH7kMDbnycByn62h7IKGkiYQRyB6EQMLfA982s5WtCtExigzEq5ZSxEcgjuP0KHkDCQ8ErgVWAguj4g8DsyUdamYLq57c7RQRiFdrJAOuRBzH6UnyjkC+CfwA+JSZDQFIGgN8B/gWcEA54rWJVgPxPKWI4zh9SN4VCZ8HJpvZPany3YDfmdlLSpKvJi3bQIrGl5Z1HGcU0O4VCdcAO2eU7wysblWItlNWwJ+nFHEcp4/Iq0AuBc6X9CFJO0vaSdKHge8RprZGDx7w5ziOUwh5bSCfJ8R+XJA4Zx3wbeCUEuQqh2TAH4x02/UpJ8dxnNw0uqDU5gxfUOq5sgTLQ1M2kKTHVIwH/DmO00cUZQOpqUAk7QXcFXtedRtNG9E94M9xnD6mXUb03wETEhe9WtKkVi/aUXwNccdxnEKop0DSr+UHAR1x2S2EdMDf0FD4TCZBdBzHcXLR6JrooxsP+HMcxymMegrEoi1dNnrxNcQdx3EKoZ4CEfB9SS9E+5sB35M0zPvKzI4oQ7jS8IA/x3GclqmnQC5K7X+/LEEcx3Gc0UVNBWJmH2+XII7jOM7oooF1Wx3HcRyngisQx3EcpylcgTiO4zhN4QrEcRzHaQpXII7jOE5TuAJxHMdxmqIrFIikQUmPSLot2g7vtEyO4zhObbopF9Y8M/tmp4VwHMdx8tEVIxDHcRxn9NFNI5Dpkj4KLAE+a2ZPZlWSdDxwfLT7jKR7WrjmBODxFs4vG5evebpZNnD5WsXla41di2ikoSVtW7qQ9HNgu4xDXwIWEW62AV8DJpnZcW2QaUkRq3KVhcvXPN0sG7h8reLytUZR8rVtBGJmb8tTT9L3gKtKFsdxHMdpka6wgaSWyX0vcFenZHEcx3Hy0S02kDMlTSZMYT0InNCm6363TddpFpevebpZNnD5WsXla41C5GubDcRxHMfpLbpiCstxHMcZfbgCcRzHcZqiLxSIpAcl3RmlSVmScVySFki6T9IdkvZpo2y7JlK43CbpKUmzUnUOlrQmUef0kmW6QNJjku5KlG0t6QZJ90afW1U599iozr2Sjm2TbN+Q9Ifob/cjSeOrnFvzOShRvlypeiQdJume6Dk8pY3yXZaQ7UFJt1U5tx337+WSfiVpmaTfSxqIyjv+/NWQrSuevxrylff8mVnPbwTD/IQaxw8HfgYImAYs7pCcY4EVwCtT5QcDV7VRjoOAfYC7EmVnAqdE308B/jXjvK2BB6LPraLvW7VBtncAG0Xf/zVLtjzPQYnyDQIn5/jb3w+8CtgEuB3Yox3ypY5/Czi9g/dvErBP9P1lwB+BPbrh+ashW1c8fzXkK+3564sRSA7eA1xsgUXA+JRrcbs4BLjfzB7qwLVfxMxuAp5IFb8HuCj6fhFwZMaphwI3mNkTFjIJ3AAcVrZsZna9ma2PdhcBOxZ5zUaocu/ysB9wn5k9YGZ/Ay4l3PNCqSWfJAFHAT8o+rp5MbPlZnZr9P1pYBmwA13w/FWTrVuevxr3Lg9NPX/9okAMuF7SUoVUKGl2AP6S2H+Y/De+SI6m+j/v/pJul/QzSa9rp1ARE81sOYQHFdg2o0433MfjCKPJLOo9B2UyPZriuKDK9Es33Ls3ASvN7N4qx9t6/yTtBLweWEyXPX8p2ZJ0xfOXIV8pz1+/KJADzWwf4J3AZyQdlDqujHPa6t8saRPgCOCKjMO3Eqa19gbOBn7cTtkaoKP3UdKXgPXAJVWq1HsOyuLbwKuBycBywjRRmo4/g8Ax1B59tO3+SXop8ENglpk9lfe0jLLC72E12brl+cuQr7Tnry8UiJk9Gn0+BvyIMFxL8jDw8sT+jsCj7ZHuRd4J3GpmK9MHzOwpM3sm+n4NsLGkCW2Wb2U8rRd9PpZRp2P3MTKYvhv4kEWTumlyPAelYGYrzWyDmQ0B36ty3Y4+g5I2At4HXFatTrvun6SNCR3gJWb231FxVzx/VWTrmucvS74yn7+eVyCStpD0svg7weCVTpXyE+CjCkwD1sTD5TZS9e1P0nbR/DSS9iP83f7aRtkg3KPYq+VY4MqMOtcB75C0VTRMfkdUViqSDgO+ABxhZs9VqZPnOShLvjypev4XeI2knaPR6NGEe94u3gb8wcwezjrYrvsXPefnA8vMbG7iUMefv2qydcvzV0O+8p6/sjwCumUjeBXcHm2/B74UlX8K+FT0XcC/EbwQ7gT2bbOMmxMUwrhEWVK+6ZHstxOMdAeULM8PCEPddYQ3k08Afwf8Arg3+tw6qrsvcF7i3OOA+6Lt422S7T7C/O1t0fadqO72wDW1noM2yfcf0XN1R/RPOSktX7R/OMFz5v52yheVXxg/b4m6nbh/byRMndyR+Hse3g3PXw3ZuuL5qyFfac+fpzJxHMdxmqLnp7Acx3GccnAF4jiO4zSFKxDHcRynKVyBOI7jOE3hCsRxHMdpClcgzgiirKEnd1qOMpF0o6RzCmjnQklXFSFTnesUIq/jFIkrkD5D0kRJ8yXdL+mFKM3zz1Ipnt8AnJs4xyS9v4BrXxi1ZZLWSXpA0jejwKquRiGlvmVkABgAPtwGEd4HnNpKA5ImSfpPhdTjGyRdWKXeP0i6O3o+7pb03hxt/72kX0t6PnqmTo+DX/O2GwXyDkp6NGrnRnUm75uTE1cgfUSUYO1WQtbSU4G9CBHIVwPfieuZ2SqrElFbAD8npJ1+FfBl4ETgmyVdq3TMbI2ZrW7DdZ6wkGG1FTYFHgfOYGQSQAAk7U9IZ3IJIXfSJcAVkqZWa1TSloTMtysJLx8zgc8Bsxts9/PAZ4EZUTuPATfEEdxOF1JGNKlv3bkB1xDy27w049hWie8PEq0fEH23xPYgsBOwgVTEPvBPhA5qkyrXv5DUuiaE3DzLE/sHETq3tYQOaV6yPeBGgrKbDzwZbd8AxmTJnzrvnBr7Hyakc3ia0HFdQUjVTfR7LbVdmPWbCJ30WZHsawmZA96YOH5wdP4h0e98DlhCtI5Djb9dWt4HCQr434GnCFHln2vgWbgq/g2p8ssIKdGTZT8HflCjrU9HMrwkUfZl4BF4MVi5ZruEbBDLSURAAy+J/h4nJMpOBx4CXiCsnXNxp/+v+nnzEUifIGlrwtoI51iUmDGJhfUTsnhD9PlPhJHDG8zsQcI//3GpuscB/2FhPYG8PA9sHMm4AyEV9u8Iqag/QcgR9vXUOR8ijJ73B04Ajgdm0RqbAHOAvQlJ8SZQyU32F+Afou+vI9yHgSrtnAn8I+FevJ6QQuJajVxf5uuEhZH2IaSxuSQ95ZODk6L29yEsZHRm9KbfCvsD16fKrgMOqHPOb8zs+dQ52xOUb552dwa2S9aJ2rspriPpH4CTCaPW1xD+Trfk+E1OSbgC6R92IbzlLWvkJDNbFX1dbWYrEvvfA46RtBmApN0Jqzmen7ftKDHkBwm5jSB0DMuBE81smZldRehkp0vaPHHqcmCmmf3BzC4njEBm0wJmdoGZXWNhQZ1bCG/Vb5K0o5ltoLII02PRfViT8Xu2iM77gpldbWbLCDnNVgKfSVU/zcx+ZWZ/AL4K7Ebja1dcb2bnmNl9ZnY2ISfTIQ22kWa7SN4kK6PyRs+Jj+Vpd7tEWbU6ryT87a83sz+b2RIzc8eCDuIKpH9o9O22HlcCfyMYdyG8cd9iZvUyjB4m6RlJa4GFhDfMGdGx3YGFFtJOx9xMGB3skihbZGbJJG4LgR2iufimkLSPpCslPSTpacK0EsArGmjm1YTR1G/jgkj5LCQsLZrkjsT3OG121iJJtbgjtf9oE21kkU6Qp4yyPOeky/O0W6vOFcBmwJ8knS/pA5I2rSOXUyKuQPqHewn/iLsX0ZiZrQMuBo5TWEviI+QbfdxEMKLuCmxmZu+zsD4C1O6oGsn6OcRIhblxtcrRyOE6gj3iI4Rpu3gp1E0auG5Wp0mVsnUZxxr9f1yX2rcm2kizgpGjjW0ZOTLIcw6J8+q1uyL6rFrHzP5CeG5OINhcvgUsHQ1efL2KK5A+wcyeIHSS0xVWLBuGpPE1Tl8HjM0o/x7wFsLU08sI6yjX47loyuWhSAkluZuwdG/yuXwjYaRzf6JsaspeMA141Cqrw60i2CkAiKbZdqsh024Em8cXzeymaFop/SYf23Wy7kPMfVG9NyauPZYw/393jfO6iYXA21Nlbwf+p845b4qnMxPnPEow9udp908EJfJinai9NyWvbWZro+nBkwiK/nXAgXV/lVMKrkD6ixMJb8lLouH/rpJ2k/RpRk6HJHkQOERhYasX11M2sz8Sppi+AfyX5V96tBrnEgyv50raXdK7CC6n59hwt+LtgbMi+d9PcBmdlzj+S+BDUezG64ALqDECAf5M8OqZLulV0XW/lqrzEOEN/12StslSwmb2LGH50DMkHR7Zhb4NTCQRV9NJJE2WNBnYEtg62k9Or80H3irp1OjZOJXwknBWjWb/kzB6u1DSnpLeR7BdzU1MNdZsN6p3FnCKpPdJ2pPg4fZM1D6SPibpk1HMyc7AxwkvN9XWcHfKptNuYL61dyO8mZ8NPEDoNB8leD69M1HnQRJusMD/IfyTrgMeTLX3UULHelCOa19Iyo03o07sxvsCFTfeTRPHbyS48Z4DrCa48X4LGJuosyXBg2oNwZX0ROq78f4jYZSzluDZc2j0uw5O1DmNYMQdIp8b7wtUd+OdkCjbKSqrupBZhrzD/kZZdaq0k3ZHtoy/6fuBPxBGU8uA96WODxL1+YmyvydMT66N7tEcIhfeBtpV1PbyqJ1fA3smjh9JGMmsBp4luF2/u9P/U/28+YJSTktI+gJhVbvXtul6NwJ3mdn0dlzPGYmkiwir2r2j07I4nWWjTgvgjE6iKZzdCPEQ/9JhcZw2Edme3krr7sJOD+A2EKdZziG4q/6WEA3t9AEWeLkF+5fT5/gUluM4jtMUPgJxHMdxmsIViOM4jtMUrkAcx3GcpnAF4jiO4zSFKxDHcRynKf4/MB7vsKtsD50AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, marker=\"x\", c=\"red\")\n",
    "plt.title(\"Food Truck Dataset\", fontsize=16)\n",
    "plt.xlabel(\"City Population in 10,000s\", fontsize=14)\n",
    "plt.ylabel(\"Food Truck Profit in 10,000s\", fontsize=14)\n",
    "plt.axis([4, 25, -5, 25])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothesis function for Linear Regression__\n",
    "Now we need to come up with a straight line which accurately represents the relationship between population and profit. This is called the hypothesis function and it’s formulated as\n",
    "\n",
    "\\begin{equation}\n",
    "hθ(x)=θ^Tx=\\mathtt{θ}_0 + \\mathtt{θ}_1\\mathtt{x}_1+\\mathtt{θ}_2\\mathtt{x}_2+…+\\mathtt{θ}_n\\mathtt{x}_n \n",
    "\\end{equation}\n",
    "\n",
    "where x corresponds to the feature matrix and θ corresponds to the vector of model parameters.\n",
    "Since we have a single feature $\\mathtt{x}_1$, we’ll only have two model parameters $\\mathtt{θ}_0$ and $\\mathtt{θ}_1$ in our hypothesis function:\n",
    "$$ \\mathtt{h}_θ(x)=θ^Tx=\\mathtt{θ}_0 + \\mathtt{θ}_1\\mathtt{x}_1 $$\n",
    "\n",
    "As you may have noticed, the number of model parameters is equal to the number of features plus 1. That’s because each feature is weighted by a parameter to control its impact on the hypothesis hθ(x). There is also an independent parameter θ0 called the intercept term, which defines the point where the hypothesis function intercepts the y-axis as demonstrated below:\n",
    "\n",
    "The predictions of a hypothesis function can easily be evaluated in Python by computing the cross product of x and θT. At the moment we have our x and y vectors but we don’t have our model parameters yet. So let’s create those as well and initialize them with zeros:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.zeros(2)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we have to make sure that the matrix dimensions of x and $\\mathtt{θ}_T$ are compatible with each other for cross product. Currently x has 1 column but $\\mathtt{θ}_T$ has 2 rows. The dimensions don’t match because of the additional intercept term $\\mathtt{θ}_0$.\n",
    "\n",
    "We can solve this issue by prepending a column to x and set it to all ones. This is essentially equivalent to creating a new feature $\\mathtt{x}_0$=1. This extra column won’t affect the hypothesis whatsoever because $\\mathtt{θ}_0$ is going to be multiplied by 1 in the cross product.\n",
    "\n",
    "Let’s create a new variable X to store the extended x matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finally, we can compute the predictions of our hypothesis as follows:'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.ones(shape=(len(x), 2))\n",
    "X[:, 1] = x\n",
    "'''Finally, we can compute the predictions of our hypothesis as follows:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = X @ theta\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Of course, the predictions are currently all zeros because we haven’t trained our model yet.__\n",
    "#### Cost Function\n",
    "\n",
    "The objective in training a linear regression model is to minimize a cost function, which measures the difference between actual y values in the training sample and predictions made by the hypothesis function $\\mathtt{h}_θ(x)$.\n",
    "\n",
    "Such a cost function can be formulated as;\n",
    "\n",
    "$$j(θ) =\\frac{1}{(2m)} \\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "where m is the number of training examples.\n",
    "\n",
    "This function is otherwise called the \"Squared error function\", or \"Mean squared error\". The mean is halved $\\left(\\frac{1}{2}\\right)$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\\frac{1}{2} $\t  term. \n",
    "\n",
    "\n",
    "__We can measure the accuracy of our hypothesis function by using a cost function.__ This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![image](image/costfunction.png)\n",
    "\n",
    "\n",
    "\n",
    "_Recap._ We're closing this problem as, find me the values of theta zero and theta one so that the average, the 1 over the 2m, times the sum of square errors between my predictions on the training set minus the actual values of the houses on the training set is minimized. So this is going to be my overall objective function for linear regression.\n",
    "\n",
    "\n",
    "$$J(\\theta_0, \\theta_1) = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left ( \\hat{y}_{i}- y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x_{i}) - y_{i} \\right)^2$$\n",
    "\n",
    "#### Cost Function - Intuition I\n",
    "\n",
    "![image](image/CostIntuition.png)\n",
    "\n",
    "It turns out that two key functions we want to understand. The first is the hypothesis function, and the second is a cost function. So, notice that the hypothesis, right, H of X. For a face value of theta one, this is a function of X. So the hypothesis is a function of, what is the size of the house X. In contrast, the cost function, J, that's a function of the parameter, theta one, which controls the slope of the straight line. Let's plot these functions and try to understand them both better. Let's start with the hypothesis. On the left, let's say here's my training set with three points at (1, 1), (2, 2), and (3, 3). Let's pick a value theta one, so when theta one equals one, and if that's my choice for theta one, then my hypothesis is going to look like this straight line over here. And I'm gonna point out, when I'm plotting my hypothesis function. X-axis, my horizontal axis is labeled X, is labeled you know, size of the house over here. Now, of temporary, set theta one equals one, what I want to do is figure out what is j of theta one, when theta one equals one. So let's go ahead and compute what the cost function has for. You'll devalue one. Well, as usual, my cost function is defined as follows, right? Some from, some of 'em are training sets of this usual squared error term. And, this is therefore equal to. And this. Of theta one x I minus y I and if you simplify this turns out to be. That. Zero Squared to zero squared to zero squared which is of course, just equal to zero. Now, inside the cost function. It turns out each of these terms here is equal to zero. Because for the specific training set I have or my 3 training examples are (1, 1), (2, 2), (3,3). If theta one is equal to one. Then h of x. H of x i. Is equal to y I exactly, let me write this better. Right? And so, h of x minus y, each of these terms is equal to zero, which is why I find that j of one is equal to zero. So, we now know that j of one Is equal to zero. Let's plot that. What I'm gonna do on the right is plot my cost function j. And notice, because my cost function is a function of my parameter theta one, when I plot my cost function, the horizontal axis is now labeled with theta one. So I have j of one zero zero so let's go ahead and plot that. End up with. An X over there. Now lets look at some other examples. Theta-1 can take on a range of different values. Right? So theta-1 can take on the negative values, zero, positive values. So what if theta-1 is equal to 0.5. What happens then? Let's go ahead and plot that. I'm now going to set theta-1 equals 0.5, and in that case my hypothesis now looks like this. As a line with slope equals to 0.5, and, lets compute J, of 0.5. So that is going to be one over 2M of, my usual cost function. It turns out that the cost function is going to be the sum of square values of the height of this line. Plus the sum of square of the height of that line, plus the sum of square of the height of that line, right? ?Cause just this vertical distance, that's the difference between, you know, Y. I. and the predicted value, H of XI, right? So the first example is going to be 0.5 minus one squared. Because my hypothesis predicted 0.5. Whereas, the actual value was one. For my second example, I get, one minus two squared, because my hypothesis predicted one, but the actual housing price was two. And then finally, plus. 1.5 minus three squared. And so that's equal to one over two times three. Because, M when trading set size, right, have three training examples. In that, that's times simplifying for the parentheses it's 3.5. So that's 3.5 over six which is about 0.68. So now we know that j of 0.5 is about 0.68.[Should be 0.58] Lets go and plot that. Oh excuse me, math error, it's actually 0.58. So we plot that which is maybe about over there. Okay? Now, let's do one more. How about if theta one is equal to zero, what is J of zero equal to? It turns out that if theta one is equal to zero, then H of X is just equal to, you know, this flat line, right, that just goes horizontally like this. And so, measuring the errors. We have that J of zero is equal to one over two M, times one squared plus two squared plus three squared, which is, One six times fourteen which is about 2.3. So let's go ahead and plot as well. So it ends up with a value around 2.3 and of course we can keep on doing this for other values of theta one. It turns out that you can have you know negative values of theta one as well so if theta one is negative then h of x would be equal to say minus 0.5 times x then theta one is minus 0.5 and so that corresponds to a hypothesis with a slope of negative 0.5. And you can actually keep on computing these errors. This turns out to be, you know, for 0.5, it turns out to have really high error. It works out to be something, like, 5.25. And so on, and the different values of theta one, you can compute these things, right? And it turns out that you, your computed range of values, you get something like that. And by computing the range of values, you can actually slowly create out. What does function J of Theta say and that's what J of Theta is. To recap, for each value of theta one, right? Each value of theta one corresponds to a different hypothesis, or to a different straight line fit on the left. And for each value of theta one, we could then derive a different value of j of theta one. And for example, you know, theta one=1, corresponded to this straight line straight through the data. Whereas theta one=0.5. And this point shown in magenta corresponded to maybe that line, and theta one=zero which is shown in blue that corresponds to this horizontal line. Right, so for each value of theta one we wound up with a different value of J of theta one and we could then use this to trace out this plot on the right. Now you remember, the optimization objective for our learning algorithm is we want to choose the value of theta one. __That minimizes J of theta one. Right?__ This was our objective function for the linear regression. Well, looking at this curve, the value that minimizes j of theta one is, you know, __theta one equals to one.__ And low and behold, that is indeed the __best possible straight line fit through our data,__ by setting theta one equals one. And just, for this particular training set, we actually end up fitting it perfectly. And that's why minimizing j of theta one corresponds to finding a straight line that fits the data well. \n",
    "\n",
    "__Thus as a goal, we should try to minimize the cost function. In this case, \\theta_1 = 1θ __\n",
    "\n",
    "#### Cost Function - Intuition II\n",
    "\n",
    "Reference : https://www.coursera.org/learn/machine-learning/supplement/9SEeJ/cost-function-intuition-ii\n",
    "Here’s its Python version:\n",
    "\n",
    "        def cost(theta, X, y):\n",
    "            predictions = X @ theta\n",
    "            squared_errors = np.square(predictions - y)\n",
    "            return np.sum(squared_errors) / (2 * len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, X, y):\n",
    "    predictions = X @ theta\n",
    "    squared_errors = np.square(predictions - y)\n",
    "    return np.sum(squared_errors) / (2 * len(y))\n",
    "\n",
    "'''Now let’s take a look at the cost of our initial untrained model:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The initial cost is:', cost(theta, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Algorithm\n",
    "https://www.coursera.org/learn/machine-learning/lecture/8SpIM/gradient-descent\n",
    "\n",
    "algorithm called gradient descent for minimizing the cost function. It turns out gradient descent is a more general algorithm, and is used not only in linear regression. It's actually used all over the place in machine learning. \n",
    "\n",
    "\n",
    "So we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That's where gradient descent comes in.\n",
    "\n",
    "Imagine that we graph our hypothesis function based on its fields $\\theta_0$  and  $\\theta_1$  (actually we are graphing the cost function as a function of the parameter estimates). We are not graphing x and y itself, but the parameter range of our hypothesis function and the cost resulting from selecting a particular set of parameters.\n",
    "\n",
    "We put $\\theta_0$  on the x axis and  $\\theta_1$  on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup.\n",
    "\n",
    "![image](image/gradient-descent.png)\n",
    "We will know that we have succeeded when our cost function is at the very bottom of the pits in our graph, i.e. when its value is the minimum. The red arrows show the minimum points in the graph.\n",
    "\n",
    "The way we do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter α, which is called the learning rate.\n",
    "\n",
    "For example, the distance between each 'star' in the graph above represents a step determined by our parameter α. A smaller α would result in a smaller step and a larger α results in a larger step. The direction in which the step is taken is determined by the partial derivative of $J(\\theta_0,\\theta_1)$. Depending on where one starts on the graph, one could end up at different points. The image above shows us two different starting points that end up in two different places.\n",
    "\n",
    "The gradient descent algorithm is:\n",
    "\n",
    "repeat until convergence:\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)$$\n",
    "\n",
    "where\n",
    "\n",
    "j=0,1 represents the feature index number.\n",
    "\n",
    "there's lot of details in this equation so let me unpack some of it.<br>\n",
    "\n",
    "    1)  First, this notation here, :=, gonna use := to denote assignment, so it's the assignment operator. So briefly, if I write a := b, what this means is, it means in a computer, this means take the value in b and use it overwrite whatever value is a. So this means set a to be equal to the value of b, which is assignment. And I can also do a := a + 1. This means take a and increase its value by one. Whereas in contrast, if I use the equal sign and I write a equals b, then this is a truth assertion.\n",
    "\n",
    "    2) So if I write a equals b, then I'll asserting that the value of a equals to the value of b, right? So the left hand side, that's the computer operation, where we set the value of a to a new value. The right hand side, this is asserting, I'm just making a claim that the values of a and b are the same, and so whereas you can write a := a + 1, that means increment a by 1, hopefully I won't ever write a = a + 1 because that's just wrong. a and a + 1 can never be equal to the same values.\n",
    "\n",
    "    3) This alpha here is a number that is called the learning rate.And what alpha does is it basically controls how big a step we take downhill with creating descent. So if alpha is very large, then that corresponds to a very aggressive gradient descent procedure where we're trying take huge steps downhill and if alpha is very small, then we're taking little, little baby steps downhill\n",
    "    \n",
    "    4) this term here, that's a derivative term.\n",
    "\n",
    "At each iteration j, one should simultaneously update the parameters $\\theta_1, \\theta_2,...,\\theta_n $updating a specific parameter prior to calculating another one on the $j^{(th)}$ iteration would yield to a wrong implementation.\n",
    "\n",
    "Since our hypothesis is based on the model parameters θ, we must somehow adjust them to minimize our cost function J(θ). This is where the gradient descent algorithm comes into play. It’s an optimization algorithm which can be used in minimizing differentiable functions. Luckily our cost function J(θ) happens to be a differentiable one.\n",
    "\n",
    "\n",
    "\n",
    "#### Gradient Descent Intuition\n",
    "\n",
    "let's take the tangent to that point, like that straight line, that red line, is just touching this function, and let's look at the slope of this red line. That's what the derivative is, it's saying what's the slope of the line that is just tangent to the function. Okay, the slope of a line is just this height divided by this horizontal thing. Now, this line has a positive slope, so it has a positive derivative. And so my update to theta is going to be theta 1, it gets updated as theta 1, minus alpha times some positive number.\n",
    "\n",
    "\n",
    "Alpha the the learning, is always a positive number. And, so we're going to take theta one is updated as theta one minus something. So I'm gonna end up moving theta one to the left. I'm gonna decrease theta one, and we can see this is the right thing to do cuz I actually wanna head in this direction. You know, to get me closer to the minimum over there.\n",
    "\n",
    "\n",
    "![image](image/gradient-Intuition.png)\n",
    "\n",
    "It turns out the local optimum, your derivative will be equal to zero. So for that slope, that tangent point, so the slope of this line will be equal to zero and thus this derivative term is equal to zero. And so your gradient descent update, you have theta one cuz I updated this theta one minus alpha times zero. And so what this means is that if you're already at the local optimum it leaves theta 1 unchanged cause its updates as theta 1 equals theta 1. So if your parameters are already at a local minimum one step with gradient descent does absolutely nothing it doesn't your parameter which is what you want because it keeps your solution at the local optimum.\n",
    "\n",
    "\n",
    "__Book Defination__ \n",
    "So here’s how the gradient descent algorithm works in a nutshell:\n",
    "\n",
    "In each iteration, it takes a small step in the opposite gradient direction of J(θ). This makes the model parameters θ gradually come closer to the optimal values. This process is repeated until eventually the minimum cost is achieved.\n",
    "\n",
    "More formally, gradient descent performs the following update in each iteration\n",
    "\n",
    "$$j(θ) := θ_j -α\\frac{1}{(m)} \\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x_j^i$$\n",
    "\n",
    "The α term here is called the learning rate. It allows us to control the step size to update θ in each iteration. Choosing a too large learning rate may prevent us from converging to a minimum cost, whereas choosing a too small learning rate may significantly slow down the algorithm.\n",
    "\n",
    "\n",
    "#### Gradient Descent For Linear Regression\n",
    "What we're going to do is apply gradient descent to minimize our squared error cost function. \n",
    "\n",
    "When specifically applied to the case of linear regression, a new form of the gradient descent equation can be derived. We can substitute our actual cost function and our actual hypothesis function and modify the equation to :\n",
    "\n",
    "\\begin{align*} \\text{repeat until convergence: } \\lbrace & \\newline \\theta_0 := & \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}(h_\\theta(x_{i}) - y_{i}) \\newline \\theta_1 := & \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}\\left((h_\\theta(x_{i}) - y_{i}) x_{i}\\right) \\newline \\rbrace& \\end{align*}\n",
    "\n",
    "where m is the size of the training set, $\\theta_0$ a constant that will be changing simultaneously with $\\theta_1$ and $x_{i}, y_{i}$ are values of the given training set (data).\n",
    "\n",
    "Note that we have separated out the two cases for $\\theta_j$  into separate equations for $\\theta_0$ and $\\theta_1$ and that for $\\theta_1$ we are multiplying $x_{i}$  at the end due to the derivative. The following is a derivation of $∂/{∂θ_j} J(θ)$ for a single example :\n",
    "\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning/supplement/U90DX/gradient-descent-for-linear-regression \n",
    "\n",
    "\n",
    "Here’s a generic implementation of the gradient descent algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha, num_iters):\n",
    "    num_features = X.shape[1]               \n",
    "    theta = np.zeros(num_features)          # initialize model parameters\n",
    "    for n in range(num_iters):\n",
    "        predictions = X @ theta             # compute predictions based on the current hypothesis\n",
    "        errors = predictions - y\n",
    "        gradient = X.transpose() @ errors\n",
    "        theta -= alpha * gradient / len(y)  # update model parameters\n",
    "    return theta  \n",
    "\n",
    "'''Now let’s use this function to train our model and plot the hypothesis function:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = gradient_descent(X, y, 0.02, 600)   # run GD for 600 iterations with learning rate = 0.02\n",
    "predictions = X @ theta                     # predictions made by the optimized model\n",
    "ax.plot(X[:, 1], predictions, linewidth=2)  # plot the hypothesis on top of the training data\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging\n",
    "\n",
    "Our linear fit looks pretty good, right? The algorithm must have successfully optimized our model.\n",
    "\n",
    "Well, to be honest, it’s been fairly easy to visualize the hypothesis because there’s only one feature in this problem.\n",
    "\n",
    "But what if we had multiple features? Then it wouldn’t be possible to simply plot the hypothesis to see whether the algorithm has worked as intended or not.\n",
    "\n",
    "Fortunately, there’s a simple way to debug the gradient descent algorithm irrespective of the number of features:\n",
    "\n",
    "1) Modify the gradient descent function to make it record the cost at the end of each iteration.<br>\n",
    "2) Plot the cost history after the gradient descent has finished.<br>\n",
    "3) Pat yourself on the back if you see that the cost has monotonically decreased over time.<br>\n",
    "\n",
    "Here’s the modified version of our gradient descent function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha, num_iters):\n",
    "    cost_history = np.zeros(num_iters)          # create a vector to store the cost history\n",
    "    num_features = X.shape[1]               \n",
    "    theta = np.zeros(num_features)\n",
    "    for n in range(num_iters):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "        gradient = X.transpose() @ errors\n",
    "        theta -= alpha * gradient / len(y)\n",
    "        cost_history[n] = cost(theta, X, y)     # compute and record the cost\n",
    "    return theta, cost_history                  # return optimized parameters and cost history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s try learning rates 0.01, 0.015, 0.02 and plot the cost history for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "num_iters = 1200\n",
    "learning_rates = [0.01, 0.015, 0.02]\n",
    "for lr in learning_rates:\n",
    "    _, cost_history = gradient_descent(X, y, lr, num_iters)\n",
    "    plt.plot(cost_history, linewidth=2)\n",
    "plt.title(\"Gradient descent with different learning rates\", fontsize=16)\n",
    "plt.xlabel(\"number of iterations\", fontsize=14)\n",
    "plt.ylabel(\"cost\", fontsize=14)\n",
    "plt.legend(list(map(str, learning_rates)))\n",
    "plt.axis([0, num_iters, 4, 6])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the gradient descent algorithm worked correctly for these particular learning rates. Notice that it takes more iterations to minimize the cost as the learning rate decreases.\n",
    "\n",
    "Now let’s try a larger learning rate and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.025\n",
    "num_iters = 50\n",
    "_, cost_history = gradient_descent(X, y, learning_rate, num_iters)\n",
    "plt.plot(cost_history, linewidth=2)\n",
    "plt.title(\"Gradient descent with learning rate = \" + str(learning_rate), fontsize=16)\n",
    "plt.xlabel(\"number of iterations\", fontsize=14)\n",
    "plt.ylabel(\"cost\", fontsize=14)\n",
    "plt.axis([0, num_iters, 0, 6000])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn’t look good… That’s what happens when the learning rate is too large. Even though the gradient descent algorithm takes steps in the correct direction, these steps are so huge that it’s going to overshoot the target and the cost diverges from the minimum value instead of converging to it.\n",
    "\n",
    "Right now we can safely set the learning rate to 0.02, because it allows us to minimize the cost and it requires relatively fewer iterations to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Now that we’ve learned how to train our model, we can finally predict the food truck profit for a particular city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, _ = gradient_descent(X, y, 0.02, 600)    # train the model\n",
    "test_example = np.array([1, 7])                 # pick a city with 70,000 population as a test example\n",
    "prediction = test_example @ theta               # use the trained model to make a prediction\n",
    "print('For population = 70,000, we predict a profit of $', prediction * 10000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Features (multivariate linear regression)\n",
    "\n",
    "Linear regression with multiple variables is also known as \"multivariate linear regression\".\n",
    "\n",
    "We now introduce notation for equations where we can have any number of input variables.\n",
    "\n",
    "\\begin{align*}x_j^{(i)} &= \\text{value of feature } j \\text{ in the }i^{th}\\text{ training example} \\newline x^{(i)}& = \\text{the input (features) of the }i^{th}\\text{ training example} \\newline m &= \\text{the number of training examples} \\newline n &= \\text{the number of features} \\end{align*}\n",
    "\n",
    "The multivariable form of the hypothesis function accommodating these multiple features is as follows:\n",
    "\n",
    "$h_\\theta (x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\cdots + \\theta_n x_n$\n",
    "\n",
    "In order to develop intuition about this function, we can think about $\\theta_0$ as the basic price of a house, $\\theta_1$ as the price per square meter, $\\theta_2$ as the price per floor, etc. $x_1$ will be the number of square meters in the house, $x_2$ the number of floors, etc.\n",
    "\n",
    "# Gradient Descent For Multiple Variables\n",
    "Gradient Descent for Multiple Variables\n",
    "The gradient descent equation itself is generally the same form; we just have to repeat it for our 'n' features:\n",
    "\n",
    "\\begin{align*} & \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)}\\newline \\; & \\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_1^{(i)} \\newline \\; & \\theta_2 := \\theta_2 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_2^{(i)} \\newline & \\cdots \\newline \\rbrace \\end{align*}\n",
    "\n",
    "In other words:\n",
    "\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \\; & \\text{for j := 0...n}\\newline \\rbrace\\end{align*}\n",
    "\n",
    "## Gradient Descent in Practice I - Feature Scaling\n",
    "\n",
    "We can speed up gradient descent by having each of our input values in roughly the same range. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "\n",
    "The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:\n",
    "\n",
    "−1 ≤ $x_{(i)} $≤ 1\n",
    "\n",
    "or\n",
    "\n",
    "−0.5 ≤ $x_{(i)} $ ≤ 0.5\n",
    "\n",
    "These aren't exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.\n",
    "\n",
    "Two techniques to help with this are __feature scaling and mean normalization.__ Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:\n",
    "\n",
    "\n",
    "$$ x_i := \\dfrac{x_i-μ_i}{s_i}$$\n",
    "\n",
    "Where μi is the average of all the values for feature (i) and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation.\n",
    "\n",
    "Note that dividing by the range, or dividing by the standard deviation, give different results. The quizzes in this course use range - the programming exercises use standard deviation.\n",
    "\n",
    "For example, if $ x_i$ represents housing prices with a range of 100 to 2000 and a mean value of 1000, then,$ x_i := \\dfrac{price-1000}{1900}$\n",
    "\n",
    "## Gradient Descent in Practice II - Learning Rate\n",
    "\n",
    "Debugging gradient descent. Make a plot with number of iterations on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α.\n",
    "\n",
    "Automatic convergence test. Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as $10^−3$. However in practice it's difficult to choose this threshold value.\n",
    "\n",
    "![image](image/Learning Rate.png)\n",
    "\n",
    "It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration.\n",
    "\n",
    "To summarize:\n",
    "\n",
    "    If α is too small: slow convergence.\n",
    "    If α is too large: ￼may not decrease on every iteration and thus may not converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features and Polynomial Regression\n",
    "\n",
    "Our hypothesis function need not be linear (a straight line) if that does not fit the data well.\n",
    "\n",
    "We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).\n",
    "\n",
    "For example, if our hypothesis function is $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 $ then we can create additional features based on $x_1$ to get the quadratic function $ h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2$  or the cubic function $ h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1^3 $\n",
    "\n",
    "In the cubic version, we have created new features $x_2$ and $x_3$ where $x_2 = x_1^2 $ and $x_3 = x_1^3$ \n",
    "To make it a square root function, we could do: $ h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 \\sqrt{x_1}$\n",
    "\n",
    "One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important.\n",
    "\n",
    "eg. if $x_1 $ has range 1 - 1000 then range of $ x_1^2 $ becomes 1 - 1000000 and that of $ x_1^3$  becomes 1 - 1000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>CA</th>\n",
       "      <th>FA</th>\n",
       "      <th>Age</th>\n",
       "      <th>CMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast  Fly Ash  Water  Superplasticizer      CA     FA  Age    CMS\n",
       "0   540.0    0.0      0.0  162.0               2.5  1040.0  676.0   28  79.99\n",
       "1   540.0    0.0      0.0  162.0               2.5  1055.0  676.0   28  61.89\n",
       "2   332.5  142.5      0.0  228.0               0.0   932.0  594.0  270  40.27\n",
       "3   332.5  142.5      0.0  228.0               0.0   932.0  594.0  365  41.05\n",
       "4   198.6  132.4      0.0  192.0               0.0   978.4  825.5  360  44.30"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Data/Concrete_Data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1030, 9)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Cement', 'Blast', 'Fly Ash', 'Water', 'Superplasticizer', 'CA', 'FA',\n",
      "       'Age'],\n",
      "      dtype='object')\n",
      "Index(['CMS'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "x = data.iloc[:,0:8]\n",
    "y = data.iloc[:,8:]\n",
    "print(x.columns )\n",
    "print(y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t.shah\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test , y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm = lm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12415357  0.10366839  0.093371   -0.13429401  0.28804259  0.02065756\n",
      "   0.02563037  0.11461733]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cement</td>\n",
       "      <td>0.124154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blast</td>\n",
       "      <td>0.103668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fly Ash</td>\n",
       "      <td>0.093371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Water</td>\n",
       "      <td>-0.134294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Superplasticizer</td>\n",
       "      <td>0.288043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CA</td>\n",
       "      <td>0.020658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FA</td>\n",
       "      <td>0.025630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.114617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         0\n",
       "0            Cement  0.124154\n",
       "1             Blast  0.103668\n",
       "2           Fly Ash  0.093371\n",
       "3             Water -0.134294\n",
       "4  Superplasticizer  0.288043\n",
       "5                CA  0.020658\n",
       "6                FA  0.025630\n",
       "7               Age  0.114617"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lm.coef_)\n",
    "coefficients = pd.concat([pd.DataFrame(x_train.columns),pd.DataFrame(np.transpose(lm.coef_))], axis = 1)\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-34.273527])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To predict the values of y on the test set we use lm.predict( )\n",
    "y_pred = lm.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>6.956941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>17.669987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>2.903714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>-8.330767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>-13.555988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CMS\n",
       "173   6.956941\n",
       "134  17.669987\n",
       "822   2.903714\n",
       "264  -8.330767\n",
       "479 -13.555988"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Errors are the difference between observed and predicted values.\n",
    "y_error = y_test - y_pred\n",
    "y_error.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.622520087740484"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R square can be obbtained using sklearn.metrics ( ):\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running linear regression using statsmodels: \n",
    "- It is to be noted that statsmodels does not add intercept term automatically thus we need to create an intercept to our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'https://www.listendata.com/2018/01/linear-regression-in-python.html#\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels\n",
    "import statsmodels.api as sma\n",
    "import statsmodels.formula.api as sm\n",
    "''''https://www.listendata.com/2018/01/linear-regression-in-python.html#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t.shah\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "X_train = sma.add_constant(x_train) ## let's add an intercept (beta_0) to our model\n",
    "X_test = sma.add_constant(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = sma.OLS(y_train,X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>CMS</td>       <th>  R-squared:         </th> <td>   0.613</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.609</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   161.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 20 Oct 2019</td> <th>  Prob (F-statistic):</th> <td>4.37e-162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:23:31</td>     <th>  Log-Likelihood:    </th> <td> -3090.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   824</td>      <th>  AIC:               </th> <td>   6199.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   815</td>      <th>  BIC:               </th> <td>   6241.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>  -34.2735</td> <td>   29.931</td> <td>   -1.145</td> <td> 0.253</td> <td>  -93.025</td> <td>   24.478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cement</th>           <td>    0.1242</td> <td>    0.010</td> <td>   13.054</td> <td> 0.000</td> <td>    0.105</td> <td>    0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Blast</th>            <td>    0.1037</td> <td>    0.011</td> <td>    9.229</td> <td> 0.000</td> <td>    0.082</td> <td>    0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fly Ash</th>          <td>    0.0934</td> <td>    0.014</td> <td>    6.687</td> <td> 0.000</td> <td>    0.066</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Water</th>            <td>   -0.1343</td> <td>    0.046</td> <td>   -2.947</td> <td> 0.003</td> <td>   -0.224</td> <td>   -0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Superplasticizer</th> <td>    0.2880</td> <td>    0.102</td> <td>    2.810</td> <td> 0.005</td> <td>    0.087</td> <td>    0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>               <td>    0.0207</td> <td>    0.011</td> <td>    1.966</td> <td> 0.050</td> <td> 2.79e-05</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FA</th>               <td>    0.0256</td> <td>    0.012</td> <td>    2.131</td> <td> 0.033</td> <td>    0.002</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Age</th>              <td>    0.1146</td> <td>    0.006</td> <td>   19.064</td> <td> 0.000</td> <td>    0.103</td> <td>    0.126</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 3.757</td> <th>  Durbin-Watson:     </th> <td>   2.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.153</td> <th>  Jarque-Bera (JB):  </th> <td>   3.762</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.165</td> <th>  Prob(JB):          </th> <td>   0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.974</td> <th>  Cond. No.          </th> <td>1.07e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.07e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    CMS   R-squared:                       0.613\n",
       "Model:                            OLS   Adj. R-squared:                  0.609\n",
       "Method:                 Least Squares   F-statistic:                     161.0\n",
       "Date:                Sun, 20 Oct 2019   Prob (F-statistic):          4.37e-162\n",
       "Time:                        17:23:31   Log-Likelihood:                -3090.4\n",
       "No. Observations:                 824   AIC:                             6199.\n",
       "Df Residuals:                     815   BIC:                             6241.\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const              -34.2735     29.931     -1.145      0.253     -93.025      24.478\n",
       "Cement               0.1242      0.010     13.054      0.000       0.105       0.143\n",
       "Blast                0.1037      0.011      9.229      0.000       0.082       0.126\n",
       "Fly Ash              0.0934      0.014      6.687      0.000       0.066       0.121\n",
       "Water               -0.1343      0.046     -2.947      0.003      -0.224      -0.045\n",
       "Superplasticizer     0.2880      0.102      2.810      0.005       0.087       0.489\n",
       "CA                   0.0207      0.011      1.966      0.050    2.79e-05       0.041\n",
       "FA                   0.0256      0.012      2.131      0.033       0.002       0.049\n",
       "Age                  0.1146      0.006     19.064      0.000       0.103       0.126\n",
       "==============================================================================\n",
       "Omnibus:                        3.757   Durbin-Watson:                   2.033\n",
       "Prob(Omnibus):                  0.153   Jarque-Bera (JB):                3.762\n",
       "Skew:                          -0.165   Prob(JB):                        0.152\n",
       "Kurtosis:                       2.974   Cond. No.                     1.07e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.07e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173     61.143059\n",
      "134     53.630013\n",
      "822     22.276286\n",
      "264     27.850767\n",
      "479     51.575988\n",
      "809     18.422829\n",
      "587     30.349921\n",
      "911     24.306373\n",
      "564     28.723397\n",
      "849     22.258975\n",
      "315     35.715836\n",
      "216     28.410082\n",
      "482     57.192237\n",
      "225     34.996770\n",
      "116     53.574541\n",
      "413     31.857952\n",
      "797     60.526751\n",
      "669     33.596762\n",
      "143     61.208316\n",
      "914     39.585952\n",
      "959     41.737767\n",
      "579     25.023221\n",
      "381     46.911193\n",
      "654     16.193706\n",
      "56      70.392998\n",
      "493     48.718059\n",
      "263     38.447670\n",
      "44      37.900187\n",
      "860     37.698172\n",
      "532     27.949321\n",
      "          ...    \n",
      "489     48.922088\n",
      "308     45.077132\n",
      "157     56.839298\n",
      "918     18.123008\n",
      "670     36.723104\n",
      "125     64.486632\n",
      "91      50.764580\n",
      "574     21.227362\n",
      "372     35.535776\n",
      "956     25.365189\n",
      "832     21.896290\n",
      "590     14.493094\n",
      "404     50.935843\n",
      "508     49.303892\n",
      "995     33.838562\n",
      "500     53.101588\n",
      "548     20.843047\n",
      "968     20.293266\n",
      "945     23.161168\n",
      "909     22.727797\n",
      "777     27.587661\n",
      "201     27.921801\n",
      "97      55.592067\n",
      "1008    33.259465\n",
      "72      54.333125\n",
      "247     28.767368\n",
      "207     29.081748\n",
      "672     15.308657\n",
      "191     22.948884\n",
      "378     35.320912\n",
      "Length: 206, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = lm2.predict(X_test) \n",
    "print(y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note that both y_pred and y_pred2 are same. It's just these are calculated via different packages.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6225200877404835\n",
      "0.6071909542477113\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_test = pd.to_numeric(y_test.CMS, errors='coerce')\n",
    "RSS = np.sum((y_pred2 - y_test)**2)\n",
    "y_mean = np.mean(y_test)\n",
    "TSS = np.sum((y_test - y_mean)**2)\n",
    "R2 = 1 - RSS/TSS\n",
    "print(R2)\n",
    "\n",
    "n=X_test.shape[0]\n",
    "p=X_test.shape[1] - 1\n",
    "\n",
    "adj_rsquared = 1 - (1 - R2) * ((n - 1)/(n-p-1))\n",
    "print(adj_rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Detecting Outliers: __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = lm2.get_influence()  \n",
    "resid_student = influence.resid_studentized_external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>CA</th>\n",
       "      <th>FA</th>\n",
       "      <th>Age</th>\n",
       "      <th>Studentized Residuals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.559672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.917354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>1.057443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>0.637504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360.0</td>\n",
       "      <td>-1.170290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast  Fly Ash  Water  Superplasticizer      CA     FA    Age  \\\n",
       "0   540.0    0.0      0.0  162.0               2.5  1040.0  676.0   28.0   \n",
       "1   540.0    0.0      0.0  162.0               2.5  1055.0  676.0   28.0   \n",
       "2   332.5  142.5      0.0  228.0               0.0   932.0  594.0  270.0   \n",
       "3   332.5  142.5      0.0  228.0               0.0   932.0  594.0  365.0   \n",
       "4   198.6  132.4      0.0  192.0               0.0   978.4  825.5  360.0   \n",
       "\n",
       "   Studentized Residuals  \n",
       "0               1.559672  \n",
       "1              -0.917354  \n",
       "2               1.057443  \n",
       "3               0.637504  \n",
       "4              -1.170290  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resid = pd.concat([x_train,pd.Series(resid_student,name = \"Studentized Residuals\")],axis = 1)\n",
    "resid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the absolute value of studentized residuals is more than 3 then that observation is considered as an outlier and hence should be removed. We try to create a logical vector for the absolute studentized residuals more than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>CA</th>\n",
       "      <th>FA</th>\n",
       "      <th>Age</th>\n",
       "      <th>Studentized Residuals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>166.8</td>\n",
       "      <td>250.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>975.6</td>\n",
       "      <td>692.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.161183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cement  Blast  Fly Ash  Water  Superplasticizer     CA     FA  Age  \\\n",
       "649   166.8  250.2      0.0  203.5               0.0  975.6  692.6  3.0   \n",
       "\n",
       "     Studentized Residuals  \n",
       "649               3.161183  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resid.loc[np.absolute(resid[\"Studentized Residuals\"]) > 3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([649], dtype='int64')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = resid.loc[np.absolute(resid[\"Studentized Residuals\"]) > 3,:].index\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t.shah\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "y_train.drop(ind,axis = 0,inplace = True)\n",
    "x_train.drop(ind,axis = 0,inplace = True)  #Interept column is not there\n",
    "X_train.drop(ind,axis = 0,inplace = True)  #Intercept column is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.47758260195686,\n",
       " 3.2696650121931814,\n",
       " 4.129325501299342,\n",
       " 82.21008475163109,\n",
       " 5.21853674386234,\n",
       " 85.86694548901554,\n",
       " 71.81633694293068,\n",
       " 1.686160096846766]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "[variance_inflation_factor(x_train.values, j) for j in range(x_train.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Regression Techniques\n",
    "\n",
    "    Linear Regression\n",
    "    Logistic Regression\n",
    "    Polynomial Regression\n",
    "    Stepwise Regression\n",
    "    Ridge Regression\n",
    "    Lasso Regression\n",
    "    ElasticNet Regression\n",
    "    \n",
    "    \n",
    "There are various kinds of regression techniques available to make predictions. These techniques are mostly driven by three metrics (**number of independent variables, type of dependent variables and shape of regression line**). We’ll discuss them in detail in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Regression\n",
    "Polynomial Regression is a form of linear regression in which the relationship between the independent variable x and dependent variable y is modeled as an nth degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x)\n",
    "\n",
    "__Why Polynomial Regression:__\n",
    "\n",
    "There are some relationships that a researcher will hypothesize is curvilinear. Clearly, such type of cases will include a polynomial term.\n",
    "Inspection of residuals. If we try to fit a linear model to curved data, a scatter plot of residuals (Y axis) on the predictor (X axis) will have patches of many positive residuals in the middle. Hence in such situation it is not appropriate.\n",
    "An assumption in usual multiple linear regression analysis is that all the independent variables are independent. In polynomial regression model, this assumption is not satisfied.\n",
    "\n",
    "\n",
    "__Uses of Polynomial Regression:__\n",
    "These are basically used to define or describe non-linear phenomenon such as:\n",
    "\n",
    "    Growth rate of tissues.\n",
    "    Progression of disease epidemics\n",
    "    Distribution of carbon isotopes in lake sediments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'Data/Polynomial_Regression_data.csv'\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sno</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sno  Temperature  Pressure\n",
       "0    1            0    0.0002\n",
       "1    2           20    0.0012\n",
       "2    3           40    0.0060\n",
       "3    4           60    0.0300\n",
       "4    5           80    0.0900\n",
       "5    6          100    0.2700"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset \n",
    "datas = pd.read_csv(url) \n",
    "datas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datas.iloc[:, 1:2].values \n",
    "y = datas.iloc[:, 2].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0],\n",
       "       [ 20],\n",
       "       [ 40],\n",
       "       [ 60],\n",
       "       [ 80],\n",
       "       [100]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0e-04, 1.2e-03, 6.0e-03, 3.0e-02, 9.0e-02, 2.7e-01])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "lin = LinearRegression() \n",
    "  \n",
    "lin.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y, color = 'blue') \n",
    "  \n",
    "plt.plot(X, lin.predict(X), color = 'red') \n",
    "plt.title('Linear Regression') \n",
    "plt.xlabel('Temperature') \n",
    "plt.ylabel('Pressure') \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overcome under-fitting, we need to increase the complexity of the model.To generate a higher order equation we can add powers of the original features as new features\n",
    "\n",
    "If we try to fit a cubic curve with different degree (degree1,3,4) to the dataset, we can see that it passes through more data points than the quadratic and the linear plots.\n",
    "\n",
    "If we further increase the degree to 20, we can see that the curve passes through more data points. \n",
    "\n",
    "For degree=20, the model is also capturing the noise in the data. This is an example of over-fitting. Even though this model passes through most of the data, it will fail to generalize on unseen data.\n",
    "To prevent over-fitting, we can add more training samples so that the algorithm doesn’t learn the noise in the system and can become more generalized. ( Note: adding more data can be an issue if the data is itself noise).\n",
    "How do we choose an optimal model? To answer this question we need to understand the bias vs variance trade-off.\n",
    "\n",
    "__The Bias vs Variance trade-off__\n",
    "\n",
    "Bias refers to the error due to the model’s simplistic assumptions in fitting the data. A high bias means that the model is unable to capture the patterns in the data and this results in under-fitting.\n",
    "\n",
    "Variance refers to the error due to the complex model trying to fit the data. High variance means the model passes through most of the data points and it results in over-fitting the data.\n",
    "\n",
    "The below picture summarizes our learning.\n",
    "\n",
    "![image](image/Bias_vs_Variance.png)\n",
    "\n",
    "From the below picture we can observe that as the model complexity increases, the bias decreases and the variance increases and vice-versa. Ideally, a machine learning model should have low variance and low bias. But practically it’s impossible to have both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "  \n",
    "poly = PolynomialFeatures(degree = 4) \n",
    "X_poly = poly.fit_transform(X) \n",
    "  \n",
    "poly.fit(X_poly, y) \n",
    "lin2 = LinearRegression() \n",
    "lin2.fit(X_poly, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y, color = 'blue') \n",
    "  \n",
    "plt.plot(X, lin2.predict(poly.fit_transform(X)), color = 'red') \n",
    "plt.title('Polynomial Regression') \n",
    "plt.xlabel('Temperature') \n",
    "plt.ylabel('Pressure') \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20675333])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.predict(110.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43295877])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin2.predict(poly.fit_transform(110.0)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Advantages of using Polynomial Regression:__\n",
    "\n",
    "Broad range of function can be fit under it.\n",
    "Polynomial basically fits wide range of curvature.\n",
    "Polynomial provides the best approximation of the relationship between dependent and independent variable.\n",
    "\n",
    "__Disadvantages of using Polynomial Regression__\n",
    "\n",
    "These are too sensitive to the outliers.\n",
    "The presence of one or two outliers in the data can seriously affect the results of a nonlinear analysis.\n",
    "In addition there are unfortunately fewer model validation tools for the detection of outliers in nonlinear regression than there are for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise Regression\n",
    "\n",
    "This form of regression is used when we deal with multiple independent variables. In this technique, the selection of independent variables is done with the help of an automatic process, which involves no human intervention.\n",
    "\n",
    "This feat is achieved by observing statistical values like R-square, t-stats and AIC metric to discern significant variables. Stepwise regression basically fits the regression model by adding/dropping co-variates one at a time based on a specified criterion. Some of the most commonly used Stepwise regression methods are listed below:\n",
    "\n",
    "Standard stepwise regression does two things. It adds and removes predictors as needed for each step.\n",
    "Forward selection starts with most significant predictor in the model and adds variable for each step.\n",
    "Backward elimination starts with all predictors in the model and removes the least significant variable for each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "\n",
    "idge Regression is a technique used when the data suffers from multicollinearity ( independent variables are highly correlated). In multicollinearity, even though the least squares estimates (OLS) are unbiased, their variances are large which deviates the observed value far from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.\n",
    "\n",
    "Above, we saw the equation for linear regression. Remember? It can be represented as:\n",
    "\n",
    "y=a+ b*x\n",
    "\n",
    "This equation also has an error term. The complete equation becomes:\n",
    "\n",
    "y=a+b*x+e (error term),  [error term is the value needed to correct for a prediction error between the observed and predicted value]\n",
    "\n",
    "\n",
    "=> y=a+y= a+ b1x1+ b2x2+....+e, for multiple independent variables.\n",
    "In a linear equation, prediction errors can be decomposed into two sub components. First is due to the biased and second is due to the variance. Prediction error can occur due to any one of these two or both components. Here, we’ll discuss about the error caused due to variance.\n",
    "\n",
    "This is a regularization method and uses l2 regularization.\n",
    "\n",
    "\n",
    "As mentioned before, ridge regression performs ‘L2 regularization‘, i.e. it adds a factor of sum of squares of coefficients in the optimization objective. Thus, ridge regression optimizes the following:\n",
    "\n",
    "                    Objective = RSS + α * (sum of square of coefficients)\n",
    "                    \n",
    "Here, α (alpha) is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients. α can take various values:\n",
    "\n",
    "__α = 0:__\n",
    "The objective becomes same as simple linear regression.\n",
    "We’ll get the same coefficients as simple linear regression.\n",
    "\n",
    "__α = ∞:__\n",
    "The coefficients will be zero. Why? Because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.\n",
    "\n",
    "__0 < α < ∞:__\n",
    "The magnitude of α will decide the weightage given to different parts of objective.\n",
    "\n",
    "The coefficients will be somewhere between 0 and ones for simple linear regression.\n",
    "I hope this gives some sense on how α would impact the magnitude of coefficients. One thing is for sure that any non-zero value would give values less than that of simple linear regression. By how much? We’ll find out soon. Leaving the mathematical details for later, lets see ridge regression in action on the same problem as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "Similar to Ridge Regression, Lasso (Least Absolute Shrinkage and Selection Operator) also penalizes the absolute size of the regression coefficients. In addition, it is capable of reducing the variability and improving the accuracy of linear regression models.  Look at the equation below: Lasso regression differs from ridge regression in a way that it uses absolute values in the penalty function, instead of squares. This leads to penalizing (or equivalently constraining the sum of the absolute values of the estimates) values which causes some of the parameter estimates to turn out exactly zero. Larger the penalty applied, further the estimates get shrunk towards absolute zero. This results to variable selection out of given n variables.\n",
    "\n",
    "__Important Points:__\n",
    "\n",
    "The assumptions of this regression is same as least squared regression except normality is not to be assumed\n",
    "It shrinks coefficients to zero (exactly zero), which certainly helps in feature selection\n",
    "This is a regularization method and uses l1 regularization\n",
    "If group of predictors are highly correlated, lasso picks only one of them and shrinks the others to zero\n",
    "\n",
    "__Difference__\n",
    "Ridge and Lasso regression uses two different penalty functions. Ridge uses l2 where as lasso go with l1. In ridge regression, the penalty is the sum of the squares of the coefficients and for the Lasso, it’s the sum of the absolute values of the coefficients. It’s a shrinkage towards zero using an absolute value (l1 penalty) rather than a sum of squares(l2 penalty).\n",
    "\n",
    "As we know that ridge regression can’t zero coefficients. Here, you either select all the coefficients or none of them whereas LASSO does both parameter shrinkage and variable selection automatically because it zero out the co-efficients of collinear variables. Here it helps to select the variable(s) out of given n variables while performing lasso regression.\n",
    "\n",
    "Another type of regularization method is ElasticNet, it is hybrid of lasso and ridge regression both. It is trained with L1 and L2 prior as regularizer. A practical advantage of trading-off between Lasso and Ridge is that, it allows Elastic-Net to inherit some of Ridge’s stability under rotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet Regression\n",
    "\n",
    "ElasticNet is hybrid of Lasso and Ridge Regression techniques. It is trained with L1 and L2 prior as regularizer. Elastic-net is useful when there are multiple features which are correlated. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n",
    "\n",
    "    It encourages group effect in case of highly correlated variables\n",
    "    There are no limitations on the number of selected variables\n",
    "    It can suffer with double shrinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
