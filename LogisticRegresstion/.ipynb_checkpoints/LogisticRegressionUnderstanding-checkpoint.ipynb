{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "I want to mentioned the Logistic regresstion si specific type of generalized liner model( GLM)\n",
    "Let's understand first __Classfication__ : \n",
    "\n",
    "A classification problem is when the output variable is a _category_, such as “red” or “blue” or “disease” and “no disease”. A classification model attempts to draw some conclusion from observed values. Given one or more inputs a classification model will try to predict the value of one or more outcomes.\n",
    "\n",
    "__For example__:<br>\n",
    "1) when filtering emails “spam” or “not spam”<br>\n",
    "2) when looking at transaction data, “fraudulent”, or “authorized”<br>\n",
    "\n",
    "In short Classification either predicts categorical class labels or classifies data (construct a model) based on the training set and the values (class labels) in classifying attributes and uses it in classifying new data.\n",
    "\n",
    "Here we have the types of classification algorithms in Machine Learning:\n",
    "\n",
    "- Linear Classifiers: Logistic Regression, Naive Bayes Classifier\n",
    "- Support Vector Machines\n",
    "- Decision Trees\n",
    "- Boosted Trees\n",
    "- Random Forest\n",
    "- Neural Networks\n",
    "- Nearest Neighbor\n",
    "\n",
    "__For example :<br>\n",
    "Which of the following is/are classification problem(s)?__\n",
    "\n",
    "- Predicting the gender of a person by his/her handwriting style<br>\n",
    "- Predicting house price based on area<br>\n",
    "- Predicting whether monsoon will be normal next year<br>\n",
    "- Predict the number of copies a music album will be sold next month<br>\n",
    "\n",
    "_Solution :_ Predicting the gender of a person and  Predicting whether monsoon will be normal next year. The other two are regression.\n",
    "\n",
    "Supervised learning is also called predictive modeling or predictive analytics, because you build a model that is capable of making predictions.\n",
    "\n",
    "Some examples of predictive modeling are classification and regression. Classification identifies which category an item belongs to (e.g., whether a transaction is fraud or not fraud), based on labeled examples of known items (e.g., transactions known to be fraud or not). Logistic regression predicts a probability (e.g., the probability of fraud). Linear regression predicts a numeric value (e.g., the amount of fraud).\n",
    "\n",
    "![image.png](image/Logistic-Linear.png)\n",
    "\n",
    "### Classification and Regression Example\n",
    "Classification and regression take a set of data with known labels and predetermined features and learns how to label new records based on that information. Features are the \"if questions\" that you ask. The label is the answer to those questions.\n",
    "\n",
    "![image.png](image/duck.png)\n",
    "\n",
    "#### Regression Example\n",
    "\n",
    "Let's go through an example of car insurance fraud:\n",
    "\n",
    "- What are we trying to predict?\n",
    "\n",
    "        This is the label: the amount of fraud\n",
    "- What are the \"if questions\" or properties that you can use to predict?\n",
    "\n",
    "        These are the features: to build a classifier model, you extract the features of interest that most contribute to the classification.        \n",
    "        In this simple example, we will use the claimed amount.\n",
    "        \n",
    "Linear regression models the relationship between the Y \"Label\" and the X \"Feature,\" in this case the relationship between the amount of fraud and the claimed amount. The coefficient measures the impact of the feature, the claimed amount, and on the label, the fraud amount.\n",
    "\n",
    "Multiple linear regression models the relationship between two or more \"Features\" and a response \"Label.\" For example, if we wanted to model the relationship between the amount of fraud and the age of the claimant, the claimed amount, and the severity of the accident, the multiple linear regression function would look like this:\n",
    "\n",
    "Yi = β0 + β1X1 + β2X2 + · · · + βp Xp + Ɛ\n",
    "\n",
    "Amount Fraud = intercept + (coefficient1 age) + (coefficient2 claimed Amount) + (coefficient3 * severity) + error.\n",
    "\n",
    "\n",
    "The coefficients measure the impact on the fraud amount of each of the features.\n",
    "\n",
    "Some examples of linear regression include:\n",
    "\n",
    "- Given historical car insurance fraudulent claims and features of the claims, such as age of the claimant, claimed amount, and severity of the accident, predict the amount of fraud.\n",
    "- Given historical real estate sales prices and features of houses (square feet, number of bedrooms, location, etc.), predict a house's price.\n",
    "- Given historical neighborhood crime statistics, predict crime rate.\n",
    "\n",
    "\n",
    "#### Classification Example\n",
    "\n",
    "Let's go through an example of debit card fraud:\n",
    "\n",
    "- What are we trying to predict?\n",
    "\n",
    "        This is the label: probability of fraud\n",
    "    \n",
    "- What are the \"if questions\" or properties that you can use to make predictions?\n",
    "\n",
    "        Is the amount spent today > historical average?\n",
    "        Are there transactions in multiple countries today?\n",
    "        Are the number of transactions today > historical average?\n",
    "        Are the number of new merchant types today high compared to the last 3 months?\n",
    "        Are there multiple purchases today from merchants with a category code of risk?\n",
    "        Is there unusual signing activity today, compared to historically using pin?\n",
    "        Are there new state purchases compared to the last 3 months?\n",
    "        Are there foreign purchases today compared to the last 3 months?\n",
    "\n",
    "To build a classifier model, you extract the features of interest that most contribute to the classification.\n",
    "\n",
    "\n",
    "_Some examples of classification include:_\n",
    "\n",
    "    Given historical car insurance fraudulent claims and features of the claims, such as age of the claimant, claimed amount, and severity of the accident, predict the probability of fraud.\n",
    "    Given patient characteristics, predict the probability of congestive heart failure.\n",
    "    Credit card fraud detection (fraud, not fraud)\n",
    "    Credit card application (good credit, bad credit)\n",
    "    Email spam detection (spam, not spam)\n",
    "    Text sentiment analysis (happy, not happy)\n",
    "    Predicting patient risk (high risk patient, low risk patient)\n",
    "    Classifying a tumor (malignant, not malignant)\n",
    "    \n",
    "Liner regresstion\n",
    "\n",
    "    \tFind the releationship between depedent variable and independent variables. Using simple line.\n",
    "        This used to continuse.\n",
    "    \tLR solve regresstion progblem.\n",
    "    \tLR Response varaible is continus in nature.\n",
    "    \tIt help estimate the depedent variable when there is change in the indepdent varaible  \n",
    "\t\n",
    "Logistic Regresstion:\n",
    "\n",
    "    \tBut here depedent variable will be catorigcal. Example. Please find below Year of experience and got promated and not promated. Hence all the point lies between 0 and 1 .\n",
    "    \tSo we will not able to fit liner line in this case a RMS value will be huge.\n",
    "    \tY = mx + c is same { y will be catogrical variable }\n",
    "    \tHence what we will do is , we create I new function and plot.\n",
    "    \tLogistic is good for outlier.\n",
    "    \tLogistic Regresstion used to solve catorigcal problem.\n",
    "    \tLogistic Regresstion response variable is catorigcal.\n",
    "    \tWhiel Logistic regresstion help to caclulation the posbilites event taking place\n",
    "![image.png](image/Difference-Between-Linear-Regression-and-Logistic-Regression.jpg)\n",
    "\n",
    "![image.png](image/Logistic-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Math Behind Logistic Regression\n",
    "_To understand Logistic Regression, lets talk about the odds of success :_\n",
    "\n",
    "![image.png](image/MathFunction.png)\n",
    "\n",
    "So when we finally obtain the equation of the sigmoid function, we plot the sigmoid curve on the axis.\n",
    "\n",
    "Imagine your car has not been serviced from quite a few years and you want to find out if it’s going to breakdown in future or not. So this is a classification problem whether your car will break down or not. Lets look at the approach.\n",
    "\n",
    "If we plot the information along the X axis and Y axis. X is number of years since the last service was performed and Y axis is the probability of your car breaking down.\n",
    "\n",
    "![image.png](image/sigmoid.png)\n",
    "\n",
    "Imagine your car has not been serviced from quite a few years and you want to find out if it’s going to breakdown in future or not. So this is a classification problem whether your car will break down or not. Lets look at the approach.\n",
    "\n",
    "If we plot the information along the X axis and Y axis. X is number of years since the last service was performed and Y axis is the probability of your car breaking down.\n",
    "\n",
    "![image.png](image/Thershold.png)\n",
    "\n",
    "\n",
    "_Let’s understand it further using an example:_\n",
    "\n",
    "We are provided a sample of 1000 customers. We need to predict the probability whether a customer will buy (y) a particular magazine or not. As you can see, we’ve a categorical outcome variable, we’ll use logistic regression.\n",
    "\n",
    "To start with logistic regression, I’ll first write the simple linear regression equation with dependent variable enclosed in a link function:\n",
    "\n",
    "                         g(y) = βo + β(Age)         ---- (a)\n",
    "Note: For ease of understanding, I’ve considered ‘Age’ as independent variable.\n",
    "\n",
    "In logistic regression, we are only concerned about the probability of outcome dependent variable ( success or failure). As described above, g() is the link function. This function is established using two things: Probability of Success(p) and Probability of Failure(1-p). p should meet following criteria:\n",
    "\n",
    "1) It must always be positive (since p >= 0)<br>\n",
    "2) It must always be less than equals to 1 (since p <= 1)<br>\n",
    "\n",
    "__Since probability must always be positive, we’ll put the linear equation in exponential form. For any value of slope and dependent variable, exponent of this equation will never be negative.__\n",
    "\n",
    "                        p = exp(βo + β(Age)) = e^(βo + β(Age))    ------- (b)\n",
    "                        \n",
    "To make the probability less than 1, we must divide p by a number greater than p. This can simply be done by:\n",
    "\n",
    "                   p  =  exp(βo + β(Age)) / exp(βo + β(Age)) + 1   =   e^(βo + β(Age)) / e^(βo + β(Age)) + 1    ----- (c)\n",
    "                   \n",
    "Using (a), (b) and (c), we can redefine the probability as:\n",
    "\n",
    "                       p = e^y/ 1 + e^y           --- (d)\n",
    "where p is the probability of success. This (d) is the Logit Function\n",
    "\n",
    "If p is the probability of success, 1-p will be the probability of failure which can be written as\n",
    "\n",
    "                        q = 1 - p = 1 - (e^y/ 1 + e^y)    --- (e)\n",
    "                        \n",
    "where q is the probability of failure On dividing, (d) / (e), we get,\n",
    "\n",
    "sigmoid function  = p/(1-p) = e^y\n",
    "\n",
    "After taking log on both side, we get,  = log[p/(1-p)] = y \n",
    "\n",
    "log(p/1-p) is the link function. Logarithmic transformation on the outcome variable allows us to model a non-linear association in a linear way.\n",
    "\n",
    "After substituting value of y, we’ll get:\n",
    "\n",
    "                            log[p/(1-p)] = βo + β(Age)\n",
    "                            \n",
    "This is the equation used in Logistic Regression. Here (p/1-p) is the odd ratio. Whenever the log of odd ratio is found to be positive, the probability of success is always more than 50%. A typical logistic model plot show above. You can see probability never goes below 0 and above 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Logistic Regression ?\n",
    "\n",
    "Logistic Regression is a classification algorithm. It is used to predict a binary outcome (1 / 0, Yes / No, True / False) given a set of independent variables. To represent binary / categorical outcome, we use dummy variables. You can also think of logistic regression as a special case of linear regression when the outcome variable is categorical, where we are using log of odds as dependent variable. In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function\n",
    "\n",
    "### Types of Logistic Regression \n",
    "\n",
    "- Binary Logistic Regression ( Two class logistic Regresstion )\n",
    "        The categorical response has only two 2 possible outcomes. Example: Spam or Not\n",
    "\n",
    "- Multinomial Logistic Regression\n",
    "        Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan)\n",
    "        \n",
    "- Ordinal Logistic Regression\n",
    "        Three or more categories with ordering. Example: Movie rating from 1 to 5\n",
    "        \n",
    "- One-vs-All Classification Using Logistic Regression\n",
    "        First of all, let me briefly explain the idea behind one-vs-all classification. we have a classification problem and there are N distinct classes. In this case, we’ll have to train a multi-class classifier instead of a binary one.\n",
    "        \n",
    "![image](image/one-vs-All.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why logistic regression not linear  .... ?\n",
    "\n",
    "When the response variable has only 2 possible values, it is desirable to have a model that predicts the value either as 0 or 1 or as a probability score that ranges between 0 and 1.\n",
    "\n",
    "Linear regression does not have this capability. Because, If you use linear regression to model a binary response variable, the resulting model may not restrict the predicted Y values within 0 and 1.\n",
    "\n",
    "This is where logistic regression comes into play. In logistic regression, you get a probability score that reflects the probability of the occurence of the event.\n",
    "\n",
    "An event in this case is each row of the training dataset. It could be something like classifying if a given email is spam, or mass of cell is malignant or a user will buy a product and so on.\n",
    "\n",
    "![image](image/Logistic-Linear_2.png)\n",
    "\n",
    "Linear Regression model probability : -∞  to +∞ <br>\n",
    "Probability of outcome lie between : 0< p< 1\n",
    "\n",
    "__Linear Regression has a considerable effects on OUTLIERS__ but to avoid this probelm, log-odds function or logit function is used.\n",
    "\n",
    "Logistic Regression will take care on __OUTLIERS__\n",
    "\n",
    "\n",
    "\n",
    "### Logit Function\n",
    "\n",
    "The odds for an event is the (probability of an event occuring) / (probability of event not occuring):\n",
    "\n",
    "\n",
    "For Linear regression: continuous response is modeled as a linear combination of the features: y = β0 + β1x\n",
    "For Logistic regression: log-odds of a categorical response being \"true\" (1) is modeled as a linear combination of the features:\n",
    "\n",
    "![image.png](image/Logistic-odds.png)\n",
    "\n",
    "_Figure.1_ This is called the logit function. On solving for probability (p) you will get:<br>\n",
    "_Figure.2_ Showing linear model and logistic model:\n",
    "\n",
    "__What is odd ?_ The odds signifies the ratio of probability of success to probability of failure.__\n",
    "\n",
    "__Example: India and Pakistan__\n",
    "\n",
    "India winning a match is termed as successful event. While india losing a match is termed as Failure.\n",
    "\n",
    "![image.png](image/Logit1.png)\n",
    "\n",
    "In other words:\n",
    "\n",
    "- Logistic regression outputs the __probabilities of a specific class__.\n",
    "- Those probabilities can be converted into __class predictions__.\n",
    "\n",
    "\n",
    "The logistic function has some nice properties:\n",
    "\n",
    "- Takes on an __\"s\"__ shape\n",
    "- Output is bounded by __0 and 1__<br/>\n",
    "\n",
    "\n",
    "We have covered how this works for binary classification problems (two response classes). But what about __multi-class classification problems__ (more than two response classes)?\n",
    "\n",
    "- Most common solution for classification models is __\"one-vs-all\"__ (also known as __\"one-vs-rest\"__): decompose the problem into multiple binary classification problems.\n",
    "- __Multinomial logistic regression__ can solve this as a single problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of Logistic Regression\n",
    "\n",
    "__Pros__\n",
    "\n",
    "- simple and efficient \n",
    "- Low variance\n",
    "- it Provide probability score for observation\n",
    "\n",
    "__ Cons__\n",
    "\n",
    "-Doesn't handle large number of categorical feature/variables well.\n",
    "- it requires transformation of non-linear features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples \n",
    "_Training a Simple Binary Classifier Using Logistic Regression_\n",
    "\n",
    "__Problem & Dataset__\n",
    "Our objective in this problem is to estimate an applicant’s probability of admission into a university based on his/her results on two exams.Our dataset contains some historical data from previous applicants, \n",
    "\n",
    "Please refer if want to understand backend of algo.\n",
    "https://utkuufuk.com/categories/Machine-Learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "ln\\frac{p}{(1-p)}=\n",
    "\\mathbf{a}_0 + \\mathbf{a}_1{x}_1 + \\mathbf{a}_2{x}_2 + \\mathbf{a}_3{x}_3\n",
    "\\end{equation*}\n",
    "\n",
    "- p is the probability of an event to happen which you are trying to predict\n",
    "- x1, x2 and x3 are the independent variables which determine the occurrence of an event i.e. p\n",
    "- a0 is the constant term which will be the probability of the event happening when no other factors are considered\n",
    "- R.H.S represent the link function which will help us to determine a non-linear relation in a linear way wherein (p/1-p) is the odd ratio. Whenever the log of the odds ratio is found to be positive, the probability of success is always more than 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How logistic regression comes to our rescue there ?\n",
    "\n",
    "Let us take a small case here, suppose we have a model whose only job is to look at the fruit in front of the camera and state whether it is an apple or not. This is a simple binary classification problem where our output variable can take only 2 states(yes/no or 0/1). Our model gives an output 1 if it recognises it as an apple and output 0 if it is not an apple.\n",
    "\n",
    "image  to visualize our linear regression model here ….\n",
    "\n",
    "- 1 represents all the points which are classified as an apple\n",
    "- 0 represents all the points which are not apple\n",
    "- The line in blue is a “best fit” line plotted by our linear regression model\n",
    "\n",
    "![image.png](image/More_exp.png)\n",
    "\n",
    "\n",
    "__Issue 1:__ Area 1 and Area 2 represent a portion of the line where the probability of any fruit seen by our model being an apple is >1(area1) and <0(area2). Since probabilities can never be greater than 1 or less than 0, part of the regression line in area 1 and area 2 does not make any sense.<br>\n",
    "__Solution 1:__ Since both, the areas are not required, we cut them and keep the remaining with us. This line has a finite limit. Whenever your x tends to infinity, y will tend to 1 and whenever your x will tend to -infinity, y will tend to 0.\n",
    "\n",
    "\n",
    "__Issue 2:__ Since binary classification problems can only have one of two possible values(0 or 1), the residuals(Actual value -predicted value) will not be normally distributed about the regression line.<br>\n",
    "__Solution 2:__ To overcome the residual issue, we identify a threshold probability value. Our final variable in logistic regression is the probability of the target event. So if my apple identifier logistic model outputs a value like 0.45 then this means that there is a 45% probability that new fruit seen by our model is an apple. Here we set a threshold value 0.5 which means that if the probability of fruit seen by the model is an apple is greater than 50% then our model will predict it to be an apple and if it is less than 50% then our model will say that it is not an apple.\n",
    "\n",
    "So every value predicted by the model below 0.5, the model will classify it as not an apple in this case and for every other value greater than threshold 0.5, the model will classify it as an apple.\n",
    "\n",
    "__Assumptions in logistic regression__\n",
    "\n",
    "There are some assumptions which need to be satisfied for having a logistic regression model which we can interpret meaningfully.\n",
    "\n",
    "But let us look at some things which we no longer have to take care of in the case of logistic regression if we compare it with logistic regression.\n",
    "\n",
    "Logistic regression does NOT require a linear relationship between the dependent and independent variables\n",
    "The error terms (residuals) do NOT need to be normally distributed\n",
    "Homoscedasticity is NOT required\n",
    "The dependent variable in logistic regression is NOT measured on an interval or ratio scale\n",
    "\n",
    "But then there are some assumptions which should __hold__\n",
    "\n",
    "Logistic regression requires the dependent variable to be categorical\n",
    "Logistic regression requires there to be little or no multicollinearity among the independent variables. This means that the independent variables should not be too highly correlated with each other.\n",
    "There should be a linear relationship between the link function (log(p/(1-p))and independent variables in the logit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods do we use to best fit the data in Logistic Regression\n",
    "\n",
    "    Maximum Likelihood  (Logistic regression uses maximum likely hood estimate for training a logistic regression.)<br>\n",
    "    Least Square Error\n",
    "    AUC-ROC\n",
    "    Accuracy\n",
    "    Logloss\n",
    "    One of the very good methods to analyze the performance of Logistic Regression is AIC,which is similar to R-Squared in Linear Regression. We prefer a model with minimum AIC value. We select the best model in logistic regression which can least AIC\n",
    "    \n",
    "\n",
    "__Probabilistic Model Selection__\n",
    "\n",
    "Probabilistic model selection (or “information criteria”) provides an analytical technique for scoring and choosing among candidate models.\n",
    "\n",
    "Models are scored both on their performance on the training dataset and based on the complexity of the model.\n",
    "\n",
    "- Model Performance. How well a candidate model has performed on the training dataset.\n",
    "- Model Complexity. How complicated the trained candidate model is after training.\n",
    "\n",
    "Model performance may be evaluated using a probabilistic framework, such as log-likelihood under the framework of maximum likelihood estimation. Model complexity may be evaluated as the number of degrees of freedom or parameters in the model.\n",
    "\n",
    "A benefit of probabilistic model selection methods is that a test dataset is not required, meaning that all of the data can be used to fit the model, and the final model that will be used for prediction in the domain can be scored directly.\n",
    "\n",
    "A limitation of probabilistic model selection methods is that the same general statistic cannot be calculated across a range of different types of models. Instead, the metric must be carefully derived for each model.\n",
    "\n",
    "\n",
    "- Akaike Information Criterion (AIC). Derived from frequentist probability.\n",
    "- Bayesian Information Criterion (BIC). Derived from Bayesian probability.\n",
    "- Minimum Description Length (MDL). Derived from information theory.\n",
    "\n",
    "Each statistic can be calculated using the log-likelihood for a model and the data. Log-likelihood comes from Maximum Likelihood Estimation, a technique for finding or optimizing the parameters of a model in response to a training dataset.\n",
    "\n",
    "In Maximum Likelihood Estimation, we wish to maximize the conditional probability of observing the data (X) given a specific probability distribution and its parameters (theta), stated formally as:\n",
    "\n",
    "    P(X ; theta)\n",
    "Where X is, in fact, the joint probability distribution of all observations from the problem domain from 1 to n.\n",
    "\n",
    "    P(x1, x2, x3, …, xn ; theta)\n",
    "The joint probability distribution can be restated as the multiplication of the conditional probability for observing each example given the distribution parameters. Multiplying many small probabilities together can be unstable; as such, it is common to restate this problem as the sum of the natural log conditional probability.\n",
    "\n",
    "    sum i to n log(P(xi ; theta))\n",
    "Given the frequent use of log in the likelihood function, it is commonly referred to as a log-likelihood function.\n",
    "\n",
    "The log-likelihood function for common predictive modeling problems include the mean squared error for regression (e.g. linear regression) and log loss (binary cross-entropy) for binary classification (e.g. logistic regression).\n",
    "\n",
    "We will take a closer look at each of the three statistics, AIC, BIC, and MDL, in the following sections.\n",
    "\n",
    "### The Akaike Information Criterion, or AIC for short, is a method for scoring and selecting a model.\n",
    "\n",
    "The Akaike Information Criterion, or AIC for short, is a method for scoring and selecting a model.\n",
    "The AIC statistic is defined for logistic regression as follows\n",
    "- AIC = -2/N * LL + 2 * k/N\n",
    "\n",
    "Where \n",
    "N is the number of examples in the training dataset, \n",
    "LL is the log-likelihood of the model on the training dataset, and \n",
    "k is the number of parameters in the model.\n",
    "\n",
    "The score, as defined above, is minimized, e.g. the model with the __lowest AIC is selected.__\n",
    "\n",
    "__To use AIC for model selection, we simply choose the model giving smallest AIC over the set of models considered.__\n",
    "\n",
    "\n",
    "### Bayesian Information Criterion\n",
    "The Bayesian Information Criterion, or BIC for short, is a method for scoring and selecting a model.\n",
    "\n",
    "It is named for the field of study from which it was derived: Bayesian probability and inference. Like AIC, it is appropriate for models fit under the maximum likelihood estimation framework.\n",
    "\n",
    "The BIC statistic is calculated for logistic regression as follow.\n",
    "\n",
    "- BIC = -2 * LL + log(N) * k\n",
    "\n",
    "Where \n",
    "log() has the base-e called the natural logarithm, \n",
    "LL is the log-likelihood of the model, \n",
    "N is the number of examples in the training dataset, and \n",
    "k is the number of parameters in the model.\n",
    "\n",
    "The score as defined above is minimized, e.g. the model with the lowest BIC is selected.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Maximum Likelihood\n",
    "\n",
    "As we know, in logistic regression we transfer y axis 0 to 1 probability to log (odd) will convert into -infinte to +infinit . If  we draw best fitting line and take residual, and that means we can't use least-squares to find best fitting line.\n",
    "So here we will be using __Maximum likelihood __ \n",
    "\n",
    "![image.png](image/max_likihood.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now by using max likihood, best fit of line but how do we know if it is useful ?\n",
    "\n",
    "How do we calculate R2 and p-value for the relationship ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Examples__\n",
    "The diabetes dataset consists of several medical predictor variables and one target variable, Outcome. Predictor variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on. The objective is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset.\n",
    "\n",
    "There are 9 columns in our dataset which includes 8 predictor variables (Pregnancies, Glucose, Blood Pressure .. etc) and 1 target variable (Outcome). The patients diagnosed with diabetes are represented 1 and patients which were not diagnosed with diabetes are represented with 0.\n",
    "\n",
    "![image.png](image/null.png)\n",
    "\n",
    "Let us have a closer look at what all the above terms are in the summary of our logistic model which will help us to gain a better understanding of the model we have built.\n",
    "\n",
    "- Null Deviance: When the model includes only an intercept term (no predictor variables are taken into account), then the performance of the model is governed by null deviance.\n",
    "- Residual Deviance: When the model has included 6 predictor variables, then the deviance is residual deviance which is lower(506.94) than null deviance(695.42). The lower value of residual deviance points out that the model has become better when it has included predictor variables\n",
    "- AIC value: Its full form is Akaike Information Criterion (AIC). This is useful when we have more than one model to compare the goodness of fit of the models. It is a maximum likelihood estimate which penalizes to prevent overfitting. It measures the flexibility of the models. It is analogous to adjusted R squared in multiple linear regression where it tries to prevent you from including irrelevant predictor variables. Lower AIC of a model is better than the model having higher AIC.\n",
    "- Fisher score: It tells how the model was estimated. The algorithm looks around to see if the fit would be improved by using different estimates. If it improves then it moves in that direction and then fits the model again. The algorithm stops when no significant additional improvement can be done. “Number of Fisher Scoring iterations,” tells “how many iterations this algorithm run before it stopped”.Here it is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Logistic Regression\n",
    "Lets talk about some practical uses of logistic regression\n",
    "\n",
    "__Weather Prediction :__ It determines what kind of weather can be expected.Is it going to rain or not, or it would be sunny or not, or it would be snowy or not. But if we want to determine what would be the temperature tomorrow, we use linear regression.<br>\n",
    "__Image Categorization :__ Identifies different components that are present in the image and helps categorize them.<br>\n",
    "__Healthcare (TRISS) :__ Determine the possibility of patient’s survival , taking Age, ISS(Injury Severity Score) , RTS (Revised Trauma Score) into consideration.<br>\n",
    "\n",
    "\n",
    "Logistic Regression was used in __biological sciences__ in early twentieth century. It was then used in many social science applications. For instance,\n",
    "- The Trauma and Injury Severity Score (TRISS), which is widely used to __predict mortality in injured patients__, was originally developed by Boyd et al. using logistic regression.<br/> \n",
    "- Many other medical scales used to __assess severity__ of a patient have been developed using logistic regression.<br/>\n",
    "- Logistic regression may be used to __predict the risk of developing a given disease__ (e.g. diabetes; coronary heart disease), based on observed characteristics of the patient (age, sex, body mass index, results of various blood tests, etc.).<br/>\n",
    "\n",
    "Now a days, Logistic Regression have the following applications \n",
    "1. Image segementation and  categorization\n",
    "2. Geographic image processing\n",
    "3. Handwriting recognition\n",
    "4. Detection of  myocardinal infarction\n",
    "5. Predict whether a person is depressed or not based on a bag of words from corpus. \n",
    "\n",
    "\n",
    "The reason why logistic regression is widely used despite of the state of the art of deep neural network is that logistic regression is very __efficient__ and does __not__ require too much __computational resources__, which makes it __affordable__ to run on production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
