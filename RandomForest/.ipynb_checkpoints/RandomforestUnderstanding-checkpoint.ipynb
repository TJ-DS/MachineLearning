{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random  Forest\n",
    "\n",
    "Genral Terms\n",
    "\n",
    "Random Forest a group/collection of decision tree. What does it mean? <br>\n",
    "Let say whenever you are going to buy a car, you will not go to one person and buy a car.\n",
    "\n",
    "        Might be you will connect with 10 friends.\n",
    "        Lots of information you will be looking.\n",
    "        Taking lot of opinion. \n",
    "And finally you decide to buy this car. You take an informed decision  \n",
    "\n",
    "\n",
    "## What is Random forest algorithm?\n",
    "\n",
    "In general, the more trees in the forest the more robust the forest looks like. In the same way in the random forest classifier, the higher the number of trees in the forest gives the high accuracy results\n",
    "\n",
    "If you know the decision tree algorithm. You might be thinking are we creating more number of decision trees and how can we create more number of decision trees. As all the calculation of nodes selection will be same for the same dataset.\n",
    "\n",
    "Yes. You are true. To model more number of decision trees to create the forest __you are not going to use the same apache of constructing the decision with information gain or gini index approach.__\n",
    "\n",
    "\n",
    "__Random forest algorithm can use both for classification and the regression kind of problems.__\n",
    "\n",
    "## Why Random forest algorithm?\n",
    "\n",
    "To address why random forest algorithm. I am giving you the below advantages.\n",
    "\n",
    "The same random forest algorithm or the random forest classifier can use for both classification and the regression task.\n",
    "Random forest classifier will handle the missing values.\n",
    "When we have more trees in the forest, random forest classifier won’t overfit the model.\n",
    "Can model the random forest classifier for categorical values also.\n",
    "\n",
    "Before you drive into the technical details about the random forest algorithm. Let’s look into a real life example to understand the layman type of random forest algorithm.\n",
    "\n",
    "Suppose Mady somehow got 2 weeks leave from his office. He wants to spend his 2 weeks by traveling to the different place. He also wants to go to the place he may like.\n",
    "\n",
    "So he decided to ask his best friend about the places he may like. Then his friend started asking about his past trips. It’s just like his best friend will ask, You have been visited the X place did you like it?\n",
    "\n",
    "Based on the answers which are given by Mady, his best start recommending the place Mady may like. Here his best formed the decision tree with the answer given by Mady.\n",
    "\n",
    "As his best friend may recommend his best place to Mady as a friend. The model will be biased with the closeness of their friendship. So he decided to ask few more friends to recommend the best place he may like.\n",
    "\n",
    "Now his friends asked some random questions and each one recommended one place to Mady. Now  Mady considered the place which is high votes from his friends as the final place to visit.\n",
    "\n",
    "In the above Mady trip planning, two main interesting algorithms decision tree algorithm and random forest algorithm used. I hope you find it already.\n",
    "\n",
    "__Decision Tree:__\n",
    "To recommend the best place to Mady, his best friend asked some questions. Based on the answers given by mady, he recommended a place. This is decision tree algorithm approach. Will explain why it is a decision tree algorithm approach.\n",
    "\n",
    "Mady friend used the answers given by mady to create rules. Later he used the created rules to recommend the best place which mady will like. These rules could be, mady like a place with lots of tree or waterfalls ..etc\n",
    "\n",
    "In the above approach mady best friend is the decision tree. The vote (recommended place) is the leaf of the decision tree (Target class). The target is finalized by a single person, In a technical way of saying, using an only single decision tree.\n",
    "\n",
    "__Random Forest Algorithm:__\n",
    "In the other case when mady asked his friends to recommend the best place to visit. Each friend asked him different questions and come up their recommend a place to visit. Later mady consider all the recommendations and calculated the votes. Votes basically is to pick the popular place from the recommend places from all his friends.\n",
    "\n",
    "Mady will consider each recommended place and if the same place recommended by some other place he will increase the count. At the end the high count place where mady will go.\n",
    "\n",
    "In this case, the recommended place (Target Prediction) is considered by many friends. Each friend is the tree and the combined all friends will form the forest. This forest is the random forest. As each friend asked random questions to recommend the best place visit.\n",
    "\n",
    "Now let’s use the above example to understand how the random forest algorithm work.\n",
    "\n",
    "In the __Decision Tree Classifier.__ We have come to the conclusion that it has the tendency to overfit. Are there any solutions to this issue? Of course. There are two main methods to reduce overfitting\n",
    "\n",
    "    pruning\n",
    "    bagging (such as Random Forest Classifier)\n",
    "    \n",
    "### BIAS-VARIANCE TRADEOFF\n",
    "for example when dealing with linear regression. We have the assumption that there is a linear relationship between the features. Such as house sizes and house prices. Low bias means low assumptions about the target function. Decision trees or Support Vector Machines fall into this category. High bias models have more assumptions. Linear regression or Logistic Regression are like this. When there is a high bias, the algorithm misses the relevant relationships between features and target variables. This is called underfitting.\n",
    "\n",
    "We train  a machine learning algorithm on a given dataset. How will it work with other datasets? It should not change too much from one training dataset to the next. High variance means the given algorithm is strongly influenced by the specifics of the training data. So variance is the error from sensitivity to small changes in the training set. High variance can cause overfitting. This is the case when dealing with decision trees.\n",
    "\n",
    "The aim of machine learning approaches is to reduce both bias and variance. But we can not achieve it. This is the bias-variance tradeoff. Increasing the bias will decrease variance. Increasing the variance will decrease bias.\n",
    "\n",
    "#### PRUNING\n",
    "So decision trees have the tendency to overfit. How to decrease it? A good idea is to construct smaller trees with fewer splits. So a solution is to grow a large tree and then prune it back to a smaller subtree.\n",
    "\n",
    "![fdff444hhh.png](http://www.globalsoftwaresupport.com/wp-content/uploads/2018/02/fdff444hhh.png)\n",
    "\n",
    "So we end up with a smaller tree. We have managed to decrease variance BUT with the cost of some extra bias. Can we do better? Yeah, we can. With bagging we do not have to take the extra bias. Thats why __bagging__ is a more popular approach.\n",
    "\n",
    "#### BAGGING\n",
    "\n",
    "Bagging stands for __bootstrap aggregation.__ The aim is the same as for pruning. We want to reduce the variance of a decision tree. By the way we can use bagging for any kinds of learning algorithms. The principle is that we can reduce the variance by averaging a set of observations.\n",
    "\n",
    "if we have X set of independent variables $  x_1, x_2, x_3....x_n$ each with variance V then the variance of the mean X ( the mean of the $x_1, x_2, x_3 .... x_n$ variable) is $v/n $\n",
    "\n",
    "\n",
    "So what do we have to do? We should have multiple training sets and construct a decision tree (without pruning) on every single training set. But there is a huge problem: we do not have several training sets. Ok, then we should take repeated samples from the single dataset and construct a tree and finally average all the predictions. All the trees are fully grown unpruned decision trees. This approach is called bagging.\n",
    "\n",
    "#### Why do we prefer bagging? \n",
    "When dealing with pruning we have to take same extra bias. Here when using bagging we do not have to. We can reduce variance without extra bias. \n",
    "\n",
    "#### What is the problem with bagging?\n",
    "When constructing the distinct decision trees we use the same features. What does it mean? These trees will be very similar, with the same root node (because the most relevant feature determines the root node), with similar branches.  So these trees have very similar predictions because they are correlated! We should decorrelate these decision trees and we can do it with Random Forest Classifier.\n",
    "\n",
    "![dsd4455.png](http://www.globalsoftwaresupport.com/wp-content/uploads/2018/02/dsd4455.png)\n",
    "\n",
    "#### RANDOM FOREST CLASSIFIER \n",
    "\n",
    "Bagging is a good idea but somehow we have to generate independent decision trees without any correlation. So what is the solution? The problem with bagging is that it uses all the features. So maybe we should use just a subset of the original features when constructing a given tree. This is the main idea behind Random Forest Classifier. It searches over a random √N features to find the best one. Why is it good? Because there are different features for different decision trees. So these trees will be of different size, with different branches. Hence these trees make different predictions.\n",
    "\n",
    "![ggff5544hh.png](http://www.globalsoftwaresupport.com/wp-content/uploads/2018/02/ggff5544hh.png)\n",
    "\n",
    "With this approach we are able to end up with decorrelated decision trees. All the generated models (the single trees) are independent of each other. So there is no correlation at all. We just have to average the predictions in order to get the final result. This is how Random Forest Classifier works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Concept behind random forest\n",
    "\n",
    "The random forest is a model made up of many decision trees. Rather than just being a forest though, this model is random because of two concepts:\n",
    "\n",
    "1. Random sampling of data points\n",
    "2. Splitting nodes based on subsets of features\n",
    "\n",
    "###   __Random Sampling__<br/>\n",
    "\n",
    "- One of the keys behind the random forest is that __each tree trains on random samples__ of the data points. \n",
    "- The samples are drawn with _replacement_ (known as __bootstrapping__) which means that some samples will be trained on in a single tree multiple times (we can also disable this behavior if we want).\n",
    "- The idea is that by training each tree on different samples, although __each tree__ might have __high variance__ with respect to a particular set of the training data, overall, the __entire forest __will have __low variance__.\n",
    "- This procedure of training each individual learner on different subsets of the data and then averaging the predictions is known as __bagging__, short for bootstrap aggregating.\n",
    "![bagging.png](https://www.analyticsvidhya.com/wp-content/uploads/2015/07/bagging.png)\n",
    "\n",
    "To more clearly understand bagging summarised below are the steps to follow: \n",
    "\n",
    "1. __Create Multiple DataSets__:\n",
    "    - Sampling is done with replacement on the original data and new datasets are formed.\n",
    "    - The new data sets can have a fraction of the columns as well as rows, which are generally hyper-parameters in a bagging model\n",
    "    - Taking row and column fractions less than 1 helps in making robust models, less prone to overfitting\n",
    "2. __Build Multiple Classifiers__:\n",
    "    - Classifiers are built on each data set.\n",
    "    - Generally the same classifier is modeled on each data set and predictions are made.\n",
    "3. __Combine Classifiers__:\n",
    "    - The predictions of all the classifiers are combined using either mean or mode value depending on the problem at hand.\n",
    "    - Generally __mean__ are used for __regression__ problems and __mode__ is used for __classification__ problems.  \n",
    "    - The combined values are generally more robust than a single model.\n",
    "    \n",
    "\n",
    "\n",
    "### __Random Subsets of Features__\n",
    "\n",
    "- Another concept behind the random forest is that only a __subset__ of all the __features__ are considered for splitting each node in each decision tree. Generally this is set to __sqrt(n_features)__ meaning that at each node, the decision tree considers splitting on a sample of the features totaling the square root of the total number of features. \n",
    "- The random forest _can_ also be trained considering __all the features__ at every node. (These options can be controlled in the Scikit-Learn random forest implementation).\n",
    "\n",
    "\n",
    "The random forest combines hundreds or __thousands of decision trees__, trains each one on a __slightly different set of the observations__ (sampling the data points with replacement) and also __splits nodes in each tree considering only a limited number of the features.__ The final predictions made by the random forest are made by __averaging the predictions of each individual tree.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image1__1.png](https://c.mql5.com/2/33/image1__1.png)\n",
    "\n",
    "### Bootstrap Method\n",
    "Before we get to Bagging, let’s take a quick look at an important foundation technique called the bootstrap.\n",
    "\n",
    "The bootstrap is a powerful statistical method for estimating a quantity from a data sample. This is easiest to understand if the quantity is a descriptive statistic such as a mean or a standard deviation.\n",
    "\n",
    "Let’s assume we have a sample of 100 values (x) and we’d like to get an estimate of the mean of the sample.\n",
    "\n",
    "We can calculate the mean directly from the sample as:\n",
    "\n",
    "mean(x) = 1/100 * sum(x)\n",
    "\n",
    "We know that our sample is small and that our mean has error in it. We can improve the estimate of our mean using the bootstrap procedure:\n",
    "\n",
    "    Create many (e.g. 1000) random sub-samples of our dataset with replacement (meaning we can select the same value multiple times).\n",
    "    Calculate the mean of each sub-sample.\n",
    "    Calculate the average of all of our collected means and use that as our estimated mean for the data.\n",
    "    For example, let’s say we used 3 resamples and got the mean values 2.3, 4.5 and 3.3. Taking the average of these we could take the estimated mean of the data to be 3.367.\n",
    "\n",
    "This process can be used to estimate other quantities like the standard deviation and even quantities used in machine learning algorithms, like learned coefficients.\n",
    "\n",
    "### Bootstrap Aggregation (Bagging)\n",
    "\n",
    "Bootstrap Aggregation (or Bagging for short), is a simple and very powerful ensemble method.\n",
    "\n",
    "An ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model.\n",
    "\n",
    "Bootstrap Aggregation is a general procedure that can be used to reduce the variance for those algorithm that have high variance. An algorithm that has high variance are decision trees, like classification and regression trees (CART).\n",
    "\n",
    "Decision trees are sensitive to the specific data on which they are trained. If the training data is changed (e.g. a tree is trained on a subset of the training data) the resulting decision tree can be quite different and in turn the predictions can be quite different.\n",
    "\n",
    "Bagging is the application of the Bootstrap procedure to a high-variance machine learning algorithm, typically decision trees.\n",
    "\n",
    "Let’s assume we have a sample dataset of 1000 instances (x) and we are using the CART algorithm. Bagging of the CART algorithm would work as follows.\n",
    "\n",
    "Create many (e.g. 100) random sub-samples of our dataset with replacement.\n",
    "Train a CART model on each sample.\n",
    "Given a new dataset, calculate the average prediction from each model.\n",
    "For example, if we had 5 bagged decision trees that made the following class predictions for a in input sample: blue, blue, red, blue and red, we would take the most frequent class and predict blue.\n",
    "\n",
    "When bagging with decision trees, we are less concerned about individual trees overfitting the training data. For this reason and for efficiency, the individual decision trees are grown deep (e.g. few training samples at each leaf-node of the tree) and the trees are not pruned. These trees will have both high variance and low bias. These are important characterize of sub-models when combining predictions using bagging.\n",
    "\n",
    "The only parameters when bagging decision trees is the number of samples and hence the number of trees to include. This can be chosen by increasing the number of trees on run after run until the accuracy begins to stop showing improvement (e.g. on a cross validation test harness). Very large numbers of models may take a long time to prepare, but will not overfit the training data.\n",
    "\n",
    "Just like the decision trees themselves, Bagging can be used for classification and regression problems.\n",
    "\n",
    "\n",
    "### Random Forest\n",
    "Random Forests are an improvement over bagged decision trees.\n",
    "\n",
    "A problem with decision trees like CART is that they are greedy. They choose which variable to split on using a greedy algorithm that minimizes error. As such, even with Bagging, the decision trees can have a lot of structural similarities and in turn have high correlation in their predictions.\n",
    "\n",
    "Combining predictions from multiple models in ensembles works better if the predictions from the sub-models are uncorrelated or at best weakly correlated.\n",
    "\n",
    "Random forest changes the algorithm for the way that the sub-trees are learned so that the resulting predictions from all of the subtrees have less correlation.\n",
    "\n",
    "It is a simple tweak. In CART, when selecting a split point, the learning algorithm is allowed to look through all variables and all variable values in order to select the most optimal split-point. The random forest algorithm changes this procedure so that the learning algorithm is limited to a random sample of features of which to search.\n",
    "\n",
    "The number of features that can be searched at each split point (m) must be specified as a parameter to the algorithm. You can try different values and tune it using cross validation.\n",
    "\n",
    "            For classification a good default is: m = sqrt(p)\n",
    "            For regression a good default is: m = p/3\n",
    "Where m is the number of randomly selected features that can be searched at a split point and p is the number of input variables. For example, if a dataset had 25 input variables for a classification problem, then:\n",
    "\n",
    "            m = sqrt(25)\n",
    "            m = 5\n",
    "            \n",
    "![RF-predict.gif?width=600&name=RF-predict.gif](https://www.datascience.com/hs-fs/hubfs/RF-predict.gif?width=600&name=RF-predict.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Terminology related to Decision Trees [1]\n",
    "Let’s look at the basic terminology used with decision trees and random forests :\n",
    "\n",
    "**Root Node:** It represents entire population or sample and this further gets divided into two or more homogeneous sets.<br>\n",
    "**Splitting:** It is a process of dividing a node into two or more sub-nodes.<br>\n",
    "**Decision Node:** When a sub-node splits into further sub-nodes, then it is called decision node.<br>\n",
    "**Leaf/ Terminal Node:** Nodes do not split is called Leaf or Terminal node.<br>\n",
    "Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.<br>\n",
    "**Branch / Sub-Tree:** A sub section of entire tree is called branch or sub-tree.\n",
    "Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application where random forest algorithm is widely used.\n",
    "\n",
    "    Banking\n",
    "    Medicine\n",
    "    Stock Market\n",
    "    E-commerce\n",
    "Let’s begin with the banking sector.\n",
    "\n",
    "    1.Banking:\n",
    "In the banking sector, random forest algorithm widely used in two main application. These are for finding the loyal customer and finding the fraud customers.\n",
    "\n",
    "The loyal customer means not the customer who pays well, but also the customer whom can take the huge amount as loan and pays the loan interest properly to the bank. As the growth of the bank purely depends on the loyal customers. The bank customers data highly analyzed to find the pattern for the loyal customer based the customer details.\n",
    "\n",
    "In the same way, there is need to identify the customer who are not profitable for the bank, like taking the loan and paying the loan interest properly or find the outlier customers. If the bank can identify theses kind of customer before giving the loan the customer.  Bank will get a chance to not approve the loan to these kinds of customers. In this case, also random forest algorithm is used to identify the customers who are not profitable for the bank.\n",
    "\n",
    "    2.Medicine\n",
    "In medicine field, random forest algorithm is used identify the correct combination of the components to validate the medicine. Random forest algorithm also helpful for identifying the disease by analyzing the patient’s medical records.\n",
    "\n",
    "    3.Stock Market\n",
    "In the stock market, random forest algorithm used to identify the stock behavior as well as the expected loss or profit by purchasing the particular stock.\n",
    "\n",
    "    4.E-commerce\n",
    "In e-commerce, the random forest used only in the small segment of the recommendation engine for identifying the likely hood of customer liking the recommend products base on the similar kinds of customers.\n",
    "\n",
    "Running random forest algorithm on very large dataset requires high-end GPU systems. If you are not having any GPU system. You can always run the machine learning models in cloud hosted desktop. You can use clouddesktoponline platform to run high-end machine learning models from sitting any corner of the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types OF Random Forests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages of random forest algorithm\n",
    "Below are the advantages of random forest algorithm compared with other classification algorithms.\n",
    "\n",
    "    The overfitting problem will never come when we use the random forest algorithm in any classification problem.\n",
    "    The same random forest algorithm can be used for both classification and regression task.\n",
    "    The random forest algorithm can be used for feature engineering.\n",
    "        Which means identifying the most important features out of the available features from the training dataset.\n",
    "    It can be used for both classification and regression problems\n",
    "    Random forests make a wrong prediction only when more than half of the base classifiers are wrong\n",
    "    This algorithm can solve both type of problems i.e. classification and regression and does a decent estimation at both fronts.\n",
    "    One of benefits of Random forest which excites me most is, the power of handle large data set with higher dimensionality. It can handle thousands of input variables and identify most significant variables so it is considered as one of the dimensionality reduction methods. Further, the model outputs Importance of variable, which can be a very handy feature (on some random data set).\n",
    "    It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n",
    "    It has methods for balancing errors in data sets where classes are imbalanced.\n",
    "    The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection.\n",
    "    Random Forest involves sampling of the input data with replacement called as bootstrap sampling. Here one third of the data is not used for training and can be used to testing. These are called the out of bag samples. Error estimated on these out of bag samples is known as out of bag error. Study of error estimates by Out of bag, gives evidence to show that the out-of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set.\n",
    "    \n",
    "# Disadvantages  of random forest algorithm  \n",
    "    Random forests have been observed to overfit for some datasets with noisy classification/regression tasks.\n",
    "    It’s more complex and computationally expensive than decision tree algorithm.\n",
    "    It surely does a good job at classification but not as good as for regression problem as it does not give precise continuous nature predictions. In case of regression, it doesn’t predict beyond the range in the training data, and that they may over-fit data sets that are particularly noisy.\n",
    "    Random Forest can feel like a black box approach for statistical modelers – you have very little control on what the model does. You can at best – try different parameters and random seeds!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refrence \n",
    "\n",
    "https://dataaspirant.com/2017/05/22/random-forest-algorithm-machine-learing/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
