{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Regression Error Metrics in Python\n",
    "\n",
    "        \n",
    "        Mean Absolute Error (MAE)\n",
    "        Mean Square Error (MSE)\n",
    "        Root Mean Squared Error (RMSE)\n",
    "        Root Mean Squared Logarithmic Error (RMSLE)\n",
    "        Mean Squared Percentage Error (MSPE)\n",
    "        \n",
    "        Mean Absolute Percentage Error\n",
    "        Mean Percentage Error  \n",
    "      \n",
    "\n",
    "        R Squared (R²)\n",
    "        Adjusted R Squared (R²)        \n",
    "        \n",
    "\n",
    "## What is Absolute Error?\n",
    "\n",
    "Absolute Error is the amount of error in your measurements. It is the difference between the measured value and “true” value. For example, if a scale states 90 pounds but you know your true weight is 89 pounds, then the scale has an absolute error of 90 lbs – 89 lbs = 1 lbs.\n",
    "\n",
    "This can be caused by your scale not measuring the exact amount you are trying to measure. For example, your scale may be accurate to the nearest pound. If you weigh 89.6 lbs, the scale may “round up” and give you 90 lbs. In this case the absolute error is 90 lbs – 89.6 lbs = .4 lbs.\n",
    "\n",
    "                                    (Δx) = x(i) – x\n",
    "                                    \n",
    "Where:\n",
    "\n",
    "    xi is the measurement,\n",
    "    x is the true value.\n",
    "Using the first weight example above, the absolute error formula gives the same result:\n",
    "\n",
    "(Δx) = 90 lbs – 89 lbs = 1 lb.\n",
    "\n",
    "Sometimes you’ll see the formula written with the absolute value symbol (these bars: | |). This is often used when you’re dealing with multiple measurements:\n",
    "                                            \n",
    "                                    (Δx) = |x(i) – x|\n",
    "                                    \n",
    "The absolute value symbol is needed because sometimes the measurement will be smaller, giving a negative number. For example, if the scale measured 89 lbs and the true value was 95 lbs then you would have a difference of 89 lbs – 95 lbs = -6 lbs. On it’s own, a negative value is fine (-6 just means “six units below”) but the problem comes when you’re trying to add several values, some of which are positive and some are negative. For example, let’s say you have:\n",
    "\n",
    "89 lbs – 95 lbs = -6 lbs and\n",
    "\n",
    "98 lbs – 92 lbs = 6 lbs\n",
    "\n",
    "On their own, both measurements have absolute errors of 6 lbs. If you add them together, you should get a total of 12 lbs of error, but because of that negative sign you’ll actually get -6 lbs + 6 lbs = 0 lbs, which makes no sense at all — after all, there was a pretty big error (12 lbs) which has somehow become 0 lbs of error. We can solve this by taking the absolute value of the results and then adding:\n",
    "\n",
    "|-6 lbs| + |6 lbs| = 12 lbs.\n",
    "\n",
    "\n",
    "![BmBC8VW.jpg](https://i.imgur.com/BmBC8VW.jpg)\n",
    "\n",
    "\n",
    "\n",
    "## Mean Absolute Error\n",
    "The Mean Absolute Error(MAE) is the average of all absolute errors. The formula is:\n",
    "mean absolute error\n",
    "\n",
    "![MAE.png](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2016/10/MAE.png)\n",
    "\n",
    "The picture below is a graphical description of the MAE. The green line represents our model’s predictions, and the blue points represent our data. \n",
    "\n",
    "![tqnei6J.jpg](https://i.imgur.com/tqnei6J.jpg)\n",
    "\n",
    "__we use the absolute value of the residual, the MAE does not indicate underperformance or overperformance of the model__\n",
    "Each residual contributes proportionally to the total amount of error, meaning that larger errors will contribute linearly to the overall error. Like we’ve said above, \n",
    "__A small MAE suggests the model is great at prediction, while a large MAE suggests that your model may have trouble in certain areas.__ \n",
    "\n",
    "    A MAE of 0 means that your model is a perfect predictor of the outputs (but this will almost never happen).\n",
    "    \n",
    "\n",
    "Where:\n",
    "\n",
    "    n = the number of errors,\n",
    "    Σ = summation symbol (which means “add them all up”),\n",
    "    |xi – x| = the absolute errors.\n",
    "    \n",
    "The formula may look a little daunting, but the steps are easy:\n",
    "\n",
    "    Find all of your absolute errors, xi – x.\n",
    "    Add them all up.\n",
    "    Divide by the number of errors. For example, if you had 10 measurements, divide by 10.\n",
    "    \n",
    "__in your data, you may want to bring more attention to these outliers or downplay them. The issue of outliers can play a major role in which error metric you use.__\n",
    "\n",
    "In MAE the error is calculated as an average of absolute differences between the target values and the predictions. The MAE is a linear score which means that all the individual differences are weighted equally in the average. For example, the difference between 10 and 0 will be twice the difference between 5 and 0. However, same is not true for RMSE.\n",
    "\n",
    "MAE is widely used in finance, where $10 error is usually exactly two times worse than $5 error. On the other hand, MSE metric thinks that $10 error is four times worse than $5 error. MAE is easier to justify than RMSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Mean square error\n",
    "\n",
    "The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. We can see this difference in the equation below. \n",
    "\n",
    "![vB3UAiH.jpg](https://i.imgur.com/vB3UAiH.jpg)\n",
    "\n",
    "Because we are squaring the difference, __the MSE will almost always be bigger than the MAE.__ For this reason, we cannot directly compare the MAE to the MSE\n",
    "\n",
    "We can only compare our model’s error metrics to those of a competing model. The effect of the square term in the MSE equation is most apparent with the presence of outliers in our data\n",
    "\n",
    "While each residual in MAE contributes proportionally to the total error, __the error grows quadratically in MSE.__ This ultimately means that outliers in our data will contribute to much higher total error in the MSE than they would the MAE.  \n",
    "\n",
    "![mLn8AeW.jpg](https://i.imgur.com/mLn8AeW.jpg)\n",
    "\n",
    "Outliers will produce these exponentially larger differences, and it is our job to judge how we should approach them.\n",
    "\n",
    "\n",
    "The higher this value, the worse the model is. It is never negative, since we’re squaring the individual prediction-wise errors before summing them, but would be zero for a perfect model .\n",
    "\n",
    "__Advantage:__ Useful if we have unexpected values that we should care about. Vey high or low value that we should pay attention.\n",
    "\n",
    "__Disadvantage:__ If we make a single very bad prediction, the squaring will make the error even worse and it may skew the metric towards overestimating the model’s badness. That is a particularly problematic behaviour if we have noisy data (that is, data that for whatever reason is not entirely reliable) — even a “perfect” model may have a high MSE in that situation, so it becomes hard to judge how well the model is performing. On the other hand, if all the errors are small, or rather, smaller than 1, than the opposite effect is felt: we may underestimate the model’s badness.\n",
    "\n",
    "\n",
    "Mean square error is always positive and a value closer to 0 or a lower value is better. Let’s see how this this is calculated;\n",
    "\n",
    "|Actual Value (y)|Predicted Value (y hat)|Error (difference)|Squared Error|​|\n",
    "|---|---|---|---|---|\n",
    "|100|130|-30|900||\n",
    "|150|170|-20|400|​|\n",
    "|200|220|-20|400|​|\n",
    "|250|260|-10|100|​|\n",
    "|300|325|-25|625|​|\n",
    "||​|​|485|Mean|\n",
    "\n",
    "if we were to run a model with different parameters/independent variables, model with lower MSE will be deemed better.\n",
    "\n",
    "\n",
    "### The problem of outliers\n",
    " Do we include the outliers in our model creation or do we ignore them? The answer to this question is dependent on the field of study, the data set on hand and the consequences of having errors in the first place.\n",
    " \n",
    "1) For example, I know that some video games achieve superstar status and thus have disproportionately higher earnings. Therefore, it would be foolish of me to ignore these outlier games because they represent a real phenomenon within the data set.  I would want to use the MSE to ensure that my model takes these outliers into account more\n",
    "\n",
    "2) If I wanted to downplay their significance, I would use the MAE since the outlier residuals won’t contribute as much to the total error as MSE. Ultimately, the choice between is MSE and MAE is application-specific and depends on how you want to treat large errors. Both are still viable error metrics, but will describe different nuances about the prediction errors of your model.\n",
    "\n",
    "\n",
    "Another error metric you may encounter is the root mean squared error (RMSE). As the name suggests, it is the square root of the MSE. Because the MSE is squared, its units do not match that of the original output. Researchers will often use RMSE to convert the error metric back into similar units, making interpretation easier. Since the MSE and RMSE both square the residual, they are similarly affected by outliers. The RMSE is analogous to the standard deviation (MSE to variance) and is a measure of how large your residuals are spread out. Both MAE and MSE can range from 0 to positive infinity, so as both of these measures get higher, it becomes harder to interpret how well your model is performing. Another way we can summarize our collection of residuals is by using percentages so that each prediction is scaled against the value it’s supposed to estimate.\n",
    "\n",
    "Like MAE, we’ll calculate the MSE for our model. Thankfully, the calculation is just as simple as MAE.\n",
    "\n",
    "        mse_sum = 0\n",
    "        for sale, x in zip(sales, X):\n",
    "        prediction = lm.predict(x)\n",
    "        mse_sum += (sale - prediction)**2\n",
    "        mse = mse_sum / len(sales)\n",
    "        print(mse)\n",
    "        >>> [ 3.53926581 ]\n",
    "With the MSE, we would expect it to be much larger than MAE due to the influence of outliers. We find that this is the case: the MSE is an order of magnitude higher than the MAE. The corresponding RMSE would be about 1.88, indicating that our model misses actual sale values by about $ 1.8M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error\n",
    "\n",
    "Most popular evalution metric usind in Regression problems.\n",
    "\n",
    "RMSE is just the square root of MSE. The square root is introduced to make scale of the errors to be the same as the scale of targets.\n",
    "\n",
    "![1*qz8jRMxmMEwNsFh0Cs5XfQ.png](https://cdn-images-1.medium.com/max/720/1*qz8jRMxmMEwNsFh0Cs5XfQ.png)\n",
    "\n",
    "For example, if we have two sets of predictions, A and B, and say MSE of A is greater than MSE of B, then we can be sure that RMSE of A is greater RMSE of B.And it also works in the opposite direction\n",
    "\n",
    "![1*qz8jRMxmMEwNsFh0Cs5XfQ.png](https://cdn-images-1.medium.com/max/720/1*e9NYGLz3a9wdKpYMuLTI0Q.png)\n",
    "\n",
    "\n",
    "\n",
    "Higly affected by outlier values.\n",
    "Square nature of this metric helps to deliver more robust reuslt which prevents cancelling the positive and negotive error values.\n",
    "\n",
    "__Model 1__\n",
    "\n",
    "|Actual Value|Predicated Vlaue|Error||\n",
    "|------------|------------|------------||\n",
    "|250|240|10||\n",
    "|645|600|45||\n",
    "|800|825|-25||\n",
    "\n",
    "__Model 2__\n",
    "\n",
    "|Actual Value|Predicated Vlaue|Error||\n",
    "|------------|------------|------------||\n",
    "|250|280|-30||\n",
    "|645|1200|-555||\n",
    "|800|1600|-800||\n",
    "\n",
    "__Result__\n",
    "\n",
    "|Error Matric|Model 1|Model 2||\n",
    "|------------|------------|------------||\n",
    "|MAE|10|10||\n",
    "|MSE|916.6666|316308.333||\n",
    "|RMSE|30.27|562.412||\n",
    "\n",
    "            MAE are same for both model.\n",
    "            MSE Punishes large errors.\n",
    "            MSE and RMSE value are less for model 1 so better is first model.\n",
    "            \n",
    "#### MAE and RMSE — Which Metric is Better?\n",
    "\n",
    "__Mean Absolute Error (MAE)__\n",
    "\n",
    "![1*OVlFLnMwHDx08PHzqlBDag.gif](https://cdn-images-1.medium.com/max/800/1*OVlFLnMwHDx08PHzqlBDag.gif)\n",
    "\n",
    "__Root mean squared error (RMSE)__\n",
    "![1*OVlFLnMwHDx08PHzqlBDag.gif](https://cdn-images-1.medium.com/max/800/1*9hQVcasuwx5ddq_s3MFCyw.gif)\n",
    "\n",
    "The three tables below show examples where MAE is steady and RMSE increases as the variance associated with the frequency distribution of error magnitudes also increases.\n",
    "\n",
    "![1*YTxb8K2XZIisC944v6rERw.png](https://cdn-images-1.medium.com/max/800/1*YTxb8K2XZIisC944v6rERw.png)\n",
    "\n",
    "    1) [MAE] ≤ [RMSE]. The RMSE result will always be larger or equal to the MAE. If all of the errors have the same magnitude, then RMSE=MAE.\n",
    "\n",
    "    2) [RMSE] ≤ [MAE * sqrt(n)], where n is the number of test samples. The difference between RMSE and MAE is greatest when all of the prediction error comes from a single test sample. The squared error then equals to [MAE^2 * n] for that single test sample and 0 for all other samples. Taking the square root, RMSE then equals to [MAE * sqrt(n)].\n",
    "\n",
    "![1*HmnyRcMjgfW-Bo2_NKLYqg.png](https://cdn-images-1.medium.com/max/720/1*HmnyRcMjgfW-Bo2_NKLYqg.png)\n",
    "\n",
    "\n",
    "\n",
    "https://www.dataquest.io/blog/understanding-regression-error-metrics/\n",
    "https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Logarithmic Error\n",
    "\n",
    "While calculating RMSLE, 1 is added as constant to actual and predicted values because they can be 0 and log of 0 is undefined. Overall formula remains same. Standard denotation for RMSLE is;\n",
    "\n",
    "\n",
    "![rmsle-2.png](https://akhilendra.com/wp-content/uploads/2019/03/rmsle-2.png)\n",
    "\n",
    "\n",
    "|Actual Value (y)|Predicted Value (y hat)|Actual + 1|Predicted + 1|log (Actual)|Log (Predicted)|Error (difference)|Squared Error||\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|100|130|101|131|2.004321374|2.117271296|-0.112949922|0.012757685||\n",
    "|150|170|151|171|2.178976947|2.23299611|-0.054019163|0.00291807||\n",
    "|200|220|201|221|2.303196057|2.344392274|-0.041196216|0.001697128||\n",
    "|250|260|251|261|2.399673721|2.416640507|-0.016966786|0.000287872||\n",
    "|300|325|301|326|2.478566496|2.5132176|-0.034651104|0.001200699||\n",
    "||||||||0.003772291|Mean|\n",
    "||||||||0.061418977|Squre root of mean|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Mean absolute Error (MAE)|Mean square Error (MSE)|Root mean square error (RMSE)|Root mean square log Error (RMSLE)|\n",
    "|-|-|-|-|\n",
    "|It doesn’t account for the direction of the value. Even if value is negative, positive value is used for calculation.|It does account for positive or negative value.|It does account for positive or negative value.|It does account for positive or negative value.|\n",
    "||RMSE & MSE share many properties with MSE because RMSE is simply the square root of MSE.|RMSE & MSE share many properties with MSE because it is simply the square root of MSE.||\n",
    "|MAE is less biased for higher values. It may not adequately reflect the performance when dealing with large error values.|MSE is highly biased for higher values.|RMSE is better in terms of reflecting performance when dealing with large error values.||\n",
    "|||RMSE is more useful when lower residual values are preferred.||\n",
    "|MAE is less than RMSE as the sample size goes up.||RMSE tends to be higher than MAE as the sample size goes up.||\n",
    "|MAE doesn’t necessarily penalize large errors.|MSE penalize large errors.|RMSE penalize large errors.|RMSLE doesn’t penalize large errors. It is usually used when you don’t want to influence the results if there are large errors. RMSLE penalize lower errors.|\n",
    "|MAE is more useful when the overall impact is proportionate to the actual increase in error. For example- if error values go up to 6 from 3, actual impact on the result is twice. It is more common in financial industry where a loss of 6 would be twice of 3.||RMSE is more useful when the overall impact is disproportionate to the actual increase in error. For example- if error values go up to 6 from 3, actual impact on the result is more than twice. This could be common in clinical trials, as error goes up, overall impact goes up disproportionately.||\n",
    "|||When actual and predicted values are low, RMSE & RMSLE are usually same.|When actual and predicted values are low, RMSE & RMSLE are usually same.|\n",
    "|||When either of actual or predicted values are high, RMSE > RMSLE.|When either of actual or predicted values are high, RMSE > RMSLE.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Percentage Error\n",
    "__First find the Error:__\n",
    "Subtract one value from the other. Ignore any minus sign.\n",
    "\n",
    "Example: I estimated 260 people, but 325 came. \n",
    "    \n",
    "    260 − 325 = −65, ignore the \"−\" sign, so my error is 65\n",
    "\n",
    "__Then find the Percentage Error: __\n",
    "Show the error as a percent of the exact value, so divide by the exact value and make it a percentage:\n",
    "\n",
    "Example continued: 65/325 = 0.2 = 20%\n",
    "This is the formula for \"Percentage Error\":\n",
    "\n",
    "      |Approximate Value − Exact Value|  × 100%\n",
    "      ---------------------------------\n",
    "            |Exact Value|\n",
    "        \n",
    "(The \"|\" symbols mean absolute value, so negatives become positive)\n",
    "\n",
    "\n",
    "__Example:__ I thought 70 people would turn up to the concert, but in fact 80 did!\n",
    "\n",
    "            |70 − 80||80|\t× 100% =  1080 × 100% = 12.5%\n",
    "            I was in error by 12.5%\n",
    "\n",
    "\n",
    "__Example:__ The report said the carpark held 240 cars, but we counted only 200 parking spaces.\n",
    "\n",
    "            |240 − 200||200|\t× 100% =  40200 × 100% = 20%\n",
    "            The report had a 20% error.\n",
    "\n",
    "\n",
    "        Use Percentage Change when comparing an Old Value to a New Value\n",
    "        \n",
    "            Percent Change =  New Value − Old Value/|Old Value|  × 100%\n",
    "            \n",
    "        Use Percentage Error when comparing an Approximate Value to an Exact Value\n",
    "            \n",
    "            Percent Error =  |Approximate Value − Exact Value|/|Exact Value|  × 100%\n",
    "            \n",
    "        Use Percentage Difference when both values mean the same kind of thing (one value is not obviously older or better than the other).\n",
    "        \n",
    "            Percentage Difference = | First Value − Second Value(First Value + Second Value)/2 | × 100%\n",
    "\n",
    "\n",
    "\n",
    "__Example:__ fence (continued)\n",
    "\n",
    "        Length = 12.5 ±0.05 m\n",
    "\n",
    "        So: Absolute Error = 0.05 m\n",
    "\n",
    "        And: Relative Error =   0.05 m12.5 m   = 0.004\n",
    "\n",
    "        And: Percentage Error = 0.4%\n",
    "\n",
    "__Example:__ The thermometer measures to the nearest 2 degrees. The temperature was measured as 38° C The temperature could be up to 1° either side of 38° (i.e. between 37° and 39°)\n",
    "\n",
    "        Temperature = 38 ±1°\n",
    "\n",
    "        So:Absolute Error = 1°\n",
    "\n",
    "        And:Relative Error =   1°38°   = 0.0263...\n",
    "\n",
    "        And:Percentage Error = 2.63...% \n",
    "\n",
    "\n",
    "The mean absolute percentage error (MAPE) is the percentage equivalent of MAE. The equation looks just like that of MAE, but with adjustments to convert everything into percentages.\n",
    "![YYMpqUY.jpg](https://i.imgur.com/YYMpqUY.jpg)\n",
    "\n",
    "![HPlrPmu.jpg](https://i.imgur.com/HPlrPmu.jpg)\n",
    "\n",
    "The MAPE is biased towards predictions that are systematically less than the actual values themselves. That is to say, MAPE will be lower when the prediction is lower than the actual compared to a prediction that is higher by the same amount. The quick calculation below demonstrates this point. \n",
    "\n",
    "![HPlrPmu.jpg](https://i.imgur.com/OBBvmIH.jpg)\n",
    "\n",
    "We have a measure similar to MAPE in the form of the mean percentage error. While the absolute value in MAPE eliminates any negative values, the mean percentage error incorporates both positive and negative errors into its calculation.\n",
    "\n",
    "mape_sum = 0\n",
    "for sale, x in zip(sales, X):\n",
    "    prediction = lm.predict(x)\n",
    "    mape_sum += (abs((sale - prediction))/sale)\n",
    "    mape = mape_sum/len(sales)\n",
    "    print(mape)\n",
    ">>> [ 5.68377867 ]\n",
    "\n",
    "We know for sure that there are no data points for which there are zero sales, so we are safe to use MAPE. Remember that we must interpret it in terms of percentage points. MAPE states that our model’s predictions are, on average, 5.6% off from actual value.<br>\n",
    "\n",
    "\n",
    "### Mean percentage error\n",
    "\n",
    "The mean percentage error (MPE) equation is exactly like that of MAPE. The only difference is that it lacks the absolute value operation.\n",
    "![ndIXERr.jpg](https://i.imgur.com/ndIXERr.jpg)\n",
    "\n",
    "Even though the MPE lacks the absolute value operation, it is actually its absence that makes MPE useful. Since positive and negative errors will cancel out, we cannot make any statements about how well the model predictions perform overall. However, if there are more negative or positive errors, this bias will show up in the MPE. Unlike MAE and MAPE, MPE is useful to us because it allows us to see if our model __systematically underestimates (more negative error) or overestimates (positive error). __\n",
    "\n",
    "![kTIYRBX.jpg](https://i.imgur.com/kTIYRBX.jpg)\n",
    "\n",
    "|acroynm|full name|residual operation?|robust to outliers?|\n",
    "|------------|------------|------------|------------|\n",
    "|MAE|Mean Absolute Error|Absolute Value|Yes|\n",
    "|MSE|Mean Squared Error|Square|No|\n",
    "|RMSE|Root Mean Squared Error|Square|No|\n",
    "|MAPE|Mean Absolute Percentage Error|Absolute Value|Yes|\n",
    "|MPE|Mean Percentage Error|N/A|Yes|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Squared\n",
    "\n",
    "How much difference in outcome is explained by the model.\n",
    "\n",
    "R-squared (R2) is a statistical measure that \n",
    "\n",
    "1) represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n",
    "\n",
    "2) Whereas correlation explains the strength of the relationship between an independent and dependent variable, R-squared explains to what extent the variance of one variable explains the variance of the second variable.\n",
    "\n",
    "3) So, if the R2 of a model is 0.50, then approximately half of the observed variation can be explained by the model's inputs.\n",
    "\n",
    "\n",
    "![R2.png](R2.png)\n",
    "\n",
    "Here R2 is 0.6 , If the points are colse to each other R2 will be 1 , so its perfect fit.\n",
    "And vice versa, if R2  is very small  like R2 is .02. the estimated point will show as below.\n",
    "R2 ---> 0 { There are no relation ship between}\n",
    "\n",
    "R2 ---> 1 { There are close releation ship\n",
    "\n",
    "![R2_1.png](R2_1.png)\n",
    "\n",
    "![R2_2.png](R2_2.png)\n",
    "\n",
    "Example : If house R2 is 70% then what about other 30% . That means 30% there might be depedent on other variables.\n",
    "might be , location , repair , style, bedrooms, etc...\n",
    "\n",
    "![R2_3.png](R2_3.png)\n",
    "\n",
    "1) Jane estimated value much higher. may be good performance<br>\n",
    "2) Bob is very low might be he hate his boss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusted R Squared\n",
    "![R2_6.png](R2_6.png)\n",
    "![R2_4.png](R2_4.png) ![R2_5.png](R2_5.png)\n",
    "\n",
    "Is it good to have as many independent variables as possible ? Nope <br>\n",
    "R-Square is deceptive. R-Squared never decreases when a new X variable is added to model - True? <br>\n",
    "We need a better measure or an adjustment to the original R-Squared Formula . <br>\n",
    "\n",
    "__used For__\n",
    "\n",
    "    1) To compare model with different no. of independent variable\n",
    "    2) To select important predictors ( independent variable) for regression model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Classification\n",
    "\n",
    "    Accuracy\n",
    "    confusion matrix "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "\n",
    "__Classification Error__\n",
    "\n",
    "                       Errors        FP + FN\n",
    "                       ------  =  ----------------- = Classification Error\n",
    "                       Total     TP + TN + FP +FN\n",
    "\n",
    "__Accuracy__\n",
    "\n",
    "Ratio of number of correct predictions to total number of samples.\n",
    "\n",
    "            Number of correct predictions         TP + TN\n",
    "            --------------------------     = --------------------  = (1 - error) = Accuracy\n",
    "         \n",
    "            Total number of predicitions       TP + TN + FP +FN\n",
    "\n",
    "Basic Measure of \"goodness\" of a classifier\n",
    "\n",
    "\n",
    "    a positive example classified as positive. This is a true positive.\n",
    "    a positive example misclassified as negative. This is a false negative.\n",
    "    a negative example classified as negative. This is a true negative.\n",
    "    a negative example misclassified as positive. This is a false positive.\n",
    "    \n",
    "    \n",
    "__Balanced and Imbalanced__\n",
    "\n",
    "1) Balanced Dataset - ~Equal number of +ve and -ve samples.<br>\n",
    "2) Imbalanced dataset - one class significantly dominates others.\n",
    "\n",
    "__When to use Accuracy__\n",
    "\n",
    "When target variable data are nearly balanced.<br>\n",
    "1) For Ex. if 55% classses in fruits dataset are oranges and 45 % are apples.\n",
    "\n",
    "__When not to use Accuracy__\n",
    "\n",
    "When target variables classes in data are majority of one class.<br>\n",
    "1) For Ex. In cancer detection dataset of 100 only 5 people has cancer.\n",
    "\n",
    "Let's say, we are working on classfication problem, where we are predicting whether a person a having cancer or not.\n",
    "\n",
    "|Cancer Detection Dataset||\n",
    "|---|---|\n",
    "|No. of people having cancer|5|\n",
    "|No.of people not having cancer|95|\n",
    "|Total People|100|\n",
    "\n",
    "Assigning labels to our target variables.<br>\n",
    "1: when person is having Cancer<br>\n",
    "0: when person is not having Cancer\n",
    "\n",
    "Suppose model has classfied all patient as __not__ Having cancer.\n",
    "Accuracy of model = 95/100*100 = 95%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confusion matrix \n",
    "\n",
    "It is a table with 4 different combinations of predicted and actual values.<br>\n",
    "Confusion matrix only can calculated when true value are know.\n",
    "![1*Z54JgbS4DUwWSknhDCvNTQ.png](https://cdn-images-1.medium.com/max/720/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n",
    "\n",
    "It is extremely useful for measuring\n",
    "\n",
    "    Recall, \n",
    "    Precision, \n",
    "    Specificity, \n",
    "    Accuracy and most importantly \n",
    "    AUC-ROC Curve.\n",
    "    \n",
    "Let’s understand TP, FP, FN, TN in terms of pregnancy analogy.\n",
    "![1*7EYylA6XlXSGBCF77j_rOA.png](https://cdn-images-1.medium.com/max/720/1*7EYylA6XlXSGBCF77j_rOA.png) \n",
    "\n",
    "Suppose I work for Target and I want to detect pregnant teenagers. So based on based on shopping patterns. I take a random sample of 500 female, teenage customers. Of these teenagers, 50 are actually pregnant. I predicted 100 total pregnant teenagers, 45 of which are actually pregnant.\n",
    "\n",
    "Our task is two-fold:<br>\n",
    "A) Identify the TP, TN, FP, FN, and construct a confusion matrix and <br>\n",
    "B) Calculate the accuracy, misclassification, precision, sensitivity, and specificity\n",
    "\n",
    "![1*8YioEcYGAYKbkQafKwaC4w.png](https://cdn-images-1.medium.com/max/720/1*8YioEcYGAYKbkQafKwaC4w.png)\n",
    "Next, we can use our labelled confusion matrix to calculate our metrics.\n",
    "\n",
    "1. Accuracy (all correct / all) = TP + TN / TP + TN + FP + FN\n",
    "        (45 + 395) / 500 = 440 / 500 = 0.88 or 88% Accuracy\n",
    "\n",
    "2. Misclassification (all incorrect / all) = FP + FN / TP + TN + FP + FN\n",
    "        (55 + 5) / 500 = 60 / 500 = 0.12 or 12% Misclassification\n",
    "        You can also just do 1 — Accuracy, so:\n",
    "        1–0.88 = 0.12 or 12% Misclassification\n",
    "\n",
    "3. Precision (true positives / predicted positives) = TP / TP + FP\n",
    "        45 / (45 + 55) = 45 / 100 = 0.45 or 45% Precision\n",
    "\n",
    "4. Sensitivity aka Recall (true positives / all actual positives) = TP / TP + FN\n",
    "        45 / (45 + 5) = 45 / 50 = 0.90 or 90% Sensitivity\n",
    "\n",
    "5. Specificity (true negatives / all actual negatives) =TN / TN + FP\n",
    "        395 / (395 + 55) = 395 / 450 = 0.88 or 88% Specificity\n",
    "\n",
    "\n",
    "![1*PBgDG3NtVMXFrkYgAG00Jw.png](https://cdn-images-1.medium.com/max/720/1*PBgDG3NtVMXFrkYgAG00Jw.png)\n",
    "\n",
    "True Positive:<br>\n",
    "Interpretation: You predicted positive and it’s true. You predicted that a woman is pregnant and she actually is.<br>\n",
    "True Negative:<br>\n",
    "Interpretation: You predicted negative and it’s true. You predicted that a man is not pregnant and he actually is not.<br>\n",
    "False Positive: (Type 1 Error) <br>\n",
    "Interpretation: You predicted positive and it’s false.You predicted that a man is pregnant but he actually is not.<br>\n",
    "False Negative: (Type 2 Error) <br>\n",
    "Interpretation: You predicted negative and it’s false.You predicted that a woman is not pregnant but she actually is\n",
    "\n",
    "![confusion_matrix_simple2.png](https://www.dataschool.io/content/images/2015/01/confusion_matrix_simple2.png)\n",
    "What can we learn from this matrix?\n",
    "\n",
    "1) There are two possible predicted classes: \"yes\" and \"no\". If we were predicting the presence of a disease, for example, \"yes\" would mean they have the disease, and \"no\" would mean they don't have the disease.<br>\n",
    "2) The classifier made a total of 165 predictions (e.g., 165 patients were being tested for the presence of that disease).<br>\n",
    "3) Out of those 165 cases, the classifier predicted \"yes\" 110 times, and \"no\" 55 times.<br>\n",
    "4) In reality, 105 patients in the sample have the disease, and 60 patients do not.<br>\n",
    "\n",
    "Let's now define the most basic terms, which are whole numbers (not rates):\n",
    "\n",
    "true positives (TP): These are cases in which we predicted yes (they have the disease), and they do have the disease.<br>\n",
    "true negatives (TN): We predicted no, and they don't have the disease.<br>\n",
    "false positives (FP): We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")<br>\n",
    "false negatives (FN): We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")<br>\n",
    "\n",
    "![confusion_matrix2.png](https://www.dataschool.io/content/images/2015/01/confusion_matrix2.png)\n",
    "\n",
    "This is a list of rates that are often computed from a confusion matrix for a binary classifier:\n",
    "\n",
    "__Accuracy:__ Overall, how often is the classifier correct?<br>\n",
    "(TP+TN)/total = (100+50)/165 = 0.91<br>\n",
    "__Misclassification Rate:__ Overall, how often is it wrong?<br>\n",
    "(FP+FN)/total = (10+5)/165 = 0.09<br>\n",
    "equivalent to 1 minus Accuracy<br>\n",
    "also known as \"Error Rate\"<br>\n",
    "__True Positive Rate:__ When it's actually yes, how often does it predict yes?<br>\n",
    "TP/actual yes = 100/105 = 0.95<br>\n",
    "also known as \"Sensitivity\" or \"Recall\"<br>\n",
    "__False Positive Rate:__ When it's actually no, how often does it predict yes?<br>\n",
    "FP/actual no = 10/60 = 0.17<br><br>\n",
    "__True Negative Rate:__ When it's actually no, how often does it predict no?<br>\n",
    "TN/actual no = 50/60 = 0.83<br>\n",
    "equivalent to 1 minus False Positive Rate<br>\n",
    "also known as \"Specificity\"<br>\n",
    "__Precision:__ When it predicts yes, how often is it correct?<br>\n",
    "TP/predicted yes = 100/110 = 0.91<br>\n",
    "__Prevalence:__ How often does the yes condition actually occur in our sample?<br>\n",
    "actual yes/total = 105/165 = 0.64<br>\n",
    "\n",
    "![Accuracy.png](Accuracy.png)\n",
    "\n",
    "![Accuracy_2.png](Accuracy_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-Score or  F-Measure\n",
    "\n",
    "    Is single mesure of classfication procedure's useful.\n",
    "    Consider both the Precision and Recall of the procedure to compute the score.\n",
    "    The higher the F-Score the better the predictive power of the classfication procedure. \n",
    "    A score of 1 means the classfication is perfect and 0 means lowest possbile F-score.\n",
    "\n",
    "![F-Score.png](F-Score.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC - AUC\n",
    "\n",
    "\n",
    "__What is AUC - ROC Curve?__\n",
    "\n",
    "ROC (Receiver Operating Characteristic) Curve tells us about how good the model can distinguish between two things (e.g If a patient has a disease or no).  Better models can accurately distinguish between the two. Whereas, a poor model will have difficulties in distinguishing between the two.\n",
    "\n",
    "1) AUC - ROC curve is a performance measurement for classification problem at various thresholds settings.<br>\n",
    "2)  ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes.<br>\n",
    "3) Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.<br>\n",
    "4) The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.<br>\n",
    "\n",
    "__Defining terms used in AUC and ROC Curve.__\n",
    "\n",
    "TPR(True Positive rate) /Recall/ Sensitivity =  TP / (TP+ FN )\n",
    "\n",
    "\n",
    "Specificity = TN / (TN+ FP)\n",
    "\n",
    "FPR = FPR = 1 - Specificity  = FP / ( FP + TN)\n",
    "\n",
    "\n",
    "    When two curves don’t overlap at all means model has an ideal measure of separability. It is perfectly able to distinguish between positive class and negative class.\n",
    "    \n",
    "    AUC is 0.7,it means there is 70% chance that model will be able to distinguish between positive class and negative class.\n",
    "    \n",
    "    When AUC approximately 0.5,model has no discrimination capacity to distinguish between positive class and negative class.\n",
    "    \n",
    "    When AUC is approximately 0, model is actually reciprocating the classes. It means, model is predicting negative class as a positive class and vice versa\n",
    "   \n",
    "   \n",
    "__Sensitivity and Specificity are inversely proportional to each other. So when we increase Sensitivity, Specificity decreases and vice versa.__\n",
    "\n",
    "When we __decrease the threshold__, we get more positive values thus it increases the sensitivity and decreasing the specificity.\n",
    "\n",
    "Similarly, when we __increase the threshold__, we get more negative values thus we get higher specificity and lower sensitivity.\n",
    "\n",
    "![AUC_ROC_5.png](AUC_ROC_5.png)\n",
    "__Step - 1__ Here, the red distribution represents all the patients who do not have the disease and the green distribution represents all the patients who have the disease.\n",
    "\n",
    "__Step - 2__ Now we got to pick a value where we need to set the cut off i.e. a threshold value, above which we will predict everyone as positive (they have the disease) and below which will predict as negative (they do not have the disease). We will set the threshold at “0.5” as shown below:\n",
    "\n",
    "__Step - 3__ All the positive values above the threshold will be “True Positives” and the negative values above the threshold will be “False Positives” as they are predicted incorrectly as positives.\n",
    "\n",
    "__Step - 4__ All the negative values below the threshold will be “True Negatives” and the positive values below the threshold will be “False Negative” as they are predicted incorrectly as negatives.\n",
    "\n",
    "__Step - 5__ Here, we have got a basic idea of the model predicting correct and incorrect values with respect to the threshold set. Before we move on, let’s go through two important terms: Sensitivity and Specificity.\n",
    "\n",
    "__What is Sensitivity and Specificity?__\n",
    "\n",
    "In simple terms, the proportion of patients that were identified correctly to have the disease (i.e. True Positive) upon the total number of patients who actually have the disease is called as Sensitivity or Recall.\n",
    "\n",
    "Similarly, the proportion of patients that were identified correctly to not have the disease (i.e. True Negative) upon the total number of patients who do not have the disease is called as Specificity.\n",
    "\n",
    "    \n",
    "    Trade-off between Sensitivity and Specificity\n",
    "    \n",
    "When we decrease the threshold, we get more positive values thus increasing the sensitivity. Meanwhile, this will decrease the specificity.\n",
    "\n",
    "Similarly, when we increase the threshold, we get more negative values thus increasing the specificity and decreasing sensitivity.\n",
    "\n",
    "As Sensitivity ⬇️ Specificity ⬆️\n",
    "\n",
    "As Specificity ⬇️ Sensitivity ⬆️\n",
    "\n",
    "    Area Under the Curve\n",
    "    \n",
    "The AUC is the area under the ROC curve. This score gives us a good idea of how well the model performances.\n",
    "\n",
    "Let’s take a few examples\n",
    "\n",
    "\n",
    "\n",
    "Let''s Start with some examples.\n",
    "\n",
    "1) The Y -axis has two categories. \"Obese\" and \"Not Obese\" <br>\n",
    "2) Along the X-Axis we do have weight.\n",
    "\n",
    "![AUC_ROC.png](AUC_ROC.png)\n",
    "However, if we want to classify the mice as \"Obese\" and \"Not Obese\" , then we need a way to turn probabilites into classification. One way to classfiy mice is to set a threshold at 0.5 and classfiy all mice with a probabilites of being obese > 0.5 and \n",
    "\n",
    "![AUC_ROC_2.png](AUC_ROC_2.png)\n",
    "![AUC_ROC_3.png](AUC_ROC_3.png)\n",
    "![AUC_ROC_4.png](AUC_ROC_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
