{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree (Enhanced greedy search algorithm )\n",
    "\n",
    "Decision Trees are a type of Supervised Machine Learning where the data is continuously split according to a certain parameter. The tree can be explained by two entities, namely decision nodes and leaves. The leaves are the decisions or the final outcomes. And the decision nodes are where the data is split.\n",
    "\n",
    "The general motive of using Decision Tree is to create a training model which can use to predict class or value of target variables by learning decision rules inferred from prior data(training data).\n",
    "\n",
    "The understanding level of Decision Trees algorithm is so easy compared with other classification algorithms. The decision tree algorithm tries to solve the problem, by using tree representation. Each internal node of the tree corresponds to an attribute, and each leaf node corresponds to a class label.\n",
    "\n",
    "![image](image/Decision-Tree-example.png)\n",
    "\n",
    "    A Decision Tree splits the data into multiple sets.\n",
    "    Then each of these set is further split into subsets to arrive at a decision.\n",
    "    Decision tree is a type of Supervised Machine Learning.\n",
    "    It works for both categorical and continuous input and output variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Types\n",
    "Decision Tree algorithms are referred to as __CART or Classification and Regression Trees.__<br>\n",
    "There are two main types of Decision Trees:\n",
    "\n",
    "__Classification trees (Yes/No types)__<br>\n",
    "where the outcome was a variable like ‘fit’ or ‘unfit’. Here the decision variable is Categorical.where the target variable is categorical and the tree is used to identify the \"class\" within which a target variable would likely fall into.\n",
    "\n",
    "__Regression trees (Continuous data types)__ <br>\n",
    "Here the decision or the outcome variable is Continuous, e.g. a number like 123.where the target variable is continuous and tree is used to predict it's value.\n",
    "\n",
    "![image](image/Types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Important Terminology related to Decision Trees\n",
    "\n",
    "Let’s look at the basic terminology used with Decision trees:\n",
    "\n",
    "- __Root Node__: <br/>It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "- __Splitting__: <br/>It is a process of dividing a node into two or more sub-nodes.\n",
    "- __Decision Node__:<br/> When a sub-node splits into further sub-nodes, then it is called decision node.\n",
    "- __Leaf/ Terminal Node__:<br/> Nodes do not split is called Leaf or Terminal node.\n",
    "\n",
    "![0*X-UrBzBeKMnoTY6H.png](https://cdn-images-1.medium.com/max/720/0*X-UrBzBeKMnoTY6H.png)\n",
    "\n",
    "- __Pruning__:<br/> When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n",
    "- __Branch / Sub-Tree__:<br/> A sub section of entire tree is called branch or sub-tree.\n",
    "- __Parent and Child Node__:<br/> A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithum used in Decision tree\n",
    "\n",
    "1) ID3 Algorithm. ID3 Stands for Iterative Dichotomiser 3.\n",
    "    \n",
    "    Entropy\n",
    "    Information Gain\n",
    "    \n",
    "2) GINI Index\n",
    "    \n",
    "    Calculation of split\n",
    "    \n",
    "3) C4.5\n",
    "\n",
    "4) Reduction in Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Algorithm. ID3 Stands for Iterative Dichotomiser 3.\n",
    "\n",
    "To understand we have to understand _Entropy_ then what is Entropy __?__\n",
    "\n",
    "- ID3 Algo. uses to calculate __homogeneity__ of sample.<br>\n",
    "- If the sample is completely Homogeneous the entropy is zero.<br>\n",
    "- If the sample is equally divided  then its has entropy is one.<br>\n",
    "\n",
    "\n",
    "Concept of __homogeneity__\n",
    "For __homogeneity__ Entropy  = 0 and for __heterogeneous__ Entropy  != 0\n",
    "\n",
    "![AJd4yO2RLW1KrOxzGrDN_Screen+Shot+2018-06-04+at+3.46.31+PM.png](https://d2jmvrsizmvf4x.cloudfront.net/AJd4yO2RLW1KrOxzGrDN_Screen+Shot+2018-06-04+at+3.46.31+PM.png)\n",
    "\n",
    "But in real life you will not get 100% _homogeneity_ data , so you have to find maximum _homogeneity_ out of _heterogeneous_  option. Hence to select the feature which provide maximum _homogeneity_ we use __Gini & Entropy techniques__.<br>\n",
    "Basically __Entropy__ is degree of randomness.\n",
    "\n",
    "For a binary classification problem\n",
    "\n",
    "1) If all examples are positive or all are negative then entropy will be zero i.e, low.<br>\n",
    "2) If half of the examples are of positive class and half are of negative class then entropy is one i.e, high.\n",
    "\n",
    "## Entropy\n",
    "\n",
    "$$Entropy(S) = -P_{positive} log_2 P_{positive}  -P_{negative} log_2 P_{negative} $$\n",
    "\n",
    "_where_ :\n",
    "- $P_{positive}$= proportion of postive examples\n",
    "- $P_{negative}$= proportion of negative examples\n",
    "\n",
    "_Example_ :\n",
    "If S is collection of 14 examples with 9 YES  and 5 NO, then \n",
    "\n",
    "$$Entropy(S) = -(9/14)log_2(9/14)  - (5/14)log_2(5/14) = 0.94$$\n",
    "\n",
    "Every data is associated its Entropy. Entropy is degree of measurement. Entropy degree of impurity of data. \n",
    "In Decision Tree it will try to understand which feature need to used so the entropy reduce take place lot. __My end goal is to make entropy is Zero. (Entropy reduces by maximum)__\n",
    "\n",
    "__Example:-__\n",
    "\n",
    "Let’s say we have a sample of 30 students with three variables Gender (Boy/ Girl), Class( IX/ X) and Height (5 to 6 ft). 15 out of these 30 play cricket in leisure time. Now, I want to create a model to predict who will play cricket during leisure period? In this problem, we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three.\n",
    "\n",
    "This is where decision tree helps, it will segregate the students based on all values of three variable and identify the variable, which creates the best homogeneous sets of students (which are heterogeneous to each other). In the snapshot below, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables.\n",
    "\n",
    "![image](image/Test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Example\n",
    "\n",
    "Consider a piece of data collected over the course of 14 days where the features are Outlook, Temperature, Humidity, Wind and the outcome variable is whether Golf was played on the day. Now, our job is to build a predictive model which takes in above 4 parameters and predicts whether Golf will be played on the day. We’ll build a decision tree to do that using ID3 algorithm.\n",
    "\n",
    "|Day|Outlook|Temperature|Humidity|Wind|Play Golf|\n",
    "|---|---|---|---|---|---|\n",
    "|D1|Sunny|Hot|High|Weak|No|\n",
    "|D2|Sunny|Hot|High|Strong|No|\n",
    "|D3|Overcast|Hot|High|Weak|Yes|\n",
    "|D4|Rain|Mild|High|Weak|Yes|\n",
    "|D5|Rain|Cool|Normal|Weak|Yes|\n",
    "|D6|Rain|Cool|Normal|Strong|No|\n",
    "|D7|Overcast|Cool|Normal|Strong|Yes|\n",
    "|D8|Sunny|Mild|High|Weak|No|\n",
    "|D9|Sunny|Cool|Normal|Weak|Yes|\n",
    "|D10|Rain|Mild|Normal|Weak|Yes|\n",
    "|D11|Sunny|Mild|Normal|Strong|Yes|\n",
    "|D12|Overcast|Mild|High|Strong|Yes|\n",
    "|D13|Overcast|Hot|Normal|Weak|Yes|\n",
    "|D14|Rain|Mild|High|Strong|No|\n",
    "\n",
    "ID3 Algorithm will perform following tasks recursively\n",
    "\n",
    "    Create root node for the tree\n",
    "    If all examples are positive, return leaf node ‘positive’\n",
    "    Else if all examples are negative, return leaf node ‘negative’\n",
    "    Calculate the entropy of current state H(S)\n",
    "    For each attribute, calculate the entropy with respect to the attribute ‘x’ denoted by H(S, x)\n",
    "    Select the attribute which has maximum value of IG(S, x)\n",
    "    Remove the attribute that offers highest IG from the set of attributes\n",
    "    Repeat until we run out of all attributes, or the decision tree has all leaf nodes\n",
    "    \n",
    "![0*kt06lkLbs7-B7SmE.png](https://cdn-images-1.medium.com/max/720/0*kt06lkLbs7-B7SmE.png)\n",
    "\n",
    "$$Entropy = I(p,n) = -\\frac{P}{\\Bigl(P+N\\Bigr)}* log_2\\frac{P}{\\Bigl(P+N\\Bigr)} -\\frac{N}{\\Bigl(N+P\\Bigr)}* log_2\\frac{N}{\\Bigl(N+P\\Bigr)} $$\n",
    "\n",
    "a) Entropy of target class of the dataset(Whole entropy): \n",
    "\n",
    "|Play| Golf|\n",
    "|---|---|\n",
    "|Yes|No|\n",
    "|9|5|\n",
    "\n",
    "![image](image\\Entropy-Outlook.png)\n",
    "b) Entropy of every attribute(take attribute Outlook for example). {Entropy using the frequency table of two attributes:}\n",
    "\n",
    "|||Play Golf|\n",
    "|---|---|---|---\n",
    "|||Yes|No||\n",
    "|Outlook|Sunny|3|2|5|\n",
    "||Overcast|4|0|4|\n",
    "||Rainy|2|3|5|\n",
    "|||||14|\n",
    "\n",
    "$$ E(outlook = Sunny) = -2/5 log_2 2/5  -3/5 log_2 3/5 = 0.971 $$\n",
    "\n",
    "$$ E(outlook = Overcast) = -4/4 log_2 4/4 -0  = 0 $$\n",
    "\n",
    "$$ E(outlook = Rainy) = -2/5 log_2 2/5  -3/5 log_2 3/5 = 0.971 $$\n",
    "\n",
    "\n",
    "|||Play Golf|\n",
    "|---|---|---|---\n",
    "|||Yes|No||\n",
    "|Temperature|Cool|2|2|4|\n",
    "||Hot|3|1|4|\n",
    "||Mild|3|2|5|\n",
    "|||||14|\n",
    "\n",
    "\n",
    "$$ E(Temperature = Cool) = -2/4 log_2 2/4  -2/4 log_2 2/4 =1 $$\n",
    "\n",
    "$$ E(Temperature = Hot) = -3/4 log_2 3/4 -1/4 log_2 1/4 = = 0.8 $$\n",
    "\n",
    "$$ E(Temperature = Mild) = -2/5 log_2 2/5  -3/5 log_2 3/5 = 0.94 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Gain\n",
    "\n",
    "Decison Tree is all about to _finding an attribute_ that retruns the __highest information gain( i.e most homogeneous branches)__\n",
    "\n",
    "It is based on the decreasing in entropy after a data set split on an attribute.\n",
    "Messures the reduction in Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "G\n",
    "a\n",
    "i\n",
    "n\n",
    "(\n",
    "O\n",
    "u\n",
    "t\n",
    "l\n",
    "o\n",
    "o\n",
    "k\n",
    ")\n",
    "=\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ")\n",
    "−\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ",\n",
    "O\n",
    "u\n",
    "t\n",
    "l\n",
    "o\n",
    "o\n",
    "k\n",
    ")\n",
    "=\n",
    "0.940\n",
    "−\n",
    "0.693\n",
    "=\n",
    "0.247\n",
    "\n",
    "The information gain of rest three attributes can be calculated in the same way:\n",
    "\n",
    "G\n",
    "a\n",
    "i\n",
    "n\n",
    "(\n",
    "T\n",
    "e\n",
    "m\n",
    "p\n",
    ")\n",
    "=\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ")\n",
    "−\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ",\n",
    "T\n",
    "e\n",
    "m\n",
    "p\n",
    ")\n",
    "=\n",
    "0.029\n",
    "\n",
    "\n",
    "G\n",
    "a\n",
    "i\n",
    "n\n",
    "(\n",
    "H\n",
    "u\n",
    "m\n",
    "i\n",
    "d\n",
    "i\n",
    "t\n",
    "y\n",
    ")\n",
    "=\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ")\n",
    "−\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ",\n",
    "H\n",
    "u\n",
    "m\n",
    "i\n",
    "d\n",
    "i\n",
    "t\n",
    "y\n",
    ")\n",
    "=\n",
    "0.152\n",
    "\n",
    "\n",
    "G\n",
    "a\n",
    "i\n",
    "n\n",
    "(\n",
    "W\n",
    "i\n",
    "n\n",
    "d\n",
    "y\n",
    ")\n",
    "=\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ")\n",
    "−\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ",\n",
    "W\n",
    "i\n",
    "n\n",
    "d\n",
    "y\n",
    ")\n",
    "=\n",
    "0.048\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Then choose the attribute with the largest information gain as the decision node. So the tree-forming method used in ID3 would choose Outlook as the attribute for the root of the decision tree. Based on the values of Outlook, the branch with entropy of zero is a leaf node and the branch with entropy larger than 0 needs further dividing.__\n",
    "\n",
    "![image](image\\tree.png)\n",
    "![image](image\\Dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the information gain calculation is change to:\n",
    "\n",
    "       \n",
    "G\n",
    "a\n",
    "i\n",
    "n\n",
    "(\n",
    "O\n",
    "u\n",
    "t\n",
    "l\n",
    "o\n",
    "o\n",
    "k\n",
    ")\n",
    "=\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ",\n",
    "t\n",
    "r\n",
    "a\n",
    "n\n",
    "s\n",
    "I\n",
    "n\n",
    "f\n",
    "o\n",
    ")\n",
    "−\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ",\n",
    "t\n",
    "r\n",
    "a\n",
    "n\n",
    "s\n",
    "I\n",
    "n\n",
    "f\n",
    "o\n",
    ",\n",
    "O\n",
    "u\n",
    "t\n",
    "l\n",
    "o\n",
    "o\n",
    "k\n",
    ")\n",
    " \n",
    "\n",
    "                                   \n",
    "=\n",
    "I\n",
    "(\n",
    "3\n",
    "+\n",
    "6\n",
    "/\n",
    "3\n",
    "+\n",
    "3\n",
    "+\n",
    "6\n",
    "+\n",
    "2\n",
    ",\n",
    "3\n",
    "+\n",
    "2\n",
    "3\n",
    "+\n",
    "3\n",
    "+\n",
    "6\n",
    "+\n",
    "2\n",
    ")\n",
    "−\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ",\n",
    "t\n",
    "r\n",
    "a\n",
    "n\n",
    "s\n",
    "I\n",
    "n\n",
    "f\n",
    "o\n",
    ",\n",
    "O\n",
    "u\n",
    "t\n",
    "l\n",
    "o\n",
    "o\n",
    "k\n",
    ")\n",
    "\n",
    "\n",
    "                                   \n",
    "=\n",
    "I\n",
    "(\n",
    "3\n",
    "+\n",
    "6\n",
    "3\n",
    "+\n",
    "3\n",
    "+\n",
    "6\n",
    "+\n",
    "2\n",
    ",\n",
    "3\n",
    "+\n",
    "2\n",
    "3\n",
    "+\n",
    "3\n",
    "+\n",
    "6\n",
    "+\n",
    "2\n",
    ")\n",
    "−\n",
    "(\n",
    "P\n",
    "(\n",
    "S\n",
    "u\n",
    "n\n",
    "n\n",
    "y\n",
    ")\n",
    "∗\n",
    "E\n",
    "(\n",
    "2\n",
    "+\n",
    "1\n",
    ",\n",
    "1\n",
    "+\n",
    "1\n",
    ")\n",
    "+\n",
    "P\n",
    "(\n",
    "O\n",
    "v\n",
    "e\n",
    "r\n",
    "c\n",
    "a\n",
    "s\n",
    "t\n",
    ")\n",
    "∗\n",
    "E\n",
    "(\n",
    "1\n",
    "+\n",
    "3\n",
    ",\n",
    "0\n",
    ")\n",
    "+\n",
    "P\n",
    "(\n",
    "R\n",
    "a\n",
    "i\n",
    "n\n",
    "y\n",
    ")\n",
    "∗\n",
    "E\n",
    "(\n",
    "0\n",
    "+\n",
    "2\n",
    ",\n",
    "2\n",
    "+\n",
    "1\n",
    ")\n",
    ")\n",
    "\n",
    "\n",
    "                                   \n",
    "=\n",
    "I\n",
    "(\n",
    "0.36\n",
    ",\n",
    "0.64\n",
    ")\n",
    "−\n",
    "(\n",
    "5\n",
    "14\n",
    "×\n",
    "I\n",
    "(\n",
    "0.6\n",
    ",\n",
    "0.4\n",
    ")\n",
    "+\n",
    "4\n",
    "14\n",
    "×\n",
    "I\n",
    "(\n",
    "1\n",
    ",\n",
    "0\n",
    ")\n",
    "+\n",
    "5\n",
    "14\n",
    "×\n",
    "I\n",
    "(\n",
    "0.4\n",
    ",\n",
    "0.6\n",
    ")\n",
    ")\n",
    "\n",
    "\n",
    "                                   \n",
    "=\n",
    "0.94\n",
    "−\n",
    "0.693\n",
    "=\n",
    "0.247\n",
    "\n",
    "       The information gain of rest three attributes can be calculated in the same way:\n",
    "\n",
    "G\n",
    "a\n",
    "i\n",
    "n\n",
    "(\n",
    "T\n",
    "e\n",
    "m\n",
    "p\n",
    ")\n",
    "=\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ",\n",
    "t\n",
    "r\n",
    "a\n",
    "n\n",
    "s\n",
    "I\n",
    "n\n",
    "f\n",
    "o\n",
    ")\n",
    "−\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ",\n",
    "t\n",
    "r\n",
    "a\n",
    "n\n",
    "s\n",
    "I\n",
    "n\n",
    "f\n",
    "o\n",
    ",\n",
    "T\n",
    "e\n",
    "m\n",
    "p\n",
    ")\n",
    "=\n",
    "0.029\n",
    "\n",
    "\n",
    "G\n",
    "a\n",
    "i\n",
    "n\n",
    "(\n",
    "H\n",
    "u\n",
    "m\n",
    "i\n",
    "d\n",
    "i\n",
    "t\n",
    "y\n",
    ")\n",
    "=\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ",\n",
    "t\n",
    "r\n",
    "a\n",
    "n\n",
    "s\n",
    "I\n",
    "n\n",
    "f\n",
    "o\n",
    ")\n",
    "−\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ",\n",
    "t\n",
    "r\n",
    "a\n",
    "n\n",
    "s\n",
    "I\n",
    "n\n",
    "f\n",
    "o\n",
    ",\n",
    "H\n",
    "u\n",
    "m\n",
    "i\n",
    "d\n",
    "i\n",
    "t\n",
    "y\n",
    ")\n",
    "=\n",
    "0.152\n",
    "\n",
    "\n",
    "G\n",
    "a\n",
    "i\n",
    "n\n",
    "(\n",
    "W\n",
    "i\n",
    "n\n",
    "d\n",
    "y\n",
    ")\n",
    "=\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ",\n",
    "t\n",
    "r\n",
    "a\n",
    "n\n",
    "s\n",
    "I\n",
    "n\n",
    "f\n",
    "o\n",
    ")\n",
    "−\n",
    "E\n",
    "n\n",
    "t\n",
    "r\n",
    "o\n",
    "p\n",
    "y\n",
    "(\n",
    "P\n",
    "l\n",
    "a\n",
    "y\n",
    "G\n",
    "o\n",
    "l\n",
    "f\n",
    ",\n",
    "t\n",
    "r\n",
    "a\n",
    "n\n",
    "s\n",
    "I\n",
    "n\n",
    "f\n",
    "o\n",
    ",\n",
    "W\n",
    "i\n",
    "n\n",
    "d\n",
    "y\n",
    ")\n",
    "=\n",
    "0.048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini Index\n",
    " Before starting with the Gini Index, let us first understand what splitting is and what are the measures used to perform it.\n",
    " \n",
    "__What are splitting measures?__\n",
    "With more than one attribute taking part in the decision-making process, it is necessary to decide the relevance and importance of each of the attributes, thus placing the most relevant at the root node and further traversing down by splitting the nodes. As we move further down the tree, the level of impurity or uncertainty decreases, thus leading to a better classification or best split at every node. To decide the same, splitting measures such as Information Gain, Gini Index, etc. are used.\n",
    "\n",
    "__What is Information Gain?__\n",
    "Information Gain is used to determine which feature/attribute gives us the maximum information about a class. It is based on the concept of entropy, which is the degree of uncertainty, impurity or disorder. It aims to reduce the level of entropy starting from the root node to the leave nodes.\n",
    "\n",
    "__What is Gini Index?__\n",
    "Gini index or Gini impurity measures the degree or probability of a particular variable being wrongly classified when it is randomly chosen. But what is actually meant by ‘impurity’? If all the elements belong to a single class, then it can be called pure. The degree of Gini index varies between 0 and 1, where 0 denotes that all elements belong to a certain class or if there exists only one class, and 1 denotes that the elements are randomly distributed across various classes. A Gini Index of 0.5 denotes equally distributed elements into some classes.\n",
    "\n",
    "Gini index performes only Binary Splits.<br>\n",
    "Higher the value of gini higher the heterogeneity\n",
    "\n",
    "__Formula__ :\n",
    "\n",
    "$$ Gini = 1 - \\sum_{i=1}^n (p_i)^2 $$\n",
    "\n",
    "where $ p_i $  is the probability of an object being classified to a particular class.\n",
    "\n",
    "Let’s understand with a simple example of how the Gini Index works.\n",
    "\n",
    "|Past Trend|Open Interest|Trading Volumn |Retrun |\n",
    "|---|---|---|---|\n",
    "|Positive|Low|high|up|\n",
    "|Negative|High|low|down|\n",
    "|Positive|low|high|up|\n",
    "|Positive|high|high|up|\n",
    "|Negative|low|high|down|\n",
    "|Positive|low|low|down|\n",
    "|Negative|high|high|down|\n",
    "|Negative|low|high|down|\n",
    "|Positive|low|low|down|\n",
    "|Positive|high|high|up|\n",
    "\n",
    "Let’s start by calculating the Gini Index for __‘Past Trend’.__\n",
    "\n",
    "    P(Past Trend=Positive): 6/10\n",
    "    P(Past Trend=Negative): 4/10\n",
    "    If (Past Trend = Positive & Return = Up), probability = 4/6\n",
    "    If (Past Trend = Positive & Return = Down), probability = 2/6\n",
    "    Gini index = 1 - ((4/6)^2 + (2/6)^2) = 0.45\n",
    "    If (Past Trend = Negative & Return = Up), probability = 0\n",
    "    If (Past Trend = Negative & Return = Down), probability = 4/4\n",
    "    Gini index = 1 - ((0)^2 + (4/4)^2) = 0\n",
    "\n",
    "_Weighted sum of the Gini Indices can be calculated as follows:_\n",
    "\n",
    "    Gini Index for Past Trend = (6/10)*0.45 + (4/10)*0 = 0.27\n",
    "    \n",
    "Calculation of Gini Index for __Open Interest__\n",
    "\n",
    "    P(Open Interest=High): 4/10\n",
    "    P(Open Interest=Low): 6/10\n",
    "    If (Open Interest = High & Return = Up), probability = 2/4\n",
    "    If (Open Interest = High & Return = Down), probability = 2/4\n",
    "    Gini index = 1 - ((2/4)^2 + (2/4)^2) = 0.5\n",
    "    If (Open Interest = Low & Return = Up), probability = 2/6\n",
    "    If (Open Interest = Low & Return = Down), probability = 4/6\n",
    "    Gini index = 1 - ((2/6)^2 + (4/6)^2) = 0.45\n",
    "\n",
    "_Weighted sum of the Gini Indices can be calculated as follows:_\n",
    "\n",
    "    Gini Index for Open Interest = (4/10)*0.5 + (6/10)*0.45 = 0.47\n",
    "    \n",
    "Calculation of Gini Index for __Trading Volume__\n",
    "\n",
    "\n",
    "    P(Trading Volume=High): 7/10\n",
    "    P(Trading Volume=Low): 3/10\n",
    "    If (Trading Volume = High & Return = Up), probability = 4/7\n",
    "    If (Trading Volume = High & Return = Down), probability = 3/7\n",
    "    Gini index = 1 - ((4/7)^2 + (3/7)^2) = 0.49\n",
    "    If (Trading Volume = Low & Return = Up), probability = 0\n",
    "    If (Trading Volume = Low & Return = Down), probability = 3/3\n",
    "    Gini index = 1 - ((0)^2 + (1)^2) = 0\n",
    "    \n",
    "_Weighted sum of the Gini Indices can be calculated as follows:_\n",
    "\n",
    "    Gini Index for Trading Volume = (7/10)*0.49 + (3/10)*0 = 0.34\n",
    "    \n",
    "|Attribute/Feture|Gini Index\n",
    "|---|---\n",
    "|Past Trend|0.27\n",
    "|Open Interest|0.47\n",
    "|Trading Volumn |0.34\n",
    "\n",
    "__From the above table, we observe that *‘Past Trend’* has the lowest Gini Index and hence it will be chosen as the root node for the decision tree.__\n",
    "\n",
    "We will repeat the same procedure to determine the sub-nodes or branches of the decision tree.\n",
    "\n",
    "We will calculate the Gini Index for the __‘Positive’__ branch of Past Trend as follows:\n",
    "\n",
    "|Past Trend|Open Interest|Trading Volumn |Retrun |\n",
    "|---|---|---|---|\n",
    "|Positive|Low|high|up|\n",
    "|Positive|low|high|up|\n",
    "|Positive|high|high|up|\n",
    "|Positive|low|low|down|\n",
    "|Positive|low|low|down|\n",
    "|Positive|high|high|up|\n",
    "\n",
    "Calculation of Gini Index of Open Interest for __Positive Past Trend__\n",
    "\n",
    "    P(Open Interest=High): 2/6\n",
    "    P(Open Interest=Low): 4/6\n",
    "    If (Open Interest = High & Return = Up), probability = 2/2\n",
    "    If (Open Interest = High & Return = Down), probability = 0\n",
    "    Gini index = 1 - (sq(2/2) + sq(0)) = 0\n",
    "    If (Open Interest = Low & Return = Up), probability = 2/4\n",
    "    If (Open Interest = Low & Return = Down), probability = 2/4\n",
    "    Gini index = 1 - (sq(0) + sq(2/4)) = 0.50\n",
    "\n",
    "_Weighted sum of the Gini Indices can be calculated as follows:_\n",
    "\n",
    "    Gini Index for Open Interest = (2/6)*0 + (4/6)*0.50 = 0.33\n",
    "\n",
    "Calculation of Gini Index for __Trading Volume__\n",
    "\n",
    "    P(Trading Volume=High): 4/6\n",
    "    P(Trading Volume=Low): 2/6\n",
    "    If (Trading Volume = High & Return = Up), probability = 4/4\n",
    "    If (Trading Volume = High & Return = Down), probability = 0\n",
    "    Gini index = 1 - (sq(4/4) + sq(0)) = 0\n",
    "    If (Trading Volume = Low & Return = Up), probability = 0\n",
    "    If (Trading Volume = Low & Return = Down), probability = 2/2\n",
    "    Gini index = 1 - (sq(0) + sq(2/2)) = 0\n",
    "\n",
    "_Weighted sum of the Gini Indices can be calculated as follows:_\n",
    "\n",
    "    Gini Index for Trading Volume = (4/6)*0 + (2/6)*0 = 0\n",
    "\n",
    "|Attribute/Feture|Gini Index\n",
    "|---|---\n",
    "|Open Interest|0.33\n",
    "|Trading Volumn |0\n",
    "\n",
    "\n",
    "We will split the node further using the ‘Trading Volume’ feature, as it has the minimum Gini index.\n",
    "\n",
    "__Conclusion__\n",
    "Gini Index, unlike information gain, isn’t computationally intensive as it doesn’t involve the logarithm function used to calculate entropy in information gain, which is why Gini Index is preferred over Information gain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues With Decision Tree\n",
    "\n",
    "__UnderFitting__\n",
    "\n",
    "Decision - Tree learner can create too simple trees.\n",
    "When model is too simple, both training and test errors are large.\n",
    "\n",
    "__OverFitting__\n",
    "\n",
    "Decision - Tree learner can create over complex trees. These tree don''t generalize the data as well.\n",
    "\n",
    "\n",
    "Consider that we are analysing height of men for a population of m number of dataset.\n",
    "\n",
    "Now, if we are Indian, we shall attempt to collect data from Indian men because it is the easiest way to collect. Am I correct?\n",
    "\n",
    "After collecting a good amount of data, we shall probably start collecting some data from other countries, but not much, as connecting with foreign nationals would be difficult.\n",
    "\n",
    "Then, we would start doing all kinds of data analysis based on the collected data. Now, tell me what would we get? Don’t you think the data tends to be more __aligned with Indian men? This is what is is known as Biased data and the bias__, in this case, is towards Indians.\n",
    "\n",
    "Now, if we have a __good amount of Bias in data then there would not be a good amount of variety.__ So we can conclude that the above mentioned data set has high bias and low Variance and it is not good for our analysis.\n",
    "\n",
    "_Next_, we have mixed data from various regions and includes a number of parameters in the dataset to meet the requirements for each country.\n",
    "\n",
    "__We have very less bias but high variance.__ What do you think? Is it good for analysis?\n",
    "\n",
    "Actually, when we have very __high variance then the model will be very complex__ with high computational costs and the predicted data shall return different values for each run of the model.\n",
    "\n",
    "So, one must have a good balance of bias and variance in the data set.\n",
    "\n",
    "![image.png](https://image.jimcdn.com/app/cms/image/transf/none/path/s8ff3310143614e07/image/i0b8889f52e9aca21/version/1550368890/image.png)\n",
    "\n",
    " hope that the above picture is clear enough to explain the relationship between bias and variance.\n",
    "\n",
    "Now, if one has high bias and low variance, the model is known as an underfitting model. For low bias but high variance, the model is called overfitting. Why?\n",
    "\n",
    "![overfitting_2.png](https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190523171258/overfitting_2.png)\n",
    "\n",
    "One can clearly see that the lefthand-side model is less complex than the righthand-side model. I hope you can now understand the concept of bias / variance, overfitting and underfitting data sets.\n",
    "\n",
    "Let’s continue with our topic on decision tree.\n",
    "\n",
    "After preparing the model of decision tree with a good amount of predictor variables, it will look like below: (righthand-side model)\n",
    "\n",
    "![image](image/Overfitting.png)\n",
    "\n",
    "But if one chooses very less predictor variables, then the model will be converted into an underfitted model.\n",
    "\n",
    "If we choose the proper bias and variance then the model will become a good model (middle one).\n",
    "\n",
    "For an overcorrected model, the model will run fine and with good accuracy for only the training data; but if we test the same model with another set of records, then the accuracy level will go down due to high variance.\n",
    "\n",
    "\n",
    "Understanding model fit is important for understanding the root cause for poor model accuracy. This understanding will guide you to take corrective steps. We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data.\n",
    "\n",
    "![mlconcepts_image5.png](https://docs.aws.amazon.com/machine-learning/latest/dg/images/mlconcepts_image5.png)\n",
    "\n",
    "Your model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). Your model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.\n",
    "\n",
    "Poor performance on the training data could be because the model is too simple (the input features are not expressive enough) to describe the target well. Performance can be improved by increasing model flexibility. To increase model flexibility, try the following:\n",
    "\n",
    "    Add new domain-specific features and more feature Cartesian products, and change the types of feature processing used (e.g., increasing n-grams size)\n",
    "    Decrease the amount of regularization used\n",
    "\n",
    "If your model is overfitting the training data, it makes sense to take actions that reduce model flexibility. To reduce model flexibility, try the following:\n",
    "\n",
    "    Feature selection: consider using fewer feature combinations, decrease n-grams size, and decrease the number of numeric attribute bins.\n",
    "    Increase the amount of regularization used.\n",
    "\n",
    "Accuracy on training and test data could be poor because the learning algorithm did not have enough data to learn from. You could improve performance by doing the following:\n",
    "\n",
    "    Increase the amount of training data examples.\n",
    "    Increase the number of passes on the existing training data.\n",
    "\n",
    "\n",
    "\n",
    "__Avoid Overfitting__\n",
    "\n",
    "How can we avoid overfitting a decision tree?\n",
    "- Prepruning : Stop growing when data split not statistically significant\n",
    "- Postpruning : Grow full tree then remove nodes\n",
    "\n",
    "Methods for evaluating subtrees to prune:\n",
    "- Minimum description length (MDL):\n",
    "  Minimum: size(tree) + size(misclassifications(tree))\n",
    "- Cross-validation\n",
    "\n",
    "Pre-Pruning (Early stopping)\n",
    "\n",
    "Evaluate splits before installing them :\n",
    "- Don't install splits that don't look worthwhile\n",
    "- When no worthwhile splits to install, done\n",
    "Typical stopping condition for a node:\n",
    "- Stop if all instances belong to the same class\n",
    "- Stop if all the attribute values are the same\n",
    "More restrictive conditions:\n",
    "- Stop if number of instance is less than some user-specified threshold\n",
    "- Stop if class distribution of instance are independent of the available features (e.g., using 𝓍2 test)\n",
    "- Stop if expanding the current node does not improve impurity measures (e.g., Gini or information gain).\n",
    "\n",
    "Reduced-error Pruning\n",
    "\n",
    "A post-pruning, cross validation approach\n",
    "- Partition training data into \"grow\" set and \"validation\" set.\n",
    "- Build a complete tree for the \"grow\" data\n",
    "- Until accuracy on validation set decreases, do:\n",
    "    For each non-leaf node in the tree\n",
    "    Temporarily prune the tree below; replace it by majority vote\n",
    "    Test the accuracy of the hypothesis on the validation set \n",
    "    Permanently prune the node with the greatest increase in accuracy on the validation test.\n",
    "Problem: Uses less data to construct the tree\n",
    "Sometimes done at the rules level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm Advantages and Disadvantages\n",
    "\n",
    "\n",
    "__Advantages:__\n",
    "\n",
    "    1) Decision Trees are easy to explain. It results in a set of rules.\n",
    "    2) It follows the same approach as humans generally follow while making decisions.\n",
    "    3) Interpretation of a complex Decision Tree model can be simplified by its visualizations. Even a naive person can understand logic.\n",
    "    4) The Number of hyper-parameters to be tuned is almost null.\n",
    "- __Easy to Understand__: \n",
    " - Decision tree output is very easy to understand even for people from non-analytical background. It does not require any statistical knowledge to read and interpret them. \n",
    " - Its graphical representation is very intuitive and users can easily relate their hypothesis.\n",
    "- __Less data cleaning required__: \n",
    " - It requires less data cleaning compared to some other modeling techniques.\n",
    " - It is not influenced by outliers and missing values to a fair degree.\n",
    "- __Data type is not a constraint__: \n",
    " - It can handle both numerical and categorical variables.\n",
    "- __Non Parametric Method__: \n",
    " - Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure.   \n",
    " \n",
    "__Disadvantages:__\n",
    "\n",
    "    1) There is a high probability of overfitting in Decision Tree.\n",
    "    2) Generally, it gives low prediction accuracy for a dataset as compared to other machine learning algorithms.\n",
    "    3) Information gain in a decision tree with categorical variables gives a biased response for attributes with greater no. of categories.\n",
    "    4) Calculations can become complex when there are many class labels.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application \n",
    "\n",
    "1) Credit risk scoring in the banking and finacial services.<br>\n",
    "2) Fraud detection in the insurance sector.<br>\n",
    "3) Predicting and reducing customer churn across many industries.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refrence \n",
    "http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/ <br>\n",
    "https://medium.com/@rishabhjain_22692/decision-trees-it-begins-here-93ff54ef134\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
